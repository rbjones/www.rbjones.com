% $Id: rjiab02.tex,v 1.1 2010/11/21 10:06:31 rbj Exp $

\chapter{Early Years: 1948-1967}

\section{A Thought at Cambridge - 1966-67}

I was at Cambridge as an undergraduate reading Mechanical Sciences for just one year.
I didn't much care for the engineering but was taken with the IBM-1130 which they had installed in the Engineering Laboratories, partly for teaching undergraduates.

We were taught how to program in FORTRAN II, and prohibited from using assembly language (this was not a multiprogramming system and there was no limit to the damage a user program might do).
Somehow I figured out how to assemble a program into a FORTRAN array and then contrive (by using negative indexing in the array I think) to overwrite the FORTRAN program so that on exit it jumped into the array.
By this means I was able to try out interesting things which couldn't be done in FORTRAN, like getting red ink on the printer.
This was a paper tape machine and the program and data tapes were punched up using these things called ``flexowriters''.
Sometimes they were in such great demand that the computer itself would be sitting idle but all the flexowriters were in use.
So I wrote a FORTRAN program which turned the computer into a flexowriter.
However, since it was in FORTRAN when a tape was read, tabs were converted into spaces, but when a tape was punched the reverse effect did not take place.
When the computer was used as a paper tape editor, the paper tape got much larger, because all the tabs were converted into multiple spaces.

This provoked the first thoughts which I can recall about AI.
The editor was introducing redundancy, and I thought about eliminating redundancy.
I decided that if you could write a program which would take a large set of true statements and code the documents up in the least redundant way possible, so that the paper tapes seemed completely random, then by feeding random numbers into the inverse of the compression function, you would get out random true statements.

OK so this is a completely and hopelessly naff idea, I never tried to implement it, and it obviously wouldn't work.
Fortunately, I don't think it is representative of the character of my subsequent thoughts about AI, though thoughts about AI don't usually actually work, some of them are not quite so easily dismissed.

\chapter{At Work: 1967-1972}

\section{Philosophy of Life c1968-1970}

After leaving Cambridge and taking a job in Computing I found myself living in bedsits, in a strange place, with raging hormones, wondering why I got depressed and what to do about it.
This provoked a certain about of thought, of the kind which might be called ``philosophy of life'', or, {\it how to be}.

I went through a number (can't remember what number) of ``philosophies'' to which I gave names, and tried each for size before deserting it for the next.
The two I remember were the early ``rationalism'' which was an attempt to persuade myself that I should really do whatever it was that I had rationally decided would be best for me, and anarchism which was more of less a capitulation to the fact that I just don't do that.
Of course, anarchism is really the name of a political philosophy, but I used it (at least in my head) for a personal philosophy for a long time, and felt that there should be some fit between the personal and the political (the way you treat yourself, the way you treat others, and the way you think the state should treat people ought to fit together).

Anyway, this was the period in my life when I worried about this kind of thing, and then settled into and gradually forgot this thing that I thought of as anarchism, and which amounted to self-trust.
Just do what you do.

I did have many subsequent periods of soul searching, even to the point of suicidal depression.
But actually, they were mostly provoked by women, or the lack of them, and they didn't result in this same kind of ``philosophical'' thinking.
There was one more period of crisis which seems to me similar, and which I hope has just now drawn to a close.

\section{Russell and Logical Positivism - 1970-71?}

Eventually I went back to University and read joint mathematics and philosophy.
I did do some philosophical reading before that, and must have thought about this, but I have no recollection of these thoughts.
However, many years later I noticed that I had a philosophical position which seemed to be at odds with contemporary philosophers, and eventually concluded that it must have been largely formed by during the philosophical reading which preceded my return to University.
Of these the most significant readings I guess were Bertrand Russell, notably his history of Western Philosophy and his Introduction to Mathematical Philosophy, and Ayer's Language Truth and Logic.

\section{Foundations - 1971-2}

I spent five years in the computer industry before returning to University at Keele.
Toward the end of this period, I was retrieving papers by Church and Turing and others from via the company library in company time, and had some story about why this was relevant to my work which I don't think my boss (John Dawes) actually believed, though he didn't seriously object.
I can't remember what the story was, but it must have been pretty tenuous.
I had a think about floating point representation of real numbers, and thought at that time that it was a mistake that the theory of real numbers was not more constructive, blaming that for the use of floating point numbers for reals.
This must have come from Turings stuff about computable reals.

Since than I have always had a leaning toward better computational support for computable reals, but have never actually done anything significant about it.
These days it seems to me essential for the application of proof technology in building correct software for doing mathematical analysis.

\chapter{At University: 1972-1977}

\section{Foundations - 1972-5}

I arrived at Keele University already knowing something about formal mathematical reasoning, already with some kind of a bee in my bonnet about proof.
I know this because when I started doing mathematics properly (in the second year, since Keele had a foundation year before one could specialise), I did some of my exercises with fairly detailed formal-ish proofs.
Presumably this didn't last very long since there isn't much university mathematics one can hope to do that way, but my notes are now lost.

The first ideas I can recollect about the foundations of mathematics concerned Principia Mathematica.
I acquired the paperback of the Principia ``to *56'' and devoted some time to it.
It seemed to me a bit unclear and I thought it would be useful to define the logical system formally.
This was my first grapple with a kind of regress problem, and I decided that one should define the logical system using the very simplest language in which it could possibly be done, so that there was no real danger of that language being misunderstood.

That language seemed to me to be the language of post productions, of which I had learnt from Marvin Minsky's book ``Finite and Infinite Machines''.
For a while this was to be a dissertation for the philosophy department, but eventually I decided that it wasn't going to work out.

It was purely about syntax, and at this time I had no idea of semantics.
It doesn't any longer seem to me problematic to define the syntax as precisely as we wish, and its clear that the way to do that is not to chose the simplest possible notation for the purpose.

By the time I got to the end of the degree I had some definite ideas for research relating to the foundations of mathematics.
One was that I wanted to work on computerised formalisation of mathematics.
The kind of thing which was actually being done round about then in Edinburgh (except that their logic LCF was a bit to weak, though it would eventually be traded up to HOL\index{HOL}).
The other was to work on support for use of computable reals.
I don't believe that I had any original ideas on how these were to be done.

\section{Philosophy - 1973-5}

I did half a degree in philosophy, part of which was mathematical logic and the philosophy of mathematics.
I was not a good philosophy student, I was more interested in working things out for myself than in studying the works of other philosophers, and quite unsuited to the latter.
In the examinations one was expected I think even when asked a question about some particular problem without reference to any other philosopher, to show some knowledge of what other philosophers had said about the problem.
I think the model answers were demonstrations of scholarly knowledge peppered with small amounts of original analysis.
However, I didn't have the scholarly knowledge and I was pretty slow.
I liked to think about the problem posed and come up with a position on the spot, with no more reference to other philosophers than could be avoided.

I recall an obscure conversation with Jonathan Dancy on a bus during the period after taking the exams but before getting the result.
He said that for the philosophy I had been on the list of those considered for viva (meaning that I was on a borderline), but that they had decided against giving me one.
He said he had argued that the style of the essays should be taken into account, evidently he thought this a strong point, but there just wasn't enough {\it content} so they wouldn't have it.
I think Dancy thought he was telling me something from which I could conclude that I would be getting a first overall (to get a first in a joint honours you needed a first in one subject but only an upper second in the other) and he must have heard enough about my maths to think that I would have no doubt about a first in mathematics.

I digress, mainly to suggest that {\it originality} is in my blood.
Not in the laudatory sense of having meritorious ideas which no-one else has had.
In the sense of ignoring what has been done before, and thinking things out from scratch.
Creating some solution of my own, not necessarily new to the world, not necessarily of outstanding merit.
Originality as a bad habit, or a pathology, like autism.

As far as ideas from this time which still have something for me, I can remember the following:

\begin{description}

\item[words]
I didn't like the idea that philosophy was just about words.
In fact I didn't like the idea that philosophy was about words at all!
I even made an attempt to articulate methods for disengaging from the meaning of words, this in an essay in political philosophy.
I was particularly unimpressed by the philosophy of Wittgenstein, both the early and the late.

\item[politics]
I wanted to talk about anarchism, as a political doctrine.
I don't think I got anywhere but the attempt was also an early attempt to wriggle out of entanglement with words.

\item[ethics]
Possibly my worst essay was on ethics, considering the question ``Is morality necessary''.
In fact this was bound up with ``anarchism'' as I construed it.

\end{description}

\chapter{Back at Work: 1977-1998}

\section{Expert Systems in Decision Support - 1982}\label{1982}

I wrote this paper while working on microcode architectures for mainframes (CME), when I had decided it was time to move on.
The paper was never published, never even submitted for publication so far as I recall, but did help to get me a job in ICL working with expert systems.

\section{Knowledge Bases - 1985}

In 1985 I moved from expert systems (not impressed) to database management software.
While working professionally on relational database software I thought privately about knowledge bases.

This topic combines an interest in AI with one in foundations, which is typical of my interest.
I thought up an architecture and made some steps toward implementing it on an ICL PC (with a Z80 processor).The starting point for this is the idea that intelligence involves reflexive reasoning, so an artificial intelligence should be a self applicative piece of software.
To make it easy to reason about itself, this should be in a nice functional language, and at this time the state of the art in implementation of functional programming languages (embodied in Miranda) used combinators.
So the knowledge base is a combinator, part of which is a program which takes the knowledge base (including itself of course), as an argument, and computes a new value for the knowledge base,
Actually, it was to be a worm, not overwriting the database but continually adding new versions of it to a list, the space cost being mitigated by the sharing one gets for free in functional systems.

The foundational element comes in when you start considering the combinator which is the knowledge base not as a data structure but as a proposition, and want to take the computations which update it not as mere computations but as inferences.
So what kind of a logic do we want for this?
Well, the self applicative bit made me think that this had to be a type-free system.
It was going to be reasoning about a function which was to be applied to something which contained itself.
The obvious answer to this need seemed to be combinatory logic, about which I knew next to nothing.
So that's how I got interested in combinatory logic, and how I resumed thinking about the foundations of mathematics.

Somewhere in this period I went to a workshop at Appin in Scotland which was sponsored by ICL and was on persistent databases, so I wrote this paper:
``Persistent Applicative Heaps and Knowledge Bases'', as my offering (which was a bit irrelevant since I was really there because ICL was paying).
I also got a slot to talk.
After hearing the tenor of the papers I sensed a consensus with which I disagreed, which was that languages with persistent storage had to have dynamic type systems.
I decided not to talk about my paper at all, but instead to argue, on the fly as it were, the falsity of this common ground.
The argument was along the following lines.
We would expect none of these programs to raise type errors during execution.
We would expect this to be provable.
It must therefore be statically determinable, and this static determination could be built into a type system.
Of course, I don't really remember the details of my argument, my memory isn't that good.
But I still think there is a case that this is possible.
Possibly not with a decidable type system, but the main point was not to advocate a static type system but simply to challenge the assumption that a dynamic type system was essential.
My talk caused an uproar and I loved it.
Like so many conferences and workshops in computing the average talk went by with very little if any comment and no serious discussion.
I livened up the proceedings considerably.

Some of the academics talked sympathetically to me afterwards, clearly thinking I had been through a terrible ordeal, which I must be regretting.
I wasn't, I was very pleased with it all.
Not that the discussion was wonderful, I only had something like twenty minutes, and the discussion had to be curtailed.
But my presentation on was quite brief, and it caused instant uproar.
In fact, discussion had to be curtailed simply so that I could complete the presentation of my argument (most people wanted to object without actually hearing the argument).

Its not that I crave infamy, but it was all pretty stodgy up 'til then.

\section{Creative Foundations - 1986}

Having decided that combinatory logic was the appropriate foundation system to use in a ``knowledge base'' I was disappointed to discover that combinatory logics, when they were consistent were weak, I spent
some time trying to devise a consistent and strong illative combinatory logic.

The first red herring on this trail was the idea that the coding of recursive functions in combinatory logic could be exploited to yield a strong system.
No-one understood the idea, not even well enough to tell me why it was a duff one.
I wrote two essays before I figured out what the problem was.
The first was ``Logical Foundations and Formal Verification'' which was presented at an Ada Verification workshop in South Carolina, but possibly not included in the proceedings.
The second was ``Creative Foundations for Program Verification'' which I thought a really neat paper, and was published in the proceedings of Milcomp86 a military computing show with a pretence at being a conference.

It wasn't till I had some conversations with Peter Aczel and tried to explain it to him that the penny dropped for me.
He asked me what was the proof theoretic strength of the system, and I said it didn't really have one.
He also spent some time explaining proof theoretic strength to me, and explaining some of his systems to me.
However, I was looking for something particular, and he took offence I think that I said that his systems were not quite what I was looking for.
I think he was thinking of me as if I was one of his students, and students it seems don't do that kind of thing.
I think they are supposed to be more like disciples.

This was for me I believe a very instructive error, though its still hard to explain exactly what was the error (mainly because its hard to make clear the rationale which made me think that the ``creative theory'' was a foundation system.

After this I continued to think about reflexive foundation systems, particularly in combinatory logic, and went down quite a few blind alleys.
This only stopped when I got deeper into well-founded set theories and concluded that what they offered was really the neatest kernel of the foundational problem, and that reflexivity if thought desirable could be done somewhere other than the foundations.

I now think that reflexive foundations if we really want them may be best approached via set theories with a universal set.
These contain well-founded sets as well as ill-founded ones of course, and it seems that you can make them as strong as you like by talking about the well-founded part.

\section{Well Founded Foundations 1988}

In 1988 I was for a while assigned as ICL's representative (and the secretary) on the VDM\index{VDM} standardisation panel.
VDM\index{VDM} was a formal specification language with a kind of domain theoretic semantics, which came from Brian Monahan.
I had an agenda which I had been pressing for a couple of years which said that specification languages should be foundation systems, and that ideally foundation systems should be reflexive.
Well VDM\index{VDM} and Z\index{Z} were both well founded specification languages, but the former was not really much of a foundation system so far as I could see.
Ontologically it did have a hierarchy of types progressing through a function space constructor.
But the function spaces were spaces of continuous functions, good enough for modelling higher order computable functions, but not for mimicking higher order logic.
So VDM\index{VDM} looked to me rather like a first order logic, an elaborate version of LCF, to weak to do serious mathematics.
I don't know whether I was right about his, possibly not.

Anyway the problem of finding a foundation system which was suitable both for Z\index{Z} and for a strong VDM\index{VDM} seemed interesting.
It seemed to me that the key problems to be solved relative to the known well-founded foundations such as ZFC and HOL\index{HOL}, were the need for polymorphism and the need for structuring.
Structuring relates both to local definitions (which don't work in HOL\index{HOL} in the way that they do in ML and in which one needs them to work in any serious specification language) and to support for modularity.

The game plan for this work was to do the work from semantic models in Cambridge HOL\index{HOL!Cambridge}.
First axiomatise ZFC in HOL\index{HOL} (not worrying about the fact that the result would be strictly stronger than the first order version).
This gives a rich ontology within which ontologies of the desired systems could be constructed.
Then three theories were to be constructed in turn.
The first was to change the ontology from an ontology of sets to one of functions.
This is done, roughly, by taking the heriditarily functional sets from the domain of ZFC in HOL\index{HOL}.
There were some tweaks to this to make the resulting theory of pure functions extensional.
The way I worked with this was to define a new type based on these pure functions, define various operations over these functions, think up an axiomatisation of the theory and prove the ``axioms''.
The adequacy of the ``axioms'' could then be established by doing the reverse construction, going back to the theory of sets using a ``hereditarily'' subset of the pure functions which mimics the sets of ZFC in HOL\index{HOL}.
I didn't to the last bit, and I didn't complete the proofs of the proposed axioms.

The next two theories were the theory of polymorphic functions, and that of ``structured functions''.
A polymorphic function was a function over the pure functions, in which the functions in the domain are interpreted as assignments of types to type variables.
So they are families of functions indexed by ``type'' assignments.
I should mention that this is all really ``type free''.
I'm heading for something like an illative combinatory logic except that it isn't really reflexive.

There is just an illusion of reflexiveness of the kind you get in a polymorphic type system.
To explain this another way, the idea is like HOL\index{HOL} without the types.
Its like HOL\index{HOL} in that everything is in the semantic domains, all the logical connectives and quantifiers, unlike first order logic in which there is a distinction between terms and formulae and only the terms denote values in the semantic domain (the domain of an interpretation).
To do this in HOL\index{HOL} without running into consistency problems you have the type system.
In the proposed system you don't actually have types, consistency is realised through well-foundedness.
There is no type system, but the well-foundedness gives you the same problems that you get with a type system, which is why we still have polymorphism.
The problems here are the problem of the non-existence of functios you would like to have, like the identity function, the equality predicate, and of course the quantifiers.
So we settle instead for families of functions.
e.g. the family of identity functions restricted to some domain.
The family of restricted quantifiers.

The ``structured functions'' where polymorphic functions defined in contexts where various identifiers are thought of as external, perhaps because they are defined in some other module, perhaps because they are defined in some local definition.
The value of the polymorphic function depends upon the values of some identifiers in relevant context.
The context is again represented by a function, which is understood as a map from names to values.

To make this work you need some constraints on the function spaces which are used to represent the polymorphic and structured functions.
I didn't progress this work far enough to be sure what the necessary conditions are.

This work petered out for several reasons, firstly there was an influx of theoreticians from Denmark into the standardisation process sponsored by Dines Bjorner, who had there own strong sense of what should be done with the semantics of VDM\index{VDM} which was quite different from my ideas.
Second I was taken off the VDM\index{VDM} work and John Dawes was put back (for reasons nothing to do with me).
Thirdly, doing all this formally would have been too large an undertaking.
This was one of a long series of formal experiments from which a little is learnt but which could not hope to be completed with the kind of effort available.

The idea of making novel well-founded foundation systems by filtering the sets of ZFC was however one which I think is quite fun.
I also have tinkered with category theoretic foundations in the same vein.

\section{Stealing Proof Theoretic Strength}

Before I gave up on non-well-founded foundation systems the method I was thinking in terms of stealing proof theoretic strength from ZFC.
The idea is to start with a weak illative combinatory logic which includes a weak arithmetic with standard semantics (i.e. the numbers really are the natural numbers).
Then you strengthen it by adding more arithmetic truths, one obvious contender is the one which says that ZFC is consistent.
There is a result of Feferman's which isn't proved in this context but which one could hope to make work in this context.
It says roughly that if you add to Robinson's Q an axiom stating the consistency of some other first order theory then that theory becomes interpretable in the extension to Q.
Consequently adding the consistency claim gives you all the proof theoretic strength of the system you assert consistent (plus a bit).

\section{Embedding Z in HOL\index{HOL} - 1990-1993}

This was one of the few good ideas which I had which actually got implemented because I got a team at ICL to do it!
The result was ProofPower\index{ProofPower}.
Not a startling idea, obvious but effective.

The whole story is of course rather complicated some of it is covered in \cite{rbjt014, rbjt015}.

\chapter{Independence: 1998-}

\section{Philosophical Revival - 1994 onwards}

In 1994 I got my first web site.
After struggling some time to get web access through ICL, it eventually happened, and with the web access came, through a backdoor an external website.
In the Finnish arm of ICL I found an outfit which had an externally visible website, contrary I think to ICL policy on firewalls, and which was willing to cross mount web-space on one of our Sun workstations, making it externally visible.

The primary purpose of this was to put up information about ProofPower\index{ProofPower}, but I also put up my own personal web-space, which was philosophical.
I did some experiments with converting philosophical classics into hypertext, of which the modern ancestors (actually not a lot different) are now on rbjones.com.
The hypertext classics are not significant.
What was significant was the gradual revival of my philosophical thinking, beyond the scope of the foundations of mathematics.
This was a very tentative process, evident initially only in the few extended periods away from work (i.e. vacations).
Its possible to pursue technical problems in private time (though I can't say the results are spectacular, this is what I have done all my life), but it was much harder to take up philosophy on that basis.
I don't actually know when this started, but it was very roughly at the time when I started experimenting with the web.

At the time I got into hypermedia I was very taken by it and utterly convinced that it was the right medium for me.
It has taken very many years for me gradually to come to the conclusion that I was wrong.

1994 was also the beginning of the end for the formal methods team in ICL.
CESG\index{CESG} was running into problems with some of its high security developments and soon there was to be a change of policy and they would simply stop funding this kind of R\&D.
The hole was partly plugged by safety critical applications but there was not enough of this to keep the formal methods unit going.
The honeymoon period for the Defence Technology Centre was past, and the focus was once more on profits. not research.

In 1994 I volunteered for other kinds of work, in the hope that a slimmed down formal methods team would have have a better chance of survival, and believing that such a slimmed unit would have to be headed by someone less senior than myself to be cost effective.
For me that was the end of interesting work at ICL, and I decided that on my fiftieth birthday I would go it alone in the hope of finding something more congenial.
I then had the sense that I wanted to write, but I had a pretty fluid sense of what I wanted to write.
Philosophy, software, ...
What I really wanted to do was to find a way to earn a living from creative activities.
In particular from doing what I though a good idea rather than what someone else thought a good idea.
I had this sense of being handicapped throughout life by this dichotomy between what I was paid to do at work and the more interesting things which I attempted in my own time.

Attempting this at the age of fifty had the additional advantage that I could fall back on a pension from ICL.
I did this rather faster than I expected to because the year following ICL changed the rules in a way which gave me an incentive to take the pension sooner rather than later.
Gradually the revenues that I was then able to obtain, first from contracts with ICL and later with contracts from Lemma1 has petered out so that we now live on two pensions between us.

From when I left ICL to the present moment is now a period of seven years in which I have failed to get things in order.
I have had what I now think of as ``philosophers block'' (if one can be said to have that without any sign of ever having a philosophical capability which might have been blocked), the beginnings of which were almost immediate, and which may just possibly be now disappearing.
Notwithstanding this ``block'' there has been quite a bit of intellectual and philosophical development during that period which is hard to trace chronologically.

For me it is important however, to exhume at least some of this stuff, so I shall attempt it in the following without worrying to much about the chronology.

\section{Artificial Intelligence}

A major theme in this period has lain on the boundaries of artificial intelligence and formal mathematics.
This began with something I called ``Engineering Logic'' and progressed first though ``The Global Super-brain'' to ``X-Logic'', all different takes on essentially the same theme, and all engineering projects bound up with philosophy.

I was not attempting to do philosophy when I started ``Engineering Logic'' but I was then seeking to put forward an architecture for, and an approach toward artificial intelligence which was primarily rooted in producing software first which would not be intelligent, in framework which might eventually lead to intelligent software.
The rationale for this framework could not be explained without getting into philosophy, and the philosophy proved awkward to explain, because it was old fashioned philosophy, closer to logical positivism than to anything which philosophers did at the time.
It was based on tenets which seemed generally discredited (especially in the marginal world of on-line philosophising).

I'm going to say what I can about the engineering side of this in this section, and deal with the more philosophical aspects later.

There are three clearly identifiable stages in my thinking on this topic, which I give the tags:

\begin{itemize}
\item Engineering Logic
\item Global Super-brain
\item X-Logic
\end{itemize}

Each of these stages is best thought of as adding something to what went before, rather than as replacing its predecessors.
All of this was conceived of at the time as something like software engineering.
I was aware that the rationale for this depended upon certain philosophical doctrines, which when I first started I did not think of as novel, and was only dimly aware of as controversial.
So my first inclinations philosophically were perhaps just to run of a position statement which gave the relevant philosophical background.
However, I became increasingly aware of the distance between the philosophy on which these architectures are based and the kind of thing which contemporary analytic philosophers are saying.
Not just that they were saying different things, but that this whole way of thinking was thought of as having been discredited.
At first I didn't even know where the philosophical ideas had come from, but eventually I figured out that it was probably predominantly from Russell and Ayer, modified by my own knowledge and experience of modern logic and computing.

Now I am returning to this, thinking of it as philosophy.

The ideas started with an architectural conception which corresponded to (what I consider) the obvious way of working out from the formalisation of logic, and there is a picture illustrating the scheme at 

\subsection{Engineering Logic}

I've chosen this tag here, even though it was just one element of the ``topography for AI'' which I sketched out, because it was for me the most important element, because I sought to advance the aspects of it which could be regarded as realisable engineering, rather than speculative research.
So ``engineering logic'' was the limits of engineering logic based automation of ({\it inter alia} but importantly) engineering design.

The idea of engineering logic is simply the idea of automating engineering design through formalisation.
This kind of work, not itself conceived of as artificial intelligence was intended to form a basis on which intelligent capabilities might later be established, but which was {\it just} a special kind of software engineering, in which logic is applied first to the formalisation and automation of mathematics and thence to science and ultimately to engineering.

Underlying this of course is a story about how one ideally can formalise science and engineering which is in spirit if not in detail similar to what logical positivists such as Carnap were engaged in, inspired to a significant extent by the prior work of Russell in the formalisation of mathematics in Principia Mathematica.
Intelligent logic is intended as an ``intelligent'' overlay on engineering logic.

\subsection{Global Super-brain}

This stage in thinking was to some extent simply connecting in with a popular theme, to some extent it was rethinking in terms of global networks.
There is a distinction in the topography of the previous scheme where one crosses the boundary from the domain of the analytic, and hence beyond the scope of logic.
This is marked by a change of colour from shades of blue to shades of green, and though the topology shows some subdivision of the green zone I didn't do any significant thinking about this side of things (not even by my own lights).

In the ``super-brain'' stage that distinction is made more prominent, the two parts are called the {\it analytic} and the {\it holistic} super-brains and the balance gets a bit better (in terms of how much I thought about these different sides of things).

The analytic super-brain was a networked version of intelligent engineering and intelligent logic.
The holistic super-brain was the use of global networks to transform the operation of markets.
An important part of this was its intended role in achieving greater transparency in the total costs and effects of products and companies upon the issues of greatest importance to us all.
This is also about the future of democratic institutions, capitalism and free markets.
So in a more concrete way the holistic side connects with the more socially oriented side of philosophy, including ethics and political and economic philosophy.

\subsection{X-Logic}

This last stage in the evolution of these ideas is where I connect the Super-brain with XML.

\section{Formal Philosophy}

Now alongside the line of thought about engineering knowledge processing artifacts, I had a more self consciously philosophical line of thinking.
This was mainly about the role of formal logic in philosophy, which following Russell, and Carnap I considered crucial.

\section{The Set/Class distinction, October 2005}\label{October2005}

The distinction between sets and classes\index{class} has I think always struck me as being arbitrary, but it was only in 2005 while engaged in a discussion on the FOM mailing list\index{FOM!mailing list} that I found an argument to that effect which I considered convincing.

The argument flows from a simplification of the description of the intended domain of well-founded set theory which goes under the headings ``the iterative conception of set'' or ``The Cumulative Hierarchy''.
There had been attempts (for example, by Dana Scott and George Boolos) to further explicate this account of set theory using some theory which talked about the stages in the iteration which supposedly lead to the cumulative hierarchy\index{hierarchy!cumulative}.
I found these unconvincing, because these theories seemed to me more complicated than the theory which they sought to explain.

Furthermore, it had begun to dawn on me, that the effect of the classic descriptions should be to simply to make more clear to concept of ``pure, well-founded set'' which one would expect to be co-extensive with the completed cumulative hierarchy.
Thinking therefore of the best way to define the concept of a pure well-founded set, I came up with the following definition by transfinite recursion (or induction):

\begin{quote}
A set is any definite collection of sets.
\end{quote}

The reading of this as an inductive definition requires that nothing counts as a set unless its sethood is justified (possibly by some highly infinitary ``proof'' by this definition alone.

Thus one may begin with the empty set, which by any account is definite and which, whatever a set may be, contains nothing else, and from there, after the manner of the interative conception but with more freedom as to order, one may see that subject to some discussion of the meaning of the term {\it definite}, any pure well-founded set may be reached.

The term definite here is essential to the definition, for if it is removed the definition engenders contradiction, for the extension of the concept thus defined would be a set, and could be shown both to be and not to be, well-founded.

An inductive definition of a concept yields (or is more fully explicated by) an induction principle which may be used to prove results true of all that falls under the defined concept.

In this case the relevant principle of induction is $\in-$ (sometimes {\it noetherian}) induction:

\begin{quote}
If some property P is:
\begin{itemize}
\item true of the empty set (the base case), and if
\item whenever it is true of every member of a sets, then it is true of the set, then
\item P is true of every set
\end{itemize}
\end{quote}

Using this principle it can be shown that the sets thus defined are:

\begin{compactdesc}
\item[pure] i.e. they are sets all the way down, constructed ultimately only from the empty set.
\item[well-founded] i.e. there are no infinite descending membership chains, if you keep going down then eventually you will get stuck in the empty set.
\end{compactdesc}

It also follows directly from the definition that every definite collection of sets is a set, and hence that if there are ultimate collections of sets (which are not themselves members of a set) then these collections are not definite.

Provided the requirement of definiteness is in there, the line of reasoning which might otherwise have obtained a contradiction suffices only to show that the collection of all sets cannot be definite.
But the requirement can be very weak indeed.

The effect of this ``idea'' is to give a more concise presentation to the iterative conception of set, in which some inessential features are omitted (in particular the iterative conception proceeds in stages according effectively to the {\it rank}\index{rank!of a set} of the sets (which is the number of steps or stages which are necessary in its construction), whereas the minimal ordering is in fact that partial ordering which is the transitive closure of the membership relationship.
The concision of the definition, makes more transparent the consequences of the definition, in particular the incoherence in the case that the constraint of ``definiteness'' is omitted or made trivial.

In effect, the whole process as described in the iterative conception cannot be completed, as a matter of logical necessity, and a choice must be made, which we might consider to be arbitrary, about where the process is to come to an end.

\section{The Set/Class distinction, January 2010}

As a result of futher provocation on the FOM mailing list\index{FOM!mailing list} in January 2010, my understanding of the relationship between the axiom of replacement and the ``limitation of size''\index{size!limitation of} principle was improved
\footnote{Admittedly the connection is pretty obvious, but I think the explanation which then occurred to me is worthwhile, and actually explains a bit more than that connection.}%

Let us consider, in the context of the definition of well-founded set\index{set!well-founded} in Section \ref{October2005}, the alternative ways of defining ``definite'' and the conceptions of set which arise.

First note that the role of the concept ``definite'' here is to separate sets from classes\index{class}.
A collection of well-founded sets is a set if it is definite, and otherwise its not a set, so we'll call it a class.
It may be helpful to switch terminology at this point, to emphasise that what's really happening here is that we are making a more-or-less arbitrary choice about when to stop making new sets.
So instead of definite we will use the term ``small'', which is familiar to set theorist for making exactly this distinction.

Our definition (of pure well-founded set) becomes:

\begin{quote}
A set is any small collection of sets.
\end{quote}

To make this definite we have to decide where to draw the line, i.e. when to stop making sets.
This we do by deciding what {\it small}\index{small} is to mean.

Its worth distinguishing two different things which {\it small} might be a property of.
The first connects us directly with the iterative conception of set, and is the {\it rank}\index{rank} of the collection, the number of stages involved in its construction.
If we construe {\it small} as a property of the rank of the set, then we are stating the number of stages in the iterative conception that are to be completed.

The most obvious alternative to this is to take ``small'' literally, as referring to the cardinality\index{cardinality} of the set.

First order set theories do not usually place upper bounds on the size of the universe, either in terms of rank or cardinality.
The effect of the axioms is to place lower bounds, their purpose being to ensure that there are sufficient sets for the purposes of mathematics.
However, the convenience of the theory depends not only on their being sufficient sets, but also on the universe of sets having strong enought closure properties, so that various kinds of construction can be relied upon always to yield sets.

If our definition of well-founded set is taken as determining a collection of models for set theory, and the collection is to be determined by chosing a particular interpretation of ``small'' the interpretations begin those obtained for values at least that large, then the kinds of theory we get are subtantially different according to whether ``small'' is taken to be a property of rank or of size.

If of rank, then by varying the value of small we can get theories of arbitrarily high strength, but which all have weak closure properties.
The universe of sets will not even be known to be closed under the pair constructor.
Chosing a large value for ``small'' will result in it being provable that there are sets with good closure properties, but the universe will not have these properties.

On the other hand, if small is a measure of size, then we get replacement, and with it other closure properties. 
