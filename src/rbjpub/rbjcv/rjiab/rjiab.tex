% $Id: rjiab.tex,v 1.1 2010/02/01 11:34:11 rbj Exp $
% bibref{rbjp006} pdfname{p006} 
\documentclass[10pt,titlepage]{book}
\usepackage{makeidx}
\usepackage[unicode,pdftex]{hyperref}
\pagestyle{headings}
\usepackage[twoside,paperwidth=5.25in,paperheight=8in,hmargin={0.75in,0.5in},vmargin={0.5in,0.5in},includehead,includefoot]{geometry}
\hypersetup{pdfauthor={Roger Bishop Jones}}
\hypersetup{colorlinks=true, urlcolor=black, citecolor=black, filecolor=black, linkcolor=black}
\usepackage{html}
\usepackage{paralist}
\usepackage{relsize}
\bodytext{BGCOLOR="#eeeeff"}
\makeindex
\newcommand{\indexentry}[2]{\item #1 #2}
\newcommand{\glossentry}[2]{\item #1 {\index #1 #2}}
\newcommand{\ignore}[1]{}
\def\ouml{\"o}

\title{Autobiographical Materials}
\author{Roger Bishop Jones}
\date{\ }

\begin{document}
\frontmatter

\begin{titlepage}
\maketitle

\vfill

%\begin{abstract}
%A kind of intellectual autobiography, organised as a catalogue of ideas organised into themes and slotted into a chronology..
%\end{abstract}

\begin{centering}

{\parskip=0.3in
This book is dedicated to my father
\vfil
{\relsize{+2} Reginald Arthur Bishop Jones}
\vfil
and my to mother
\vfil
{\relsize{+2} Vina Bishop Jones}
}

\vfill

\footnotesize{
Started 2005-01-26

Last Change $ $Date: 2010/02/01 11:34:11 $ $

\href{http://www.rbjones.com/rbjpub/www/papers/p006.pdf}{http://www.rbjones.com/rbjpub/www/papers/p006.pdf}

Draft $ $Id: rjiab.tex,v 1.1 2010/02/01 11:34:11 rbj Exp $ $

\copyright\ Roger Bishop Jones;

}%footnotesize
\end{centering}

\thispagestyle{empty}
\end{titlepage}

{\parskip=0pt\tableofcontents}


\mainmatter

\section*{Preface}

This is a copy of a document which was not intended as autobiography but ended up heading too far in that direction.
I needed to start pretty much from scratch, so I made this copy before doing that.

\chapter{Introduction}

This ``book'' contains notes on some of my ideas, and on the themes which tie them together.
Ideas which go places are usually collected together in substantial pieces of work and documented as such, but many of my ideas do no more than provide a fragment of the lens through which I peer upon the world.

I have adopted both a thematic and chronological presentation in the first two parts of the book, with various supplementary materials provided in a third part.

As it stands the material includes what now seems to me extraneous autobiographical material, which I expect to remove.

I offer no warranty that ideas presented here as if they were my own, really are my own ideas.
I do not wilfully present as my own ideas which I know came to me from some other, but I do not pretend to remember the source of everything which I learn, or the sources which contribute to my own innovations.
An idea, when its time has come, will occur in many thinkers at once.

\part{Themes}

\chapter{Artificial Intelligence}

My claim to have had any ideas of interest which properly belong to the field of Artificial Intelligence is moot.
Its inclusion here among the principle themes, rather than as a small collection of oddments, reflect the role which the automation of reason plays in the motivation of much which belongs properly to the other themes.

So I will here mention, not just the few ideas which I can must which belong to artificial intelligence, but also the ways in which these ideas have influenced my thought in foundational and philosophical matters.

I like to think about {\it the big picture}.
For me this involves thinking about the distant future and the relationship between natural and artificial intelligence in that context.
In the latter part of my life it has seemed natural to construe this in the context of pervasive networked computing, in which intelligence is a characteristic of the whole, as well as of many of its parts.
In which it becomes decreasingly possible to localise any particular capability.

This networked intelligence is sometimes called the {\it Global Brain}\index{Global Brain} a term coined in 1982 by Peter Russell \cite{russellp82}.

\chapter{Foundations}



\section{Principlia Mathematica and Post Productions}

Having begun with Russell's \emph{Introduction to Mathematical Philosophy} \cite{russell10} it was natural eventually to come down to (some of) the detail in \emph{Principia Mathematica} \cite{russell10} and \emph{Mathematical Logic as Based on The Theory of Types} \cite{russell1908}.
Coming to Russell's theory of types from a background in computing, I was immediately inclined to regard the vicious circle principle as heavy handed.
The apparent proscription of recursive definitions (though the self reference in well-founded recursion turns out to be eliminable), and the real proscription of self-applicative functions is an enduring problem in the foundations of mathematics.

It was another kind of reservation about Principia Mathematica which lead to my first conception of how the formal foundations of mathematics might be improved.
This reservation was about how well the theory of types was defined in Principia Mathematica, which by contrast with the precision with which the syntactic aspects of programming languages were defined seemed very woolly and unclear.
I had previously been impressed by the various simple languages for describing Turing computable functions, and it seemed in the spirit of foundationalism to reduce the syntax of the foundation to one of these very simple languages.
I chose post productions as the notation most appropriate for the description of syntax and set out briefly as an undergraduate to effect a description of Russell's theory of Types ultimately in post productions.
I didn't get very far.

This was the first of a long sequence of forays into the design of foundation systems which continue to the present day.

\subsection{Reflection in Logical AI}

When I graduated with my degree in Mathematics and Philosophy I intended to do a PhD relating to the development of software supporting the formal derivation of mathematics, and went to the Computer Science Department at the University of Warwick with that in mind.
Personal problems interfered with this plan, and after less than one year at Warwick I abandoned my studies are took a job in software development.
My interest in the foundations of mathematics then lay dormant for several years.

It resurfaced after a period working in ``knowledge engineering'' \cite{jones82} while I was working on database query software.
I then started to think about what kind of logical foundation system would be appropriate for use in a ``knowledge base'' in which the database was logically an suitably structured collection of definitions, query a process of inference in the theory determined by those definitions, and database update a process of extending or modifying these definitions \cite{jones85}.
It seemed then to me that in order for this knowledge base to evolve towards intelligent behaviour it would have to be possible for it to reflect upon and modify its methods in the light of experience.
The deductive system which maintained the knowledge would have to be stored in the knowledge base, it would represent a functional program operating on a functional data structure which included itself.

In this way I came to seek a logical foundation system which was capable of reasoning about itself, and in particular, which was implemented as a function which was continually applied to an elaborate data structure which included itself.
The foundation system would therefore have to admit functions which fell within their own domain.

It was natural to look to combinatory logic for such a foundation system, particularly because combinatory logic had recently become fashionable in the implementation of functional programming languages.
I was disappointed when I eventually discovered that combinatory logics were, when not inconsistent, rather weak.
There then began a period in which I conceived of myself as in search of strong reflexive foundation systems.
Though I might have been happy to adopt one if I could find it, it seemed likely that I would have to devise one for myself, for which task I was ill-equipped. 

\subsection[Foundation Systems]{Foundation Systems for Formal Specification and Verification}

Having transferred from databases to the application of formal methods to the development of secure systems my interest in foundation systems developed further.

There were first of all some instructive errors (see: \cite{jones86a,jones86b}).

Consistency strength as a key element in a foundation system,

\subsection[Well-Founded Semantics]{Semantics for Well-Founded Specification Languages}

Semantics for VDM\index{VDM} and Z\index{Z}.
Alternate foundational ontologies.

\subsection{Set Theories with a Universal Set}
Set theories with a Universal Set.

\chapter{Philosophy}

\chapter{Other Matters}


\part{As it Went}

\chapter{Early Years: 1948-1967}

\section{A Thought at Cambridge - 1966-67}

I was at Cambridge as an undergraduate reading Mechanical Sciences for just one year.
I didn't much care for the engineering but was taken with the IBM-1130 which they had installed in the Engineering Laboratories, partly for teaching undergraduates.

We were taught how to program in FORTRAN II, and prohibited from using assembly language (this was not a multiprogramming system and there was no limit to the damage a user program might do).
Somehow I figured out how to assemble a program into a FORTRAN array and then contrive (by using negative indexing in the array I think) to overwrite the FORTRAN program so that on exit it jumped into the array.
By this means I was able to try out interesting things which couldn't be done in FORTRAN, like getting red ink on the printer.
This was a paper tape machine and the program and data tapes were punched up using these things called ``flexowriters''.
Sometimes they were in such great demand that the computer itself would be sitting idle but all the flexowriters were in use.
So I wrote a FORTRAN program which turned the computer into a flexowriter.
However, since it was in FORTRAN when a tape was read, tabs were converted into spaces, but when a tape was punched the reverse effect did not take place.
When the computer was used as a paper tape editor, the paper tape got much larger, because all the tabs were converted into multiple spaces.

This provoked the first thoughts which I can recall about AI.
The editor was introducing redundancy, and I thought about eliminating redundancy.
I decided that if you could write a program which would take a large set of true statements and code the documents up in the least redundant way possible, so that the paper tapes seemed completely random, then by feeding random numbers into the inverse of the compression function, you would get out random true statements.

OK so this is a completely and hopelessly naff idea, I never tried to implement it, and it obviously wouldn't work.
Fortunately, I don't think it is representative of the character of my subsequent thoughts about AI, though thoughts about AI don't usually actually work, some of them are not quite so easily dismissed.

\chapter{At Work: 1967-1972}

\section{Philosophy of Life c1968-1970}

After leaving Cambridge and taking a job in Computing I found myself living in bedsits, in a strange place, with raging hormones, wondering why I got depressed and what to do about it.
This provoked a certain about of thought, of the kind which might be called ``philosophy of life'', or, {\it how to be}.

I went through a number (can't remember what number) of ``philosophies'' to which I gave names, and tried each for size before deserting it for the next.
The two I remember were the early ``rationalism'' which was an attempt to persuade myself that I should really do whatever it was that I had rationally decided would be best for me, and anarchism which was more of less a capitulation to the fact that I just don't do that.
Of course, anarchism is really the name of a political philosophy, but I used it (at least in my head) for a personal philosophy for a long time, and felt that there should be some fit between the personal and the political (the way you treat yourself, the way you treat others, and the way you think the state should treat people ought to fit together).

Anyway, this was the period in my life when I worried about this kind of thing, and then settled into and gradually forgot this thing that I thought of as anarchism, and which amounted to self-trust.
Just do what you do.

I did have many subsequent periods of soul searching, even to the point of suicidal depression.
But actually, they were mostly provoked by women, or the lack of them, and they didn't result in this same kind of ``philosophical'' thinking.
There was one more period of crisis which seems to me similar, and which I hope has just now drawn to a close.

\section{Russell and Logical Positivism - 1970-71?}

Eventually I went back to University and read joint mathematics and philosophy.
I did do some philosophical reading before that, and must have thought about this, but I have no recollection of these thoughts.
However, many years later I noticed that I had a philosophical position which seemed to be at odds with contemporary philosophers, and eventually concluded that it must have been largely formed by during the philosophical reading which preceded my return to University.
Of these the most significant readings I guess were Bertrand Russell, notably his history of Western Philosophy and his Introduction to Mathematical Philosophy, and Ayer's Language Truth and Logic.

\section{Foundations - 1971-2}

I spent five years in the computer industry before returning to University at Keele.
Toward the end of this period, I was retrieving papers by Church and Turing and others from via the company library in company time, and had some story about why this was relevant to my work which I don't think my boss (John Dawes) actually believed, though he didn't seriously object.
I can't remember what the story was, but it must have been pretty tenuous.
I had a think about floating point representation of real numbers, and thought at that time that it was a mistake that the theory of real numbers was not more constructive, blaming that for the use of floating point numbers for reals.
This must have come from Turings stuff about computable reals.

Since than I have always had a leaning toward better computational support for computable reals, but have never actually done anything significant about it.
These days it seems to me essential for the application of proof technology in building correct software for doing mathematical analysis.

\chapter{At University: 1972-1977}

\section{Foundations - 1972-5}

I arrived at Keele University already knowing something about formal mathematical reasoning, already with some kind of a bee in my bonnet about proof.
I know this because when I started doing mathematics properly (in the second year, since Keele had a foundation year before one could specialise), I did some of my exercises with fairly detailed formal-ish proofs.
Presumably this didn't last very long since there isn't much university mathematics one can hope to do that way, but my notes are now lost.

The first ideas I can recollect about the foundations of mathematics concerned Principia Mathematica.
I acquired the paperback of the Principia ``to *56'' and devoted some time to it.
It seemed to me a bit unclear and I thought it would be useful to define the logical system formally.
This was my first grapple with a kind of regress problem, and I decided that one should define the logical system using the very simplest language in which it could possibly be done, so that there was no real danger of that language being misunderstood.

That language seemed to me to be the language of post productions, of which I had learnt from Marvin Minsky's book ``Finite and Infinite Machines''.
For a while this was to be a dissertation for the philosophy department, but eventually I decided that it wasn't going to work out.

It was purely about syntax, and at this time I had no idea of semantics.
It doesn't any longer seem to me problematic to define the syntax as precisely as we wish, and its clear that the way to do that is not to chose the simplest possible notation for the purpose.

By the time I got to the end of the degree I had some definite ideas for research relating to the foundations of mathematics.
One was that I wanted to work on computerised formalisation of mathematics.
The kind of thing which was actually being done round about then in Edinburgh (except that their logic LCF was a bit to weak, though it would eventually be traded up to HOL\index{HOL}).
The other was to work on support for use of computable reals.
I don't believe that I had any original ideas on how these were to be done.

\section{Philosophy - 1973-5}

I did half a degree in philosophy, part of which was mathematical logic and the philosophy of mathematics.
I was not a good philosophy student, I was more interested in working things out for myself than in studying the works of other philosophers, and quite unsuited to the latter.
In the examinations one was expected I think even when asked a question about some particular problem without reference to any other philosopher, to show some knowledge of what other philosophers had said about the problem.
I think the model answers were demonstrations of scholarly knowledge peppered with small amounts of original analysis.
However, I didn't have the scholarly knowledge and I was pretty slow.
I liked to think about the problem posed and come up with a position on the spot, with no more reference to other philosophers than could be avoided.

I recall an obscure conversation with Jonathan Dancy on a bus during the period after taking the exams but before getting the result.
He said that for the philosophy I had been on the list of those considered for viva (meaning that I was on a borderline), but that they had decided against giving me one.
He said he had argued that the style of the essays should be taken into account, evidently he thought this a strong point, but there just wasn't enough {\it content} so they wouldn't have it.
I think Dancy thought he was telling me something from which I could conclude that I would be getting a first overall (to get a first in a joint honours you needed a first in one subject but only an upper second in the other) and he must have heard enough about my maths to think that I would have no doubt about a first in mathematics.

I digress, mainly to suggest that {\it originality} is in my blood.
Not in the laudatory sense of having meritorious ideas which no-one else has had.
In the sense of ignoring what has been done before, and thinking things out from scratch.
Creating some solution of my own, not necessarily new to the world, not necessarily of outstanding merit.
Originality as a bad habit, or a pathology, like autism.

As far as ideas from this time which still have something for me, I can remember the following:

\begin{description}

\item[words]
I didn't like the idea that philosophy was just about words.
In fact I didn't like the idea that philosophy was about words at all!
I even made an attempt to articulate methods for disengaging from the meaning of words, this in an essay in political philosophy.
I was particularly unimpressed by the philosophy of Wittgenstein, both the early and the late.

\item[politics]
I wanted to talk about anarchism, as a political doctrine.
I don't think I got anywhere but the attempt was also an early attempt to wriggle out of entanglement with words.

\item[ethics]
Possibly my worst essay was on ethics, considering the question ``Is morality necessary''.
In fact this was bound up with ``anarchism'' as I construed it.

\end{description}

\chapter{Back at Work: 1977-1998}

\section{Expert Systems in Decision Support - 1982}\label{1982}

I wrote this paper while working on microcode architectures for mainframes (CME), when I had decided it was time to move on.
The paper was never published, never even submitted for publication so far as I recall, but did help to get me a job in ICL working with expert systems.

\section{Knowledge Bases - 1985}

In 1985 I moved from expert systems (not impressed) to database management software.
While working professionally on relational database software I thought privately about knowledge bases.

This topic combines an interest in AI with one in foundations, which is typical of my interest.
I thought up an architecture and made some steps toward implementing it on an ICL PC (with a Z80 processor).The starting point for this is the idea that intelligence involves reflexive reasoning, so an artificial intelligence should be a self applicative piece of software.
To make it easy to reason about itself, this should be in a nice functional language, and at this time the state of the art in implementation of functional programming languages (embodied in Miranda) used combinators.
So the knowledge base is a combinator, part of which is a program which takes the knowledge base (including itself of course), as an argument, and computes a new value for the knowledge base,
Actually, it was to be a worm, not overwriting the database but continually adding new versions of it to a list, the space cost being mitigated by the sharing one gets for free in functional systems.

The foundational element comes in when you start considering the combinator which is the knowledge base not as a data structure but as a proposition, and want to take the computations which update it not as mere computations but as inferences.
So what kind of a logic do we want for this?
Well, the self applicative bit made me think that this had to be a type-free system.
It was going to be reasoning about a function which was to be applied to something which contained itself.
The obvious answer to this need seemed to be combinatory logic, about which I knew next to nothing.
So that's how I got interested in combinatory logic, and how I resumed thinking about the foundations of mathematics.

Somewhere in this period I went to a workshop at Appin in Scotland which was sponsored by ICL and was on persistent databases, so I wrote this paper:
``Persistent Applicative Heaps and Knowledge Bases'', as my offering (which was a bit irrelevant since I was really there because ICL was paying).
I also got a slot to talk.
After hearing the tenor of the papers I sensed a consensus with which I disagreed, which was that languages with persistent storage had to have dynamic type systems.
I decided not to talk about my paper at all, but instead to argue, on the fly as it were, the falsity of this common ground.
The argument was along the following lines.
We would expect none of these programs to raise type errors during execution.
We would expect this to be provable.
It must therefore be statically determinable, and this static determination could be built into a type system.
Of course, I don't really remember the details of my argument, my memory isn't that good.
But I still think there is a case that this is possible.
Possibly not with a decidable type system, but the main point was not to advocate a static type system but simply to challenge the assumption that a dynamic type system was essential.
My talk caused an uproar and I loved it.
Like so many conferences and workshops in computing the average talk went by with very little if any comment and no serious discussion.
I livened up the proceedings considerably.

Some of the academics talked sympathetically to me afterwards, clearly thinking I had been through a terrible ordeal, which I must be regretting.
I wasn't, I was very pleased with it all.
Not that the discussion was wonderful, I only had something like twenty minutes, and the discussion had to be curtailed.
But my presentation on was quite brief, and it caused instant uproar.
In fact, discussion had to be curtailed simply so that I could complete the presentation of my argument (most people wanted to object without actually hearing the argument).

Its not that I crave infamy, but it was all pretty stodgy up 'til then.

\section{Creative Foundations - 1986}

Having decided that combinatory logic was the appropriate foundation system to use in a ``knowledge base'' I was disappointed to discover that combinatory logics, when they were consistent were weak, I spent
some time trying to devise a consistent and strong illative combinatory logic.

The first red herring on this trail was the idea that the coding of recursive functions in combinatory logic could be exploited to yield a strong system.
No-one understood the idea, not even well enough to tell me why it was a duff one.
I wrote two essays before I figured out what the problem was.
The first was ``Logical Foundations and Formal Verification'' which was presented at an Ada Verification workshop in South Carolina, but possibly not included in the proceedings.
The second was ``Creative Foundations for Program Verification'' which I thought a really neat paper, and was published in the proceedings of Milcomp86 a military computing show with a pretence at being a conference.

It wasn't till I had some conversations with Peter Aczel and tried to explain it to him that the penny dropped for me.
He asked me what was the proof theoretic strength of the system, and I said it didn't really have one.
He also spent some time explaining proof theoretic strength to me, and explaining some of his systems to me.
However, I was looking for something particular, and he took offence I think that I said that his systems were not quite what I was looking for.
I think he was thinking of me as if I was one of his students, and students it seems don't do that kind of thing.
I think they are supposed to be more like disciples.

This was for me I believe a very instructive error, though its still hard to explain exactly what was the error (mainly because its hard to make clear the rationale which made me think that the ``creative theory'' was a foundation system.

After this I continued to think about reflexive foundation systems, particularly in combinatory logic, and went down quite a few blind alleys.
This only stopped when I got deeper into well-founded set theories and concluded that what they offered was really the neatest kernel of the foundational problem, and that reflexivity if thought desirable could be done somewhere other than the foundations.

I now think that reflexive foundations if we really want them may be best approached via set theories with a universal set.
These contain well-founded sets as well as ill-founded ones of course, and it seems that you can make them as strong as you like by talking about the well-founded part.

\section{Well Founded Foundations 1988}

In 1988 I was for a while assigned as ICL's representative (and the secretary) on the VDM\index{VDM} standardisation panel.
VDM\index{VDM} was a formal specification language with a kind of domain theoretic semantics, which came from Brian Monahan.
I had an agenda which I had been pressing for a couple of years which said that specification languages should be foundation systems, and that ideally foundation systems should be reflexive.
Well VDM\index{VDM} and Z\index{Z} were both well founded specification languages, but the former was not really much of a foundation system so far as I could see.
Ontologically it did have a hierarchy of types progressing through a function space constructor.
But the function spaces were spaces of continuous functions, good enough for modelling higher order computable functions, but not for mimicking higher order logic.
So VDM\index{VDM} looked to me rather like a first order logic, an elaborate version of LCF, to weak to do serious mathematics.
I don't know whether I was right about his, possibly not.

Anyway the problem of finding a foundation system which was suitable both for Z\index{Z} and for a strong VDM\index{VDM} seemed interesting.
It seemed to me that the key problems to be solved relative to the known well-founded foundations such as ZFC and HOL\index{HOL}, were the need for polymorphism and the need for structuring.
Structuring relates both to local definitions (which don't work in HOL\index{HOL} in the way that they do in ML and in which one needs them to work in any serious specification language) and to support for modularity.

The game plan for this work was to do the work from semantic models in Cambridge HOL\index{HOL!Cambridge}.
First axiomatise ZFC in HOL\index{HOL} (not worrying about the fact that the result would be strictly stronger than the first order version).
This gives a rich ontology within which ontologies of the desired systems could be constructed.
Then three theories were to be constructed in turn.
The first was to change the ontology from an ontology of sets to one of functions.
This is done, roughly, by taking the heriditarily functional sets from the domain of ZFC in HOL\index{HOL}.
There were some tweaks to this to make the resulting theory of pure functions extensional.
The way I worked with this was to define a new type based on these pure functions, define various operations over these functions, think up an axiomatisation of the theory and prove the ``axioms''.
The adequacy of the ``axioms'' could then be established by doing the reverse construction, going back to the theory of sets using a ``hereditarily'' subset of the pure functions which mimics the sets of ZFC in HOL\index{HOL}.
I didn't to the last bit, and I didn't complete the proofs of the proposed axioms.

The next two theories were the theory of polymorphic functions, and that of ``structured functions''.
A polymorphic function was a function over the pure functions, in which the functions in the domain are interpreted as assignments of types to type variables.
So they are families of functions indexed by ``type'' assignments.
I should mention that this is all really ``type free''.
I'm heading for something like an illative combinatory logic except that it isn't really reflexive.

There is just an illusion of reflexiveness of the kind you get in a polymorphic type system.
To explain this another way, the idea is like HOL\index{HOL} without the types.
Its like HOL\index{HOL} in that everything is in the semantic domains, all the logical connectives and quantifiers, unlike first order logic in which there is a distinction between terms and formulae and only the terms denote values in the semantic domain (the domain of an interpretation).
To do this in HOL\index{HOL} without running into consistency problems you have the type system.
In the proposed system you don't actually have types, consistency is realised through well-foundedness.
There is no type system, but the well-foundedness gives you the same problems that you get with a type system, which is why we still have polymorphism.
The problems here are the problem of the non-existence of functios you would like to have, like the identity function, the equality predicate, and of course the quantifiers.
So we settle instead for families of functions.
e.g. the family of identity functions restricted to some domain.
The family of restricted quantifiers.

The ``structured functions'' where polymorphic functions defined in contexts where various identifiers are thought of as external, perhaps because they are defined in some other module, perhaps because they are defined in some local definition.
The value of the polymorphic function depends upon the values of some identifiers in relevant context.
The context is again represented by a function, which is understood as a map from names to values.

To make this work you need some constraints on the function spaces which are used to represent the polymorphic and structured functions.
I didn't progress this work far enough to be sure what the necessary conditions are.

This work petered out for several reasons, firstly there was an influx of theoreticians from Denmark into the standardisation process sponsored by Dines Bjorner, who had there own strong sense of what should be done with the semantics of VDM\index{VDM} which was quite different from my ideas.
Second I was taken off the VDM\index{VDM} work and John Dawes was put back (for reasons nothing to do with me).
Thirdly, doing all this formally would have been too large an undertaking.
This was one of a long series of formal experiments from which a little is learnt but which could not hope to be completed with the kind of effort available.

The idea of making novel well-founded foundation systems by filtering the sets of ZFC was however one which I think is quite fun.
I also have tinkered with category theoretic foundations in the same vein.

\section{Stealing Proof Theoretic Strength}

Before I gave up on non-well-founded foundation systems the method I was thinking in terms of stealing proof theoretic strength from ZFC.
The idea is to start with a weak illative combinatory logic which includes a weak arithmetic with standard semantics (i.e. the numbers really are the natural numbers).
Then you strengthen it by adding more arithmetic truths, one obvious contender is the one which says that ZFC is consistent.
There is a result of Feferman's which isn't proved in this context but which one could hope to make work in this context.
It says roughly that if you add to Robinson's Q an axiom stating the consistency of some other first order theory then that theory becomes interpretable in the extension to Q.
Consequently adding the consistency claim gives you all the proof theoretic strength of the system you assert consistent (plus a bit).

\section{Embedding Z in HOL\index{HOL} - 1990-1993}

This was one of the few good ideas which I had which actually got implemented because I got a team at ICL to do it!
The result was ProofPower\index{ProofPower}.
Not a startling idea, obvious but effective.

The whole story is of course rather complicated some of it is covered in \cite{rbjt014, rbjt015}.

\chapter{Independence: 1998-}

\section{Philosophical Revival - 1994 onwards}

In 1994 I got my first web site.
After struggling some time to get web access through ICL, it eventually happened, and with the web access came, through a backdoor an external website.
In the Finnish arm of ICL I found an outfit which had an externally visible website, contrary I think to ICL policy on firewalls, and which was willing to cross mount web-space on one of our Sun workstations, making it externally visible.

The primary purpose of this was to put up information about ProofPower\index{ProofPower}, but I also put up my own personal web-space, which was philosophical.
I did some experiments with converting philosophical classics into hypertext, of which the modern ancestors (actually not a lot different) are now on rbjones.com.
The hypertext classics are not significant.
What was significant was the gradual revival of my philosophical thinking, beyond the scope of the foundations of mathematics.
This was a very tentative process, evident initially only in the few extended periods away from work (i.e. vacations).
Its possible to pursue technical problems in private time (though I can't say the results are spectacular, this is what I have done all my life), but it was much harder to take up philosophy on that basis.
I don't actually know when this started, but it was very roughly at the time when I started experimenting with the web.

At the time I got into hypermedia I was very taken by it and utterly convinced that it was the right medium for me.
It has taken very many years for me gradually to come to the conclusion that I was wrong.

1994 was also the beginning of the end for the formal methods team in ICL.
CESG\index{CESG} was running into problems with some of its high security developments and soon there was to be a change of policy and they would simply stop funding this kind of R\&D.
The hole was partly plugged by safety critical applications but there was not enough of this to keep the formal methods unit going.
The honeymoon period for the Defence Technology Centre was past, and the focus was once more on profits. not research.

In 1994 I volunteered for other kinds of work, in the hope that a slimmed down formal methods team would have have a better chance of survival, and believing that such a slimmed unit would have to be headed by someone less senior than myself to be cost effective.
For me that was the end of interesting work at ICL, and I decided that on my fiftieth birthday I would go it alone in the hope of finding something more congenial.
I then had the sense that I wanted to write, but I had a pretty fluid sense of what I wanted to write.
Philosophy, software, ...
What I really wanted to do was to find a way to earn a living from creative activities.
In particular from doing what I though a good idea rather than what someone else thought a good idea.
I had this sense of being handicapped throughout life by this dichotomy between what I was paid to do at work and the more interesting things which I attempted in my own time.

Attempting this at the age of fifty had the additional advantage that I could fall back on a pension from ICL.
I did this rather faster than I expected to because the year following ICL changed the rules in a way which gave me an incentive to take the pension sooner rather than later.
Gradually the revenues that I was then able to obtain, first from contracts with ICL and later with contracts from Lemma1 has petered out so that we now live on two pensions between us.

From when I left ICL to the present moment is now a period of seven years in which I have failed to get things in order.
I have had what I now think of as ``philosophers block'' (if one can be said to have that without any sign of ever having a philosophical capability which might have been blocked), the beginnings of which were almost immediate, and which may just possibly be now disappearing.
Notwithstanding this ``block'' there has been quite a bit of intellectual and philosophical development during that period which is hard to trace chronologically.

For me it is important however, to exhume at least some of this stuff, so I shall attempt it in the following without worrying to much about the chronology.

\section{Artificial Intelligence}

A major theme in this period has lain on the boundaries of artificial intelligence and formal mathematics.
This began with something I called ``Engineering Logic'' and progressed first though ``The Global Super-brain'' to ``X-Logic'', all different takes on essentially the same theme, and all engineering projects bound up with philosophy.

I was not attempting to do philosophy when I started ``Engineering Logic'' but I was then seeking to put forward an architecture for, and an approach toward artificial intelligence which was primarily rooted in producing software first which would not be intelligent, in framework which might eventually lead to intelligent software.
The rationale for this framework could not be explained without getting into philosophy, and the philosophy proved awkward to explain, because it was old fashioned philosophy, closer to logical positivism than to anything which philosophers did at the time.
It was based on tenets which seemed generally discredited (especially in the marginal world of on-line philosophising).

I'm going to say what I can about the engineering side of this in this section, and deal with the more philosophical aspects later.

There are three clearly identifiable stages in my thinking on this topic, which I give the tags:

\begin{itemize}
\item Engineering Logic
\item Global Super-brain
\item X-Logic
\end{itemize}

Each of these stages is best thought of as adding something to what went before, rather than as replacing its predecessors.
All of this was conceived of at the time as something like software engineering.
I was aware that the rationale for this depended upon certain philosophical doctrines, which when I first started I did not think of as novel, and was only dimly aware of as controversial.
So my first inclinations philosophically were perhaps just to run of a position statement which gave the relevant philosophical background.
However, I became increasingly aware of the distance between the philosophy on which these architectures are based and the kind of thing which contemporary analytic philosophers are saying.
Not just that they were saying different things, but that this whole way of thinking was thought of as having been discredited.
At first I didn't even know where the philosophical ideas had come from, but eventually I figured out that it was probably predominantly from Russell and Ayer, modified by my own knowledge and experience of modern logic and computing.

Now I am returning to this, thinking of it as philosophy.

The ideas started with an architectural conception which corresponded to (what I consider) the obvious way of working out from the formalisation of logic, and there is a picture illustrating the scheme at 

\subsection{Engineering Logic}

I've chosen this tag here, even though it was just one element of the ``topography for AI'' which I sketched out, because it was for me the most important element, because I sought to advance the aspects of it which could be regarded as realisable engineering, rather than speculative research.
So ``engineering logic'' was the limits of engineering logic based automation of ({\it inter alia} but importantly) engineering design.

The idea of engineering logic is simply the idea of automating engineering design through formalisation.
This kind of work, not itself conceived of as artificial intelligence was intended to form a basis on which intelligent capabilities might later be established, but which was {\it just} a special kind of software engineering, in which logic is applied first to the formalisation and automation of mathematics and thence to science and ultimately to engineering.

Underlying this of course is a story about how one ideally can formalise science and engineering which is in spirit if not in detail similar to what logical positivists such as Carnap were engaged in, inspired to a significant extent by the prior work of Russell in the formalisation of mathematics in Principia Mathematica.
Intelligent logic is intended as an ``intelligent'' overlay on engineering logic.

\subsection{Global Super-brain}

This stage in thinking was to some extent simply connecting in with a popular theme, to some extent it was rethinking in terms of global networks.
There is a distinction in the topography of the previous scheme where one crosses the boundary from the domain of the analytic, and hence beyond the scope of logic.
This is marked by a change of colour from shades of blue to shades of green, and though the topology shows some subdivision of the green zone I didn't do any significant thinking about this side of things (not even by my own lights).

In the ``super-brain'' stage that distinction is made more prominent, the two parts are called the {\it analytic} and the {\it holistic} super-brains and the balance gets a bit better (in terms of how much I thought about these different sides of things).

The analytic super-brain was a networked version of intelligent engineering and intelligent logic.
The holistic super-brain was the use of global networks to transform the operation of markets.
An important part of this was its intended role in achieving greater transparency in the total costs and effects of products and companies upon the issues of greatest importance to us all.
This is also about the future of democratic institutions, capitalism and free markets.
So in a more concrete way the holistic side connects with the more socially oriented side of philosophy, including ethics and political and economic philosophy.

\subsection{X-Logic}

This last stage in the evolution of these ideas is where I connect the Super-brain with XML.

\section{Formal Philosophy}

Now alongside the line of thought about engineering knowledge processing artifacts, I had a more self consciously philosophical line of thinking.
This was mainly about the role of formal logic in philosophy, which following Russell, and Carnap I considered crucial.

\section{The Set/Class distinction, October 2005}\label{October2005}

The distinction between sets and classes\index{class} has I think always struck me as being arbitrary, but it was only in 2005 while engaged in a discussion on the FOM mailing list\index{FOM!mailing list} that I found an argument to that effect which I considered convincing.

The argument flows from a simplification of the description of the intended domain of well-founded set theory which goes under the headings ``the iterative conception of set'' or ``The Cumulative Hierarchy''.
There had been attempts (for example, by Dana Scott and George Boolos) to further explicate this account of set theory using some theory which talked about the stages in the iteration which supposedly lead to the cumulative hierarchy\index{hierarchy!cumulative}.
I found these unconvincing, because these theories seemed to me more complicated than the theory which they sought to explain.

Furthermore, it had begun to dawn on me, that the effect of the classic descriptions should be to simply to make more clear to concept of ``pure, well-founded set'' which one would expect to be co-extensive with the completed cumulative hierarchy.
Thinking therefore of the best way to define the concept of a pure well-founded set, I came up with the following definition by transfinite recursion (or induction):

\begin{quote}
A set is any definite collection of sets.
\end{quote}

The reading of this as an inductive definition requires that nothing counts as a set unless its sethood is justified (possibly by some highly infinitary ``proof'' by this definition alone.

Thus one may begin with the empty set, which by any account is definite and which, whatever a set may be, contains nothing else, and from there, after the manner of the interative conception but with more freedom as to order, one may see that subject to some discussion of the meaning of the term {\it definite}, any pure well-founded set may be reached.

The term definite here is essential to the definition, for if it is removed the definition engenders contradiction, for the extension of the concept thus defined would be a set, and could be shown both to be and not to be, well-founded.

An inductive definition of a concept yields (or is more fully explicated by) an induction principle which may be used to prove results true of all that falls under the defined concept.

In this case the relevant principle of induction is $\in-$ (sometimes {\it noetherian}) induction:

\begin{quote}
If some property P is:
\begin{itemize}
\item true of the empty set (the base case), and if
\item whenever it is true of every member of a sets, then it is true of the set, then
\item P is true of every set
\end{itemize}
\end{quote}

Using this principle it can be shown that the sets thus defined are:

\begin{compactdesc}
\item[pure] i.e. they are sets all the way down, constructed ultimately only from the empty set.
\item[well-founded] i.e. there are no infinite descending membership chains, if you keep going down then eventually you will get stuck in the empty set.
\end{compactdesc}

It also follows directly from the definition that every definite collection of sets is a set, and hence that if there are ultimate collections of sets (which are not themselves members of a set) then these collections are not definite.

Provided the requirement of definiteness is in there, the line of reasoning which might otherwise have obtained a contradiction suffices only to show that the collection of all sets cannot be definite.
But the requirement can be very weak indeed.

The effect of this ``idea'' is to give a more concise presentation to the iterative conception of set, in which some inessential features are omitted (in particular the iterative conception proceeds in stages according effectively to the {\it rank}\index{rank!of a set} of the sets (which is the number of steps or stages which are necessary in its construction), whereas the minimal ordering is in fact that partial ordering which is the transitive closure of the membership relationship.
The concision of the definition, makes more transparent the consequences of the definition, in particular the incoherence in the case that the constraint of ``definiteness'' is omitted or made trivial.

In effect, the whole process as described in the iterative conception cannot be completed, as a matter of logical necessity, and a choice must be made, which we might consider to be arbitrary, about where the process is to come to an end.

\section{The Set/Class distinction, January 2010}

As a result of futher provocation on the FOM mailing list\index{FOM!mailing list} in January 2010, my understanding of the relationship between the axiom of replacement and the ``limitation of size''\index{size!limitation of} principle was improved
\footnote{Admittedly the connection is pretty obvious, but I think the explanation which then occurred to me is worthwhile, and actually explains a bit more than that connection.}%

Let us consider, in the context of the definition of well-founded set\index{set!well-founded} in Section \ref{October2005}, the alternative ways of defining ``definite'' and the conceptions of set which arise.

First note that the role of the concept ``definite'' here is to separate sets from classes\index{class}.
A collection of well-founded sets is a set if it is definite, and otherwise its not a set, so we'll call it a class.
It may be helpful to switch terminology at this point, to emphasise that what's really happening here is that we are making a more-or-less arbitrary choice about when to stop making new sets.
So instead of definite we will use the term ``small'', which is familiar to set theorist for making exactly this distinction.

Our definition (of pure well-founded set) becomes:

\begin{quote}
A set is any small collection of sets.
\end{quote}

To make this definite we have to decide where to draw the line, i.e. when to stop making sets.
This we do by deciding what {\it small}\index{small} is to mean.

Its worth distinguishing two different things which {\it small} might be a property of.
The first connects us directly with the iterative conception of set, and is the {\it rank}\index{rank} of the collection, the number of stages involved in its construction.
If we construe {\it small} as a property of the rank of the set, then we are stating the number of stages in the iterative conception that are to be completed.

The most obvious alternative to this is to take ``small'' literally, as referring to the cardinality\index{cardinality} of the set.

First order set theories do not usually place upper bounds on the size of the universe, either in terms of rank or cardinality.
The effect of the axioms is to place lower bounds, their purpose being to ensure that there are sufficient sets for the purposes of mathematics.
However, the convenience of the theory depends not only on their being sufficient sets, but also on the universe of sets having strong enought closure properties, so that various kinds of construction can be relied upon always to yield sets.

If our definition of well-founded set is taken as determining a collection of models for set theory, and the collection is to be determined by chosing a particular interpretation of ``small'' the interpretations begin those obtained for values at least that large, then the kinds of theory we get are subtantially different according to whether ``small'' is taken to be a property of rank or of size.

If of rank, then by varying the value of small we can get theories of arbitrarily high strength, but which all have weak closure properties.
The universe of sets will not even be known to be closed under the pair constructor.
Chosing a large value for ``small'' will result in it being provable that there are sets with good closure properties, but the universe will not have these properties.

On the other hand, if small is a measure of size, then we get replacement, and with it other closure properties. 

\appendix

\chapter{Research Topics}\label{ResearchTopics}

Here are specifically the research topics I considered for a PhD dissertation.

\section{Keele}\label{KeeleProposal}

\subsection{Background}

The following is a transcription of a note that I made, probably in early 1976, while an undergraduate at Keele University looking for a University to do research towards a PhD.

The professor of mathematics at that time gave me the advice (which now seems to me rather bad advice) that I should avoid doing an MSc and get straight into working on a PhD.
So I wrote down the kinds of things I wanted to do research in and sent the note to some Universities to ask them whether I might be able to do that kind of research with them.
I think it just went to Oxford and Manchester at first, then Peter Aczel at Manchester said that the Computer Science department at Warwick would be a good place to do that kind of work and suggested I talk to David Park.
The copy to Oxford was addressed to Dana Scott, with a covering letter explaining that I didn't want to do the MSc but was looking to get straight into research.
He replied very concisely to the effect that he was not able to help me.
Later I got a more conciliatory response from Robin Gandy, who invited me to go and talk to him, but by that time I had already agreed to go to Warwick.
For more than one reason this was unfortunate, and I would have been much better doing the MSc at Oxford.

\subsection[Possible Topics for Research]{Possible Topics for Research in Mathematical Logic, Foundations of Mathematics, and Computer Science}

\subsubsection{High Level Logics}

My most consuming ambition in Mathematical Logic concerns the development of ``High Level'' formal systems of logic. By ``High Level'' I do not mean ``Higher Order'', I mean something analogous to the use in ``Higher Level Programming Language''.

The purpose of a high level programming language might be:

\begin{description}
\item[(A)]	To facilitate the precise formulation of algorithms (possibly in some special problem area, but not necessarily)
\item[(B)]	To enable people to make use of the facilities of a complex computer system without needing first to acquire detailed knowledge of the workings of the machine and its software.
\end{description}

The "sine-qua-non" of a programming language is translatability.

By analogy the purpose of a high level formal system of logic might be:
\begin{description}
\item[(A)]	To facilitate production of completely formal proofs of mathematical theorems (possibly in some particular branch of mathematics but not necessarily)
\item[(B)]	To enable mathematicians to produce completely formal mathematical proofs without needing to master all the intricacies of mathematical logic and set theory.
\end{description}

The ``sine-qua-non'' of a formal system of logic is that there be an effective method for verifying proofs (not to mention that it should be consistent!)

The development of such languages is a prerequisite of any extensive practical use of computers for proof finding.

\subsubsection{The Foundations of Mathematics}

Concurrently with work on the above some work on the foundations of mathematics would be necessary.
I could of course stick to formalising versions of the tried and tested axiomatisations of set theory, but I have traces of intuitionism lurking within me and would like to investigate other possibilities.

One possibility which interests me is that of basing mathematics on strings rather than sets. Strings however, unless infinite strings were admitted would serve only for a version of constructive analysis.

\subsubsection{Computing with Reals}

The principle source of my intuitionist leanings was the perception several years ago of the disparity between theory as embodied by real analysis and practice found in numerical analysis.
When I was familiar with the way mathematicians use numbers on computers the beautiful completeness and simplicity of the real number system seemed a rather shallow pretence.
In practice all mathematical computations were approximated using using a subset of the rational numbers.
What seemed worse than the understandable failure of numerical analysts to use real numbers was their failure to exploit the full range of computable numbers.
It seemed to me that it was just because mathematicians allowed themselves the pretence of real numbers in their theory that they failed in practice to fully exploit the computational capabilities of their machines.
It is well known that a wide range of functions are computable, it would be a good thing if computers were could be organised so that any ``computable'' function can in practice and without much difficulty be computed within any specified degree of accuracy (space and time permitting).
This is not presently the case.
A typical example of the deficiencies of the present techniques is a problem as simple as the inversion of a matrix.
Any non-singular matrix with rational entries has a computable inverse and so one could reasonably expect that any substantial computing complex would have facilities to compute any such matrix to any prescribed accuracy.
In fact, I doubt that there is any complex which will guarantee to compute any inverse to within 1\% accuracy (subject to time and space limitations only).
In practice the accuracy of the inversion will depend upon the size of the matrix, and more importantly, on how close to being singular it is.
In general we can only guess at how accurate the inversion is.

It would be interesting and valuable (and not trivial) to investigate the software and hardware necessary to render practicable the computation of any ``computable'' function to an arbitrary number of places. On the more purely mathematical side there is the problem of sorting out which of the problems of analysis known to have solutions actually have computable solutions, and the problem of arriving at practical algorithms for obtaining those that are computable. It must be emphasised that the algorithms here referred to are quite different in character to the algorithms used in numerical analysis at present, which are generally algorithms for approximating solutions. There is no way of obtaining solutions of arbitrary and known precision without abandoning present methods of representing numbers in machine memories.

\section{Other Proposals}

Many years later, during my period of formal methods work with ICL, I considered again the possibility of getting a PhD.
I talked with Keith Hanna about the possibility of doing a PhD at Kent under his supervision.
However, I wanted to do a purely theoretical dissertation, on some logical foundation system,
Keith had already adapted other logical foundation systems for use in his Veritas hardware verification tool, so he was not uncomfortable decising new logical systems, but the main point of his work was the development and application of tools for the verification of digital hardware, and he was not willing to supervise a more theoretical foundational enterprise (and may have doubted that the department of Electrical Engineering at Kent would support him).

Since my interest at that time was in foundation systems which were similar to Combinatory Logic, I also talked to Roger Hindley, who was possibly embarassed to have to decline my suggestion that he supervise me in a PhD.
In retrospect I imagine that I can see why he did.

I also enquired with the Open University, who said that they had no-one with sufficient competence in the research area I proposed, but that I could do a PhD with them if I could find a suitable external supervisor.
I soon realised that the prospects of ever finding one were slender, but only later realised how unlikely success for me in such a project would have been even if I had found a supervisor.

None of the details of my proposals at this time has survived, but I am pretty sure that during this period my interest would have been in devising new logical foundation systems more suitable for use in mechanised mathematics.
The single most distinctive feature I was then interested in was what I called {\it reflexiveness} meaning the possibility of functions falling within their own domain, in set theoretic terms, non-well-foundedness.

%\addcontentsline{toc}{chapter}{Glossary}
\chapter{Glossary}\label{glossary}

\begin{description}
\item[CESG]{\index{CESG}} Communications and Electronic Security Group (a part of GCHQ).
\item[class]{\index{class}} A large collection.
\item[GCHQ]{\index{GCHQ}} Government Communications Headquarters, UK.
\item[HOL]{\index{HOL}} Higher Order Logic, a specification language, a formal deductive logic, an interactive proof tool.
\item[NuPrl]{\index{NuPrl}} A tool supporting proof development in a ``New Proof/Program Refinement Logic'', which is a constructive type theory.
\item[PRG]{\index{PRG}} The Programming Research Group, at the University of Oxford.
\item[set]{\index{set}} A small collection.
\end{description}

\backmatter

\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{alpha}
\bibliography{rbj}

\addcontentsline{toc}{chapter}{Index}\label{index}
\twocolumn[]
{\small\printindex}

\end{document}
