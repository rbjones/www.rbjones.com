% $Id: ProofPowerStory.tex,v 1.2 2010/02/06 16:19:48 rbj Exp $
% bibref{rbjp006} pdfname{p006} 

\documentclass[10pt,titlepage]{book}
\usepackage{makeidx}
\usepackage[unicode,pdftex]{hyperref}
\pagestyle{headings}
\usepackage[twoside,paperwidth=5.25in,paperheight=8in,hmargin={0.75in,0.5in},vmargin={0.5in,0.5in},includehead,includefoot]{geometry}
\hypersetup{pdfauthor={Roger Bishop Jones}}
\hypersetup{colorlinks=true, urlcolor=black, citecolor=black, filecolor=black, linkcolor=black}
%\usepackage{html}
\usepackage{paralist}
\usepackage{relsize}
%\bodytext{BGCOLOR="#eeeeff"}
\makeindex
\newcommand{\indexentry}[2]{\item #1 #2}
\newcommand{\glossentry}[2]{\item #1 {\index #1 #2}}
\newcommand{\ignore}[1]{}
\def\Product{ProofPower}
\def\ouml{\"o}

\title{The Story of ProofPower}
\author{Roger Bishop Jones}
\date{\ }

\begin{document}
\frontmatter

\begin{titlepage}
\maketitle

\vfill

%\begin{abstract}
%An interview conducted by Tony Dale with Roger Biehop Jones on mechanised proof.
%\end{abstract}


\vfill

\begin{centering}

\vfill

\footnotesize{
Started 2005-01-26

Last Change $ $Date: 2010/02/06 16:19:48 $ $

\href{http://www.rbjones.com/rbjpub/www/papers/p006.pdf}{http://www.rbjones.com/rbjpub/www/papers/p006.pdf}

Draft $ $Id: ProofPowerStory.tex,v 1.2 2010/02/06 16:19:48 rbj Exp $ $

\copyright\ Roger Bishop Jones;

}%footnotesize
\end{centering}

\end{titlepage}

{\parskip=0pt\tableofcontents}

\mainmatter

\chapter{ProofPower}


\section{Introduction}

In 1990 the then International Computers Limited (ICL) began a collaborative R\&D project of which its part was to be the implementation of the software which is now known as {\Product}.
{\Product} is a proprietary open source software suite which provides support for the application of proof oriented formal methods to the development of information systems.
{\Product} is now owned by Lemma1 Limited, and the software and supporting documentation may be downloaded from the {\tt lemma-one.com} web site.

{\Product} is itself well described in its manuals and tutorials.
Its use is illustrated by examples which come with the system or which have been published separately.
The main emphasis of this material will therefore be on aspects of {\Product} which have been less well treated elsewhere, namely, its interrelated history and rationale.

\subsection{Market Context}

The origins of {\Product} lie principally in three market factors which came together towards the latter part of the 1980's.

\begin{description}

\item [ICL Enters Defence Market]
\ 

The first of these was the entry of ICL into the defence marketplace, at the end of a delay imposed by the labour government when in 1967 it formed ICL from the remnants of the British commercial computing industry.
The abstinence by ICL from operational defence computing was a time-limited {\it quid pro quo} for a period of protection which it was to enjoy in its core markets from competition by the (mainly defence) companies which had retained relevant technologies for military rather than commercial applications.
ICL saw entry into operational defence related computing as a commercial opportunity and hoped to exploit its most innovative technologies such as the SIMD distributed array processor.
Anticipating the need for considerable R\&D to underpin its penetration of the defence industry ICL established a Defence Technology Centre which enjoyed for a while the latitude to invest in building expertise and capability without too close an eye on the bottom line.

\item [Demand for Formal Methods in Security]
\ 
 
The second significant factor was pressure from the special relationship between the UK and the USA in relation to the handling of classified intelligence data.
Mindful of the threat to its national security which was posed by insecure information processing systems, the US government had set out guidelines for the development of secure systems which laid great emphasis on the use of mathematical methods in their design and implementation.
Regulations promulgated on the certification of computer system for use in security critical applications mandated the use of {\it formal methods} where the highest levels of certification were sought.
In order to continue in its privileged position in relation to access to sensitive information, the UK government would either have to buy its computer equipment from manufacturers who had completed the required US certification, or they would have to put in place their own comparable system of certification and encourage or contract the UK computer industry to develop systems for certification under that scheme.
The UK was the first country to follow the US in this regard with its own national certification scheme, but this was to lead to a harmonised regime across Europe.

It was this which lead indirectly to the formation of a formal methods team at ICL Defence Systems motivated to reason formally about specifications in the Z language.

\item [DTI IED]
\ 

The third ingredient of the political pot-pourri was the desire of the Department of Trade and Industry to bring academic research closer to the needs of industry.
Through its Information Engineering Directorate a programme of R\&D through mixed industry and academic collaborative projects.
The call for proposals arrived just when the formal methods team was beginning to understand the problem and the available technologies well enough to formulate a project which would address those problems by progress the technologies.

\end{description}

These are the political factors which made possible the FST project (IED 1563) under which {\Product} was first developed.

To understand the details of what the project produced and why and how it did so we need to look a little more closely into the technicalities.

\subsection{Technical Context}

While the establishment of this scheme was in progress the government was placing R\&D contracts with UK industry relating to the development of secure systems.
ICL defence systems won some of these contracts, and was made aware that this kind of work would have to be undertaken in a manner consistent with the emerging certification regime.
A formal methods capability would therefore have to be established.
THe first steps were taken in 1985.

Z had already been identified as the preferred formal specification language for work on government secure systems and so familiarisation with Z was an early priority, as was gaining experience in the proof technologies which seemed most promising for the kind of work expected.
The two proof tools which were being evaluated by the formal methods team were NQTHM (otherwise known as the Boyer-Moore prover after its authors) and the Cambridge HOL system, both of which had also been used at RSRE Malvern for the formal treatment of digital hardware.

Of these two the Cambridge HOL system seemed to offer best prospect of success in reasoning about specifications in Z, in default of any proof tool for Z itself.

The main factors in this assessment were:
\begin{itemize}
\item firstly that the language HOL (Higher Order Logic) supported by the Cambridge HOL system was much closer in logical expressiveness to Z than was the impoverished first order logic of NQTHM.
\item secondly that the LCF paradigm, giving the user a powerful functional programming language for programming proofs and for other kinds of extension seemed to promise a greater flexibility of the tool for adaptation to tasks for which it had not been originally intended.
\end{itemize}

By contrast with work in HOL, for which the concrete syntax was entirely coded in ASCII, Z had a culture of pencil and paper specification using not only more customary logical symbols but also many other special mathematical symbols and also a special graphical layout in which formal specifications were presented in various kinds of boxes and other structures.
It was also a novelty of the Z culture that specifications were presented embedded into into formal textual discourse.
The relationship the formal and the informal material in computer documents (usually programs of course) was inverted, instead of adding annotations into the formal material, Z added formal materials into a proper English document.

Cambridge HOL needed no special facilities for document preparation, expecting HOL scripts to be treated just like a program.
Specifications in Z however were intended for a readership who might admire a beautifully handwritten English document with interspersed mathematics, but would look askance at a program listing.
An element of our initial tasks in developing ways of reasoning formally about Z was therefore to provide document preparation facilities.

The method adopted was to manually transcribe a specification from Z to HOL, maintaining as close a correspondence as possible between the original specification in Z and the translated specification in HOL.

Sun workstations came with a font editor and it proved straightforward to edit into a standard font a good selection of the logical and mathematical symbols used by Z.
Cambridge HOL could be wrapped in a couple of UNIX filters which translated between these symbols and the corresponding ASCII sequences used by HOL.

As well as crudely effecting a partial reconciliation between the lexis of Z and HOL, some more fundamental issue were addressed at this early stage.

The Cambridge HOL user community attached value to the fact that HOL is the kind of logical system in which mathematics and its applications could be undertaken by conservative extension (it provides a {\it foundation} for mathematics).
The introduction of new axioms was supported by the tool, but frowned upon by its users except in those very rare circumstances in which it was not technically avoidable.

The Z notation had no apparent concern for this matter, and the approved specification style freely admitted the introduction of ``axiomatic specifications'' which combined the introduction of new (so called) global variables (constants) with an axiom stating any desired property.

At ICL we were fully sympathetic to the HOL tradition harking back to {\it Principia Mathematica} but could not eschew the use of axiomatic specifications in Z.
We therefore translated Z axiomatic specifications into a good but not absolutely faithful approximation in Z by a (behind the document) translation into a definition using the HOL choice function.
When we later came to a substantial project in which formal verification would be critical a special version of Cambridge HOL was produced for that project in which, along with a small number of other security enhancements, a feature was introduced which make such conservative translations more fully faithful to the meaning of the original specification.
An equivalent facility was later introduced to the main Cambridge HOL in response to our enquiries on this point.

So far {\Product} has not appeared on the scene, but we have seen some minutiae which eventually left their mark on its character.

The DTC formal methods team applied formal methods to the development of secure systems, using transcription into HOL for formal proofs for three years before the opportunity arose to proposed a substantial program of work on the development of tools to support this kind of work.
This work included the formal aspects of a complete hardware development and manufacturing project which was undertaken so as to achieve certification at the highest possible levels of assurance, involving the development of a formal machine checked proof that the system met the formal statement of the critical security requirements.

Transcribing Z into HOL with care to ensure that meaning is preserved and then reasoning formally with the resulting specification, all the while wondering how the methods of transcription might be made more complete and systematic to the point that they could be implemented in an embedding is not a bad way to get a good understanding of the semantics of Z.
We were therefore reasonably equipped when the opportunity to start developing formal methods tools in earnest to make some substantial strides forward.

Nevertheless, full and faithfully support for Z via embedding into HOL was not something we were confident could be done so as to yield a usable and productive proof environment.

\subsection{The Proposal}

It was a prime objective of ICL's part of the FST project to realise the best possible support for proof in Z by some kind of embedding into HOL.
At the time that the proposal was prepared, however, there was considerable uncertainty about what could be achieved and how.
The project proposal was therefore almost mute on this topic, and put forward as ICL's task the production of a proof assistant for the HOL logic engineered to standards appropriate for its use in commercial developments of software for certification at the highest levels of assurance.
Additional declared desiderata were, improved usability of the tool and productivity in its intended uses.

These desiderata were to be realised by:

\begin{itemize}
\item Development following industrial product quality standard methods (including inspection of detailed design).
\item The application of formal methods to the design of the new tool.
Formal specifications of the syntax and semantics of the HOL language were prepared, and of the ``critical aspects'' of the design of the proof assistant (viz. those which were pertinent to the risk of the tool accepting as a theorem some sentence which was not legitimately derivable in the relevant logical context).
\end{itemize}

\section{The Development of ProofPower}

Work began on a disposable prototype proof tool, which was called ``ICL HOL'' several months before the official start of the FST project at the beginning of 1990, so a working proof tool was available very early in the project.
Though at the time of writing the project proposal no complete injection of Z into HOL was known, in the first few months of 1990 a mapping was devised and partially specified in Z which was thought to be semantically correct (up to a bit of debate about the semantics of undefinedness) and practicable.
The code of the prototype was not used in the product standard development which followed rather more slowly, but the prototype was used as a base for prototype Z proof tool which was developed concurrently with the product version of the HOL tool which was called {\Product}.



\section{Features of ProofPower}

{\Product} is {\it primarily} a conservative re-engineering of `classic' Cambridge HOL (HOL88) with support for Z added by semantic embedding.
The innovations, such as they are, can be related to these two factors.

\subsection{Engineering}

\paragraph{Use of Standard ML}

Standard ML failed to draw in the entire ML community, but adopting standard ML for this development still looks like the right choice.

\paragraph{Formal Specification of Key Features}

Key features of the tool, notably the logic it supports and the logical kernel which ensures that the tool will only accept as theorems sentences which are derivable in that logic, and formally specified (in \Product-HOL).
The logical kernel includes critical aspects of management of the theory hierarchy.

\paragraph{Systematic Software Development}

{\Product} has a detailed design, implementation and test documents for each software module.
Since all these documents contain SML code which is either compiled into or executed to test {\Product}, (detailed design documents include signatures) there is little tendency for the detailed design to slip out of sync with the implementation.

\paragraph{Name Space Improvements}

One feature of LCF-like provers is that most of the tools used to implement the prover are accessible to the user through the metalanguage namespace and may be re-used in extending the capabilities of the system.
This make for a rich and powerful environment for programming proofs, but also for a very large namespace in which the user may struggle to find the features he needs.
It was a design objective of {\Product} to ameliorate this situation in modest ways.

The general policy was as follows:

\begin{itemize}

\item[(a)] For low level features needed for efficient coding of new proof facilities the coverage of each kind of facility should be complete and systematic, so that the names of the various individual interfaces can often be obtained by following some obvious rule.
Elementary examples of such conventions are the systematic use of ``mk\_'' and ``dest\_'' prefixes for syntactic constructors and destructors.

\item[(b)] User level proof facilities (e.g. tactics) should so far as possible make it unnecessary for the user to select a specific tool.
An elementary example is {\it STRIP\_TAC} which inspect the principle logical connective of the current goal and perform an appropriate proof step, making it less likely that the user will have to know the name of the low level tactic which deals with each individual connective.
The tactic of the same name in {\Product} is considerably extended in power, addressing not only principle logical connective of a formula but also in many cases the relation of an atomic formula.
It is context sensitive, applying more rules in richer logical contexts and, for example, in the case of membership predications will be able to apply in appropriate contexts a range of different rules for the various kinds of structure of which membership can be asserted (e.g. membership of an intersection will be transformed into a conjunction of membership assertions).

\item[(c)] documentation of the namespace in the reference manual is generated automatically from the detailed design documentation, and includes a KeyWord In Context index making it easy to discover all the names which contain a particular substring.
Names are usually compounded from a sequence of elements related to the function of the name.

\item[(d)] the use by {\Product} of special logical and mathematical symbols extends to the names in the standard ML namespace (of which {\Product} effects a lexical variant with an extended character set).

\end{itemize}

\paragraph{The Theory Hierarchy}

For theorem proving in the context of large specifications some way of structuring the logical context is desirable, and in these tools this is done through a hierarchy of ``theories'', which in this context are a kind of hybrid between the logician's notion of a theory and the computer scientists of a module.
In normal use a theory is the largest unit of conservative extension of the logical system, though in extremis extensions not know to be conservative will also be permitted and recorded as such.
It is important for the preservation of logical coherence that when changes other than mere additions are made to the hierarchy that everything which logically depends upon the material changed is modified or invalidated at that point.
In the Cambridge HOL system this was accomplished by deleting theories, and any change to a theory could be accomplished only by deleting that theory in its entirety and all its descendants.

Flexibility in this matter depends upon how much detailed information is held about the interdependecies.
Holding more information permits greater flexibility, but complicates the data structures involved in theorem proving.
When LCF was first developed computers were very slow, and one put up with an inflexible system.

The {\Product} theory hierarchy is designed with a greater degree of flexibility, falling short of the full flexibility which might flow from the fullest recording of dependencies.
A definition or specification can be deleted from a theory without deleting the entire theory.
It is however necessary to delete (and reload if required) all the definitions in the theory which took place subsequent to the one under modification (and those in theories lower in the hierarchy).

\paragraph{The Logical Kernel}

Cambridge HOL, as well as the inferences rule of HOL provides a back-door ``mk\_thm'' which allows any sequent to be made into a theorem and thereby rendering the logic technically inconsistent.
In a tool intended for use in developments subject to formal evaluation this is difficult to defend.
It is of course no feat of engineering to omit this feature, the rationale for which still escapes me even though some have felt it worth defending.
It is I believe now used in Cambridge HOL to check the validity of intermediate states of the sub-goal package.

\paragraph{The Sub-Goal Package}

LCF-like provers are implemented using an abstract data type to implement a type of theorems, the only constructors of which are the rules of the logic (let us assume for present purposes that axioms are rules without premises).
In all the cases of interest here the underlying logic is in fact an asymmetric sequent calculus (in which the sequents have a list of assumptions and a single conclusion) permitting a forward (or Hilbert style) proof system which has some similarities with a natural deduction system (and can be made very similar with the benefit of derived rules).

The full convenience of ``backward proof'' is then realised using a ``sub-goal package''.
The user starts a proof by passing to the sub-goal package the sequent which he wishes to prove (as a sequent rather than a theorem, since it will not be available as a theorem until after it has been proven).
At each step in the proof which follows the sub-goal package presents to the use a single ``current goal'' and the user nominates to the subsoil package a ``tactic'' to be applied to the sub-goal.
When a tactic is successfully applied to a sub-goal it breaks it into zero or more further sub-goals and supplies with it a rule which derives the sub-goal (as a theorem) from the list of theorems corresponding to the new sub-goals.
If a tactic can solve a sub-goal, then its introduces no new sub-goals and offers a proof which requires no theorems to be supplied to it, and which can therefore be invoked by the sub-goal package to obtain the desired theorem.
If the user persists in supplying relevant tactics to the sub-goal package until all sub-goals are in this way discharged, the sub-goal package will be able to construct a proof of the original goal by composing the proofs obtained from each of these steps.
This proof will be a rule which when applied to the empty list of theorems will compute and return the desired theorem.

Tactics are not infallible, not only may they fail to offer a step in the desired proof at all, they may make an offer which they later fail to redeem.
The failure of a tactic to deliver on its promise is a bug in the tactic, and these are rare.
Tactical programming is often done by users, and it is very inconvenient to discover an error in a tactic only at the point of completion of a large proof which uses the tactic, not least because the diagnostics available at this point may not be good.
This kind of thing did happen in the Cambridge HOL system which we were using before the development of {\it Product}.

The sub-goaling package design in {\Product} is immune to this problem.
Instead of remembering a tree of sub-goals and proof functions the state of the sub-goal package is coded up as a theorem in which the assumptions are (codings of) the outstanding sub-goals, and the conclusion is a coding of the target goal (a constant is used which mimics the semantics of the sequent turnstile).
The construction of this sub-goal package state theorem involves invocation of the proof function at the same time as the tactic is invoked so that its failure is detected immediately.

In some versions of Cambridge HOL the desired checking of the proof function takes place at the point at which it is produced using ``mk\_thm'' to obtain the necessary premises.
This method however does depend upon this hole in the abstract data type, which might undermine trust in the proof tool.

\subsection{Z and Other Embedded Languages}

The idea of a semantic embedding of one language into some other language is to treat the first language as if it were syntactic sugar for expressions in the other.
The idea is that some capability in respect of the second language is thereby transferred to the first.
In this case the primary capability of interest is proof construction and checking.

In the case of embedding the language Z in HOL though in some deep logical sense the languages are closely related, in the superficial matters such as syntax they are worlds apart.
A semantically correct embedding for the whole of Z into HOL would be relatively complex, and in all aspects of functionality a proof tool would have to be customised to Z in order to achieve reasonable levels of usability and productivity.

The advantages are few but substantial.
An important advantage is that by this means proof capability can be realised where the semantics are known but the proof rules are not known.
Soundness of inference is guaranteed by the soundness of the proof system for the target language provided that the embedding is semantically correct.
A second advantage is that sharing between the languages of that most precious and costly item, theorems, is maximised.
Thus a theory of integer or real numbers developed for HOL will be substantially re-usable in Z, together with domain specific proof automation such as a linear arithmetic prover.

Z was not the only other language for which support might be needed, though it was thought to be the most important (in terms of prospective business), so a generic tool was desirable.
A syntactically generic approach might have been adopted, but we decided instead to aim for genericism via embedding.

In this section, as well as describing some of the key features of the support for Z in {\Product} the effect on the underlying core implementation of HOL will be picked out, not only specifically of the need to do Z, but also of the perceived need for a degree of genericism.

\paragraph{document preparation}

It is the Z style of literate formal specification which has determined that {\Product} is oriented around processing of formal texts extracted from {\LaTeX} documents.
The document processing facilities are based around a program called ``sieve'' which processes documents according to the instructions in a sieve steering file watching for tags in the document which introduce the many different kinds of formal material which may be included in such a document.
The same kinds of documents may be processed in different ways for different purpose, for example to yield pure {\LaTeX} for printing, or scripts suitable for loading the formal material into the {\Product} proof tool  for processing to enter specifications into the theory hierarchy, to generate and check formal proofs and store the resulting theorems.
It is hard now to see how we could have managed without this kind of machinery, but it remains the case that the initial impetus to this manner of working came from Z, and that the academic versions of HOL still work directly from ASCII text files and do not involve themselves in these matters.

\paragraph{term quotations and pretty printing}

The dialogue with the proof tool takes place through the interactive metalanguage ML.
Cambridge HOL had special mechanisms to make invocation of an object language parser straightforward in the form of special quotations marks for this purpose.
Metalanguage quoting in object language terms is also supported, allowing the insertion into an object language term of an expression of the metalanguage of type {\it TERM}.

In {\Product} this kind of facility is extended in two main ways.

Firstly the character set is extended so that the dialogue includes the most commonly used logical and mathematical symbols.
This is done by coding up the special characters into strings of characters which are acceptable in standard ML.

The effect is that not only the quoted object language terms may contain these special characters, but also the ML names, so that ML names may be chosen which directly relate to the symbols in the language, e.g.
=INLINEFT
%implies%_elim, %forall%_intro
=TEX
.

The object language quotation facilities in {\Product} include not only the primary object language HOL and embedded ML, but also the Z language, and designed to allow other languages to be added.
Full multilingual mixed language parsing and pretty printing is supported.
That means that HOL and Z can be mixed together in a single term quotation, fragments of HOL being included inside Z or vice-versa.
Of course there are constraints on what is allowed and this is largely controlled by the type system and the injection used by the embedding of Z in HOL of the types of Z into those in HOL.
A HOL term quoted inside a Z term must have a type which is in the image of the injection and its position in the surrounding Z term or predicate must be consistent with that type.
To make pretty printing of a term possible constants are associated with languages, and this information control the selection of pretty printers for the different parts of a term.
Mixed language terms are not normally encountered in Z proofs, the proof facilities are smart enough to keep the proof in Z.
However, tactical programming will frequently involve programming transformations which provide an inference within one language using transformations passing through terms which do not belong to that language (the primitive rules of the language will not generally stay in the image of a language embedding).

\paragraph{stripping and proof contexts}

There is here both another application of the obvious principle that to provide generic multi-language support via semantic embedding one must make most features of the system potentially customisable to particular languages, or, more generally to particular contexts, which include both a position in the theory hierarchy and a language associated with that context, and also an example of a specific generalisation of a feature of HOL88 which arose from need to support Z.

When prototyping of support for Z on the first prototype ICL HOL reached the point of attempting goal oriented proofs using the goal package it was necessary to make the behaviour of {\it strip\_tac} sensitive to the language so that it could handle correctly the Z universal quantifier.
As soon as the possibility of making {\it strip\_tac} context sensitive is considered the question what it can beneficially be used for becomes open.
ICL were not involved in the early development of the LCF system of which {\it strip\_tac} appears to be a highly used historical remnant.
{\it strip\_tac} looks like it is a somewhat incomplete attempt to provide a tactic which knows the basic natural deduction rules for the predicate calculus and automatically applies the rule relevant to the current goal, provided that can be done without extra information (such as an existential witness).
This picture makes more sense in terms of the original LCF logic, in which there were fewer logical connectives than there are in HOL.
If this idea is thought through it becomes apparent that a natural generalisation of {\it strip\_tac} can be produced which is complete in respect of the propositional calculus (when repeated).

Z is based on set theory, and the natural systematic approach to proof in Z is to characterise each of the set-valued constructs in the Z language extensionally.
Set theory used in this way fits well into a natural deduction framework.
The simplest example is the handling of intersection.
A goal which is a membership assertion of which the right hand expression is an intersection can be transformed into a conjunction of membership statements, and this behaviour is a natural extension to the capabilities of {\it strip\_tac}.
In general, if the semantics of the set valued Z terms is given extensionally as an equivalence statement in which an assertion of membership in the construct is said to be equivalent to some formula in which that construct does not occur, then these equivalence claims provide extensions to the stripping behaviour, or to the default behaviour of rewriting facilities.
The effect of systematic adoption of these methods is to automate the transformation of quite elaborate expressions in Z's set theory into predicate calculus in which the set theoretic vocabulary has been eliminated.

\section{DAZ and CLAWZ}

\subsection{The Changing Marketplace}

The initial development of {\Product} was motivated by the apparent demand for formal verification against specifications in Z at the extremes of high assurance in secure computing, and by the need for tools to support that process.
The development supplied tools which enabled ICL to complete the design and verification of secure systems, and also gave skills to the formal methods team in the development of proof tools in a context in which external contracts for such development were in prospect.
By the time that the FST project was complete, the context had changed.
The first setback had been that expected open tenders for development of proof tools for use in secure systems development failed to materialise.
The developments did take place, but were undertaken under existing formal methods consultancy contracts.
The more serious setback was a complete volte-face in government policy on the development of secure systems.
The dominant trend in military computing procurement was towards ``COTS'', Customised Off The Shelf procurements, rather than the more traditional and more expensive development of bespoke systems.
For this or other reasons the expected stream of government contracts for the development of highly secure systems dried up.

At the same time the honeymoon period for the Defence Technology Centre had ended, the formal methods team had to make profits.

At the same time as the prospects for formal methods in secure computing were faltering, the application of formal methods to safety critical military systems was being underpinned by Defence Standard 00-55.

The Royal Signals and Radar Establishment at Malvern had pioneered research for the Ministry of Defence in formal methods, and (inter alia) had developed a ``compliance notation'' which permitted refinement of specifications in Z into programs in a safe subset of Ada.
As the FST project came to an end, RSRE, by then part of the Defence Research Agency, put out an open tender for a tool to support the use of their compliance notation in the development of safety critical systems.
This would open an alternative marketplace to {\Product} and the ICL High Assurance Team if the contract could be secured and the compliance tool built on {\Product}. 

\chapter{Z in HOL}

\section{Introduction}

In 1990 International Computers Limited began 3 year collaborative research and development project, in which its effort was to be devoted to the engineering of a proof assistant for the Z specification language, intended for applications in the development of highly assured secure computing systems.

This paper is a kind of ideological reverse engineering of the software development which ensued, and a cameo showing how ideas formulated before their time can reach across centuries of cultural evolution to inspire effects which could not have been foreseen.
It traces back through hundreds of years and several academic disciplines the history of the ideas which shaped the product {\it ProofPower} which emerged from that project.

The character of the paper has been influenced by the work of Donald MacKensie in his book {\it Mechanising Proof - Computing, Risk and Trust} \cite{mackensie}, which provides a broader perspective on the context in which this work took place.

The known history of these ideas goes back of course through millenia before the trail goes cold, and the choice of where to begin is therefore to some extent arbitrary.

\subsection{Gottfried Wilhelm Leibniz}

The story begins with Leibniz(1646-1716), who I think of as the patron saint of automated of reasoning.
Leibniz conceived of a {\it lingua characteristica} and a {\it calculus ratiocinator}.
The first was a universal formal language, in which all things could be expressed with mathematical precision.
The second was a calculus which permitted any question formulated in this language to be reliably answered by mechanical computation.

Though Leibniz also contributed to the mechanisation of such computations through his work on calculating machines, digital computers as we know them were hundreds of years into the future.
Not only were Leibniz's aspirations ahead of the technology of his day, they depended on advances in symbolic logic which were not to be realised for another quarter of a millenium.

Leibniz was in other respects of his time.
Galileo's new scientific methods, involving the use of mathematics in formulating precise models of aspects of physical reality, ushered in a new era for science and stimulated new developments in mathematics to support this new kind of science.
A fundamental building block for the kind of applicable mathematics which was needed by the new science and engineering was what we now call {\it the calculus}, on whose development many mathematicians of the time were engaged.
Though many other mathematicians contributed, Newton and Leibniz are generally credited with independently fitting the story together (though the question of precedence was a source of bitter controversty between them).

The invention of the differential and integral calculus was however just the starting point for a period in which the new kinds of mathematics opened up by the calculus were progressed under continuing stimulus from the development of science.
In periods such as this it is practical achievement in the establishment of applicable mathematical methods which drive the development of the subject.
Rigour takes a back seat, uncertainties are put aside for so long as new mathematical developments prove to be effectively applicable.

Doubts there were, at the very beginning of this process, notably a corrosive critique by the Irish philosopher Berkeley.
The difficulties appear most conspicuously for Leibniz's approach to the calculus in his use of infinitesimal quantities.
As a result of developments in the 20th century it is now known how infinitesimals can coherently be used in the development of analysis (this yields what is now called ``non-standard analysis'', ``analysis'' being the part of mathematics in which the differential and integral calculi now belong). 
But when the calculus was first introduced, and for a couple of centuries during which there was continuous rapid growth of mathematics based on the calculus, neither the real numbers nor the non-standard reals were adequately understood to make a rigorous foundation for this mathematics possible. 

Only when we come to the nineteenth century does the rush to exploit the calculus calm sufficiently for mathematicians to pause and consider more carefully its foundations.
Once mathematicians began to look toward the foundations of analysis, a new trend begins which leads in due course to the new discipline of mathematical logic, and to the advances in logic necessary for any real progress towards Leibniz's universal language and calculus of reason.

Prominent in the results of these foundational studies we find the ancestors of Z and HOL, Zermelo's set theory, and Russell's theory of types.

\subsection{The Foundations of Mathematics}

The first step in the rigourisation of analysis was to find a way of doing without infinitesimals (though it is now known how one can rigorously do {\it with} them, this was not to be discovered until much later, and non-standard analysis has not supplanted the ``standard'' methods whose development now concerns us).

The concept of ``limit'' was the key to defining key concepts such as ``continuous'', ``differential'' and ``integral''.
Once these concept are defined as limits there is immediately a problem about when these things exist, which places the focus on the number system in which this work is taking place.
The next step is therefore to clarify the concept of ``real number'', giving a conception of number in which limits exists when it seems natural and convenient that they should.
It is not needed that all series of numbers should have a limit, the kinds of series which one would like to have limits are the {\it convergent} series.

To realise a number system in which convergent series have limits, numbers may be thought of as ``cuts'' in the rational numbers.
A cut divides the rationals into two non-empty collection, a lower and an upper, such that every rational is in just one of these collections and every rational in the lower collection is smaller than every rational in the upper collection, and such that the upper collection has no least member.
The limit of a series is then the set of rationals which are less that at most finitely elements in the series and it can be shown that every convergent series of rationals has a limit in this number system, and that every convergent series of these new {\it real} numbers has a limit which is a real number.

We now see that in our search for rigour we have found it desirable to speak of sets.
The real numbers are defined as sets of rationals, the rationals are readily understood as constructions from the whole numbers of arithmetic, and for a mathematician acquainted with the ideas of Leibniz it is natural to ask whether one further step reducing the whole to a formal logic might be possible.

\subsection{Mathematical Logic}

Another thread in the mathematics of this period, alongside the rigourisation of analysis consists in attempts to treat logic mathematically.
In Leibniz's conception of a calculus ratiocinator the computations involved were numerical computations, his lingua characteristica involving numerical coding of concepts and propositions.
In the nineteenth century mathematicians found new more abstract ways of studying number systems, the methods of abstract algebra.
From the perspective of abstract algebra the truth functional logic of propositional connectives {\it and}, {\it or} and {\it not} could be seen as an example of an algebra, called a {\it boolean} algebra after the mathematician George Boole.

This approach to a mathematical treatment of logic was limited.
It did not connect well with the conception of logic as concerned with formal deductive systems.

\section{HOL and its Roots}

HOL is an acronym for Higher Order Logic.
There are many variants of higher order logic, but in this essay the acronym HOL will be used specifically for the version of higher order logic implemented at Cambridge University as a proof assistant, also called HOL.

HOL is a direct descendant of Bertrand Russell's {\it Theory of Types}, details of which were first published in 1908, and the line of descent is simple enough for a reasonable account to fit into these pages.

In brief the story begins with Russell's original {\it ramified} theory of types\cite{russell08}, which is then simplified by removal of the ramifications to give the {\it simple theory of types}.
The next step, due to Alonzo Church\cite{church40} is a further simplification obtained by basing the system on functional abstraction.
This ends the line of descent belonging to mathematical logic leaving only those further elaborations which were thought necessary for application of the logic to hardware verification.

\subsection{Russell's Theory of Types}

Russell devised his theory as a logic in which the reduction of mathematics to logic could be carried through.
The main problem which Russell had to solve in devising his logic was to avoid in a philosophically satisfactory way the antinomies which had already been noted in the foundations of mathematics, including but not limited to the one which now bears his name.
On examining all the known foundational paradoxes Russell observed that they had all in common ``the assumption of a totality such that, if it were legitimate it would at once be enlarged by new members defined in terms of itself'', and proposed the rule that ``no totality can contain members defined in terms of itself''.
This is Russell's {\it vicious circle principle} and is the basis of his type theory, in which context it assumes the more technical form ``whatever contains an apparent variable cannot be a possible value of that variable''.

In the resulting theory the universe of discourse is considered to be partitioned into a countably infinite sequence of {\it types}.
The first type consists of individuals and the rest of propositional functions.
A propositional function should be thought of as a proposition containing real (i.e. free) variables, and which therefore may be considered as a function which yields a proposition for each of the possible values which these free variables might take.
A propositional function may also be expressed using apparent variables (variables bound by quantifiers).
In the ramified type system the type of such a propositional function is the least type greater than that of any variable (real or apparent) which occurs in it.

In this {\it predicative} type theory only limited parts of mathematics can be derived.
Russell therefore found it expedient to adopt his {\it axiom of reducibility}, which states that every propositional function is co-extensive with a {\it predicative} function.
A predicative function is one in which the type of every apparent variable is no greater than the type of a real variable, so the effect of the axiom of reducibility is to negate the effect of the ``ramification'' (i.e. of taking into effect the types of apparent variables).

\subsection{The Simple Theory of Types}

It was inevitable that Russell's theory of types would be simplified.
This was first proposed by Ramsey \cite{ramsey31} after Peano had observed that the ramifications were relevant only to the resolution of the semantic paradoxes (such as {\it the liar}), which, even
without the ramification to the type system, are not reproducible in this kind of logical system.

After a bit of cleaning up this becomes the standard textbook presentation of higher order logic.
But alongside the establishment of this logical system investigations into founding logic more radically and thoroughly on functions lead to a reformulation of the simple theory of types both more elegant and better suited to the applications in computer science which concern us here.
This formulation of the simple theory of types is due to Alonzo Church \cite{church40}, and is simple enough to present here.

In Church's paper the system is described using concrete syntax considering syntactic entities as strings.
The system presented here is the same logical system, but is presented instead using {\it abstract syntax} an innovation in the description of languages which arose in Computer Science.

They type system of Church's formulation of the simple theory of types consists of the two types {\it i} and {\it o} which should be thought of respectively as the types of {\it individuals} and of {\it propositions}.
It has a single binary type constructor which given two types ${\alpha}$ and ${\beta}$ yields the type $(\beta\alpha)$ which is a type of (total) functions whose domain is the type $\alpha$ and whose co domain is the type $\beta$.
The abstract syntax of the type system is the single sorted free algebra with this signature.

The terms of the language are either {\it variables}, {\it constants}, {\it lambda abstractions} or {\it function applications}.

A variable is a name together with a type, as is a constant, and is written as the name subcripted with the type.
Lambda abstraction is a ternary term constructor, whose operands are a name {\it n}, a type {\it ty} and a term {\it tm}, and is written as the letter ${\lambda}$ followed by the {\it bound variable} whose name is {\it n} and whose type is {\it ty}, followed by the term {\it tm}.
Function application consists of the juxtaposition of a term whose type is compound on the left with another term on the right, subject to the constraint that the type of the first term is a function type of which the second argument is the type of the second argument.
Application of this constraint of course depends upon the assignment of types to arbitrary terms which is done as follows.

The type of a variable or a constant is the type from which it was formed.
The type of an abstraction is the function type of which the first argument is the type of the term which is the body of the abstraction, and the second is the type of the bound variable in the abstraction.
They type of a function application which complies with the above mentioned constraint is the type of the co-domain of the first term of the application.

The logical system is a Hilbert style deductive system in which the role of formulae is undertaken by terms of type {\it o}.

The system has two primitive constants and two families of primitive constants which are:
\begin{itemize}
\item The constant whose name is `N' and whose type is `({\it oo})'.
\item The constant whose name is `$\lor$' and whose type is `({\it (oo)o})'.
\item The constants whose names are `$\forall$' and whose types are `$({\it ((o\alpha)\alpha)})$' for each type $\alpha$.
\item The constants whose names are `$\iota$' and whose types are `$({\it \alpha(o\alpha)})$' for each type $\alpha$.
\end{itemize}

These constants should be thought of respectively as negation, disjunction, universal quantification and description.

The axioms and inference rules of the system also involve various additional constants which may be defined in terms of the above primitive constants.
The constants defined by Church are conjunction, implication, equivalence, existential quantification, equality, inequality, the identity function, the combinator K, the numerals (defined as function iterators) the successor function, the property of being a natural number, 

The deductive system consists of six rules and eleven axioms or axiom schemata.

The first three rules are rules of lambda conversion, viz. renaming of bound variables, lambda-reduction and its inverse.
The fourth rule is a limited rule of substitution, the fifth modus ponens and the last is the rule of generalisation permitting the introduction of universal quantifiers.

Of the eleven axiom schemata the first four provide for the propositional calculus, the fifth allows specialisation of universally quantified formulae, and the sixth allows universal quantifiers to be pushed in over disjunctions (subject to appropriate conditions).
Axioms seven and eight provide for arithmetic, the first asserting that there is more than one individual, and the second that no two distinct numbers have the same successor.
Schema nine provides for descriptions by asserting that the description operator, when applied to a propositional function which is true of exactly one value, yields that value.
Schema ten asserts that functions are extensional, and eleven is an axiom of choice which upgrades the description functions to choice functions.

\subsection{LCF}

The details of the HOL logic and its implementation as the HOL proof assistant arise from adapting a system originally devised to support a quite different logic to provide support for an elaboration of Church's system.

The original logic and the software system which supported its use were both called LCF.
LCF stands for {\it Logic for Computable Functions}.
The logic was devised by the logician Dana Scott for reasoning about denotation semantics.
It is a first order logic in which the terms are the terms of a typed lambda calculus, construed as functions in ``Scott Domains'' (continuous partial orders), supported by reasoning using fixed point induction.
In the logic as formulated by Scott, the type system for the terms is the same as that used by Church in his Simple Theory of Types, except that the primitive types differ.
In Scott's LCF formulae were distinct from terms and there was therefore no need for a type of propositions.
Scott was at pains to point out that the variables in his presentation ranging over types were variables in the metalanguage, not variables in the object language LCF.

The first tool providing support for this logic was {\it Stanford LCF}, implemented at Stanford University as a tool which would assist users in constructing and checking proofs in the LCF.
The team developing the Standford LCF tool was lead by Robin Milner, who then moved to Edinburgh and with the benefit of experience in applying that tool came up with some radical new ideas for a successor which would be know as Edinburgh LCF.

One of those ideas was to develop a new {\it typed} functional programming language for use both in implementing the proof tool and as a metalanguage for interacting with the users of the tool when constructing and checking proofs.
Stanford LCF had been implemented in LISP, which is an untyped functional programming language.
It was immediately clear that if a typed functional language was to be substituted for LISP, the type system would have to be polymorphic.
(Otherwise, for example, all functions over lists would have to be coded up many times, once for every different type of list for which they were needed).

The closeness between such a typed functional programming language and the language of terms in LCF made it obvious to consider whether, if polymorphism was required in the metalanguage, it should not also be permitted in the object language.
Why was Scott at pains to point out that his type variables were in the metalanguage, were there doubts about the consistency of the logic if they were admitted in the object language, or was just a question he did not want to consider?

Anyway, the upshot was that polymorphism, in the form of free type variables which could not be bound but which could be instantiated, was included not only in the new Metalanguage, ML, but also in the version of LCF which was supported by Edinburgh LCF.
When later the LCF system was modified to support HOL, polymorphism in the object language was naturally retained.
 
The effects of LCF on HOL were not confined to the introduction of type variables.
It was natural in switching from LCF to HOL to make minimal changes to the code of the LCF system, and hence to the LCF deductive system, which had been developed with efficiency in mind at a time when computers where several orders of magnitude slower, and a great deal less common, than they now are.
The LCF system es further developed at Cambridge University, and it is the Cambridge version of LCF which was adapted to make the HOL system.
The version of LCF supported by Cambridge LCF was called PPLAMBDA and was documented by Larry Paulson in technical report TR-36.

The key features of PPLAMBDA were:

\begin{itemize}
\item[substitution]
\end{itemize}

\subsection{HOL}



\section{Z and its Roots}

\subsection{Zermelo Set Theory}

In the same year that Russell published his {\it Theory of Types}, Ernst Zermelo published a paper\cite{zermelo08} in which he offered a resolution to the set theoretic paradoxes through an axiomatisation of the theory of sets.

This paper is less ambitious than Russell's.
It seeks to present a consistent basis for the further development of set theory but neither attempts to formalise nor to provide a philosophical justification for the chosen axiom system.
The introduction to the paper makes clear that Zermelo has very specifically aimed to produce a set of axioms which are sufficient for the derivation of set theory as it was then understood, but which is immune to the paradoxes.
It is this paper which provided the foundation for twentieth century mathematics, needing relatively modest changes to make it suitable for most modern mathematics.

Subsequent philosophical rationales for Zermelo's axiom system have revolved either around the principle of limitation of size, or on the interactive conception of set.
However, the relationship between this system and the type theories we have discussed is perhaps more clearly seen in terms of the principle of well-foundedness.

Russell's saw the paradoxes as arising from circularity in definitions.
However, if definitions are conceived as picking out individuals from an already well established domain of discourse more liberal principles of definition than those proposed by Russell may be admitted.
The can be accomplished by moving the constraint against circularity from the definitions to the membership relation.
This is accomplished if the membership relation is required to be well-founded.
Zermelo does not himself place this constraint on sets, though it will later be added.
However, his theories are consistent with the constraint that sets be well-founded and the concept of well-founded set can be made intuitively reasonably clear through the idea of the iterative or cumulative hierarchy, in which sets are conceived as ordered by a rank which corresponds to the stage at which it appears in an iterative process of set formation which may be considered to populate the domain of discourse.

Zermelo's axioms were as follows:
\begin{itemize}
\item[I] Extensionality - two sets are the same if they have the same members
\item[II] That an empty set exists, and, for each set its unit set, and for any two sets a set which has just those sets as members.
\item[III] Separation - for each set and for each ``definite property'' of sets, there exists a set which contains just those elements of the first set which satisfy the property.
\item[IV] Power set - for each set there exists a power set whose members are the subsets of the set.
\item[V] Union - the union of the members of a set is a set.
\item[VI] Choice - for each set of non-empty sets whose members are pairwise disjoint there exists a choice set containing exactly one element from each of the members of the set.
\item[VII] Infinity - there is a set which contains the empty set and contains the unit set of each of its members.
\end{itemize}

This set of axioms appears to be consistent with an interpretation whose elements are the well-founded sets, which may be thought to be justified by the iterative or cumulative conception of set.
It appears immune to the known paradoxes, while being sufficient for most of the set theory known at the time.
This it achieves without the complexity and inconvenience of a type system.

The principle weaknesses of the system are:
\begin{enumerate}
\item Because this is an {\it informal} axiom system it is not easy to make precise the notion of {\it definite property} which appears in the axiom of separation.
\item It lacks sufficiently strong existence principles for some kinds of mathematics (but is sufficient for arithmetic and analysis).
\end{enumerate}

These weaknesses were eventually remedied, yielding the first order set theory now known as ZFC, which was widely regarded over the main part of the last century as providing a safe context in which mathematics could be undertaken.
It is the formalisation of the axiom system in first order logic which permitted the notion of {\it definite property} to be made definite.
The axiom schema of replacement was added permitting many more collections to be shown to exist.
Of these, for applications in formal methods for computer science the former was essential but the latter was less crucial since the branches of mathematics depending on the axiom of replacement are much less likely to be required for applications in computing.

\subsection{The Z specification language}

The Z specification language is in superficial respects much richer than the language of first order set theory.
The richness is what computer scientists call ``syntactic sugar'', which can be unravelled to render the same message in the simpler language of first order set theory.

In this section I will attempt to describe the key features of Z by comparison both with Zermelo set theory and also with higher order logic, informally but in sufficient detail to permit subsequent description of the interpretation of Z in HOL.

To understand Z and its interpretation in HOL it is best first to understand the kinds of objects which populate the universe of discourse of Z and then to know what can be done with them.

Unlike Zermelo set theory, which is pure untyped first order theory, Z has a type system, and is in this respect more like a type theory or a higher order logic.
This facilitates the interpretation of Z in HOL, and permits a first relationship to be established by consideration of the type systems of the two languages.

However, the type system of Z operates {\it behind the scenes}, and a first understanding of Z is perhaps better realised through an untyped interpretation.

\subsubsection{Z Without Types}

A Z specification consists of a sequence of paragraphs which serve to define various ``global variables'' (``global variable'' is Z-speak for ``constant'', the difference between them and local variables is not just that they have global scope, if they really were global variables in a first order logic then one could generalise assertions involving them, which is definitely not intended in Z).

At this stage I will describe just three of these kinds of paragraph, the given set paragraph and the axiomatic specification paragraph.

A given set paragraph consists in a list of given set names.
These are the names of sets whose interpretation is left loose, i.e. they are new ``global variables'' about which nothing is known except that they denote sets.

An axiomatic specification introduces one or more new global variables, which are specified in two ways.
Firstly they are specified in a signature which supplies for each of these variables a set of which it is required to be a member.
This signature looks like a type constraint, insofar as the membership of the new name in the set is written using a colon sign, but the expression on the right can be an arbitrary non-empty set.
In addition to the signature a formula is supplied in which the new global variables may appear as free variables.
This formula is called the {\it predicate} of the specification. 

The effect of this specification paragraph is to introduce a new axiom which asserts the conjunction of the constraints imposed by the signature and the predicate.

The third kind of paragraph is the schema box, which introduces a new global variable which denotes a set of bindings.
A binding is like an ordered tuple except that the components are named rather than merely ordered.
A schema box, like the axiomatic specification, contains a signature and a predicate.
Instead of introducing new global variable with the names in the signature, it introduces a single global variable whose name is declared on the schema box, and whose value a the set of bindings whose components are the component names in the signature.
The set is the set of those bindings which satisfy the conjunction of the signature and the predicate.
The schema box is therefore a kind of set comprehension, in which the signature of the schema serves to identify a set from which the required set can be obtained by separation.

The use of signatures which appear similar to type constraints but in fact constrain names to fall within arbitrary sets is pervasive in Z, and appears not only in the paragraphs which provide top level definitions of global variables, but also in all the variable binding constructs which are permitted in the formulae or terms of the language.
The effect is that all the abstractions which are legal in Z are sufficiently constrained that they can be interpreted as separation in an untyped first order set theory such as Zermelo set theory.

Though signatures are in fact set constraints rather than type constraints, and almost all
\footnote{The exceptions are the schema operations which involve negation, viz. schema negation itself, schema implication and equivalence, which, because they take a complement would not be interpretable if they were not constrained by a type system (on pain of letting in Russell's paradox).}
 the language can be interpreted in an untyped set theory, Z is in fact a typed set theory.

\subsubsection{Types in Z}

The type system of Z is more elaborate than that of STT.
Users of the language are encouraged to make use of uninterpreted types of entities which Z calls {\it given sets}.
The types of Z are the given sets introduced by a specification, the type Z of integers, and types which can be formed from other types by applications of the following type constructors:

\begin{itemize}
\item[Powerset] - the type whose members are sets of objects of some type.
\item[Tuple] - for each tuple of n types there is a type of n-tuples in which the nth element has the nth type
\item[Schema] - for each signature (identifier indexed finite family of types) a schema or labelled product type is available whose members are {\it bindings} with the given signature.
A binding is like a tuple but the elements are identified by tags or component names rather than a numeric position in the tuple.
\end{itemize}

All these kinds of construction can be undertaken in Zermelo set theory, provided that the given sets are all pure sets, using a variety of coding tricks, but in Z these coding tricks are hidden.
For the Z user every type is a set, but the elements of that set will not in general be known to be sets (they are known to be sets only for the powerset construction).

In these type constructors there is some redundancy, since n-tuples are considered identical with schemas with corresponding numeric component labels.
The types are therefore the smallest set which can be constructed from the given sets by application of power set and schema type constructions.

One further elaboration is the type genericism in Z.
The paragraphs constituting a Z specification may be parametrised by sets, and the objects defined can then be instantiated by supplying actual sets in place of these parameters.
The type of the Z entities defined must then reflect this set genericism.
A generic type is then a finite sequence of parameter names together with a monotype in which these names may appear as if they were given sets.

The most important of these types in shaping the character of the Z language is the schema type, whose role I will therefore discuss in greater detail in the next section.

\subsubsection{Schema Types}

An early stage in the development of set theory for use in pure mathematics is the selection of a method for representing ordered pairs (of sets) as sets.
This is normally done using a method devised by Sierpinski, though other methods are available.
Once order pairs are available the construction can be iterated to give arbitrary n-tuples.
Ordered pairs are used in the representation of functions as sets, and thence sequences and a wide variety of complex mathematical (or data) structures.

When mathematics is used for modelling computer systems the use of tuples is pervasive and the problem of remembering the significance of each element in these tuple becomes an impediment to ease in understanding the mathematics.
In programming languages (particularly and originally in such commercial programming languages as COBOL) this problem of intelligibility is alleviated by the use of tuples with named components (sometimes called structures or records).

The most important single innovation of the Z specification language relative to first order set theory is the introduction of labelled tuples, which are called in Z {\it bindings}.
Set of these binding, which are called {\it schemas} are ubiquitous in Z specification and give the language its special character.
The term {\it schema} is used both for the type constructor which gives a type of bindings, and for an object whose type is the powerset of a schema type (i.e. for subsets of schema types), as well as for a special kind syntactic object (a box containing a signature and a formula) which denotes a set of bindings.

As well as being used simply for types or sets of data objects with named components, schemas have special roles which exploit the correspondence between sets and properties or predicates.
A may be defined in Zermelo set theory using separation of the elements satisfying some definite property from some already available set.
Once a type system is imposed on set theory, sets of any type of value can be defined taking the type as the set from which separation takes place, and a bijection is established between typed propositional functions or predicates and typed sets.
As a result of this correspondence a set of binding can be taken to represent a property, the property of being a member of that set.

This also provides a convenient way of defining a schema.
The schema may be defined by providing:

\begin{itemize}
\item a {\it signature} which identifies the names of the components of the bindings in the schema together with the a set for each component from which the values are drawn (and which determines the type of the values of the component)
\item a formula, called in Z the {\it predicate} in which the names of the components may appear as free variables, and which is required to be true for every binding in the schema when the components of the binding are assigned to the appropriately named free variables in this formula.
This is analogous to the ability in some programming languages to use structured objects which can be ``opened'' in some context, effectively assigning the value of each component to a local variable of that name and making it possible to access these values without reference (in the relevant context) to the structure from which they are drawn.
\end{itemize}

As well as using formulae with free variables as a way of defining a schema having bindings with a matching signature, the opposite effect is possible.
A name whose value is a schema can be used in a formula as a shorthand for the formula defining the schema.
When this is done, something logically strange happens, a formula contains for logical purposes occurrences of free variables which do not explicitly occur in the formula.
This is a very special feature of Z, making the kinds of specification written in Z much more concise than they might otherwise be.
It does however present some special difficulties from the point of view of embedding the language into some more conventional language such as HOL, and for defining or implementing a sound deductive system for the language.

To explain (as we will do shortly) how this exotic use of covert or implicit variables can be properly interpreted in HOL it is convenient to describe how its use in Z can be eliminated to yield a Z specification in which all occurrences of variables are explicit.
To do this it is necessary to have a notation for an explicit binding display, i.e. a notation in which the desired values of the components of some binding can be combined to yield a binding having those components.
Such a notation was not available in Z at the time of the development we are describing.
The notation we use here was added to the Z implemented in ProofPower, a different notation was eventually added to Z during the ISO standardisation process.

For our discussion a binding display will be presented as a bracketed sequence of comma separated pairs, each pair consisting of the name of a component of the binding and the value which that component is to take, with the $\widehat=$ symbol appearing between the two.

Thus a binding with two components one of which is called `one' and has the value 1 and the other `two' having the value 2 is written:

=GFT
(one  1, two  2)
=TEX


It should be understood that the names in the above schema display are not variables.

There are two notations in Z which introduce covert variables and which we therefore wish to explain in terms of equivalent Z with explicit variables.

The first is the ``theta term''.
A theta term is the name of a schema (i.e. a variable whose type is the power set of a schema type) immediately preceded with the Greek letter theta, e.g. ${\theta}TWO$.
If $TWO$ is a name which has been given to the type of bindings to which the above schema display belongs, then this can be understood as an abbreviation for the following binding display:

=GFT
	(one  two, two  two)
=TEX

In this expression the occurrences of $one$ and $two$ to the right of the definition symbols are variables (free in this context).
Thus the expression ${\theta}TWO$ must be considered as containing two hidden or implicit occurrences of variables, and its value will be determined by the values of these variables in the context in which the theta term appears.

The second kind of notation in which variables may be implicit is the ``schema reference''.
A schema reference is simple the use of the name of a schema as if it were a formula in which the names in the signature of the schema type occur as free variables.

This should simply be understood as an abbreviation of the formula which asserts the membership of the corresponding theta term in the schema.
Thus, if the name $TWO$ when declared as described above, is used in a context where a formula would be expected, then it will be understood as the formula ${\theta}TWO \in TWO$, which in turn should be construed:

=GFT
	(one  two, two  two)  TWO
=TEX

\subsubsection{Undefinedness in Z}

Z was originally construed as a first order set theory, with added types (which makes it higher order) but certain constructs in the language are naturally construed as having no value.
For example a definite description which is not satisfied by exactly one value.
There are ways to treat these features within the context of a first order set theory, but when the first explicit accounts of the semantics of Z were published they were inconsistent with the classic first order interpretation.

Before looking at these it will be useful to explain the natural treatment in first order logic.
The key to these is the common method of introducing the axiom of choice using a choice function.
In this method a new function symbol, let us write it $$ (Hilbert's epsilon), is introduced and the axiom of choice is expressed:
=GFT
	s (e e  s)   s  s	
=TEX
The choice function is a function over all sets which when applied to a non-empty set yields an element of the set.
From the axiom it is not possible to determine which element is chosen for a set which has more than one element, or the value of the function when applied to the empty set.
This means that these details of the choice function will vary across different models of the theory.
But nevertheless, in every model (indeed in every interpretation, by definition) the choice function is a total function defined over all sets.

In some early formulations of Z the function $$ was described as a choice function 
However, in Spivey's publications it is merely definite description, so that the axiom describing it is more like:

=GFT
	s (1e e  s)   s  s
=TEX

Using definite description application of a function represented by a set which is its graph to some argument can be defined, and function application, even for partial functions, becomes a total function.
Function application having been rendered as a loose total function all other term constructions (such as set or function abstraction, become unproblematic and all the normal behaviour of two valued classical logic is preserved.

In Spivey's treatment definite description is give no value if applied to other than a unit set.
Term constructors are {\it strict}, i.e. an undefined value supplied to such a construction renders the result of the construction undefined.
The classical two-valued logic is then recovered by the behaviour of the primitive predicates equality and membership, which when supplied with undefined arguments are deemed false.

Though this has some aesthetic appeal it is inconvenient.
It is inconvenient especially in an proof tool which makes extensive use of equational rewriting, for we lose the universal reflexivity of equality.
The effects ripple through the logic and we find that many kinds of elementary reasoning the definedness of the terms involved must be proven, though in a more purely classical logic the reasoning would go through without need to establish well definedness.

The following tables illustrate the effects.

The following tables summarise the treatment of partiality as it would be if Z were viewed as a first order set theory and as it is presented in the two books by Spivey \cite{spivey88} and \cite{spivey89}.

The aspects of the language which characterise its treatment of partiality:

\begin{description}
\item[UDP] are there undefined predicates (formulae)
\item[UDT] are there undefined terms
\item[ROFV] what is the range of the free variables (does it include undefined values)?
\item[=] How does equality behave with undefined operands?
\item[] How does membership behave with undefined operands?
\item[$\{\}$] How are undefined values treated in set abstractions?
\item[$$] How are undefined values treated in functional abstractions?
\item[$$] How are definite descriptions treated in functional abstractions?
\end{description}

The following additional abbreviations are also used in the table:
\begin{description}
\item[D] free variables range over defined values only
\item[U] free variables range over defined values and `undefined',
\item[C] `classical'
\item[S] strict (for terms) false on undefined (for predicates)
\item[LO] loose
\item[LI] liberal (not strict)
\end{description}

=GFT
ISSUE	FOL	Blue book	Red book
UDP	No	No		No
UDT	No	Yes		Yes
ROFV	D	U?		U?
=	C	S		S
	C	S		S
{}	C	S		S
	C	S		S
	
=TEX


\section{Interpreting Z in HOL}

A typed polymorphic set theory is logically similar in strength to a polymorphic simple theory of types, and so in principle one ought to be able to interpret Z in HOL.
The challenge is to devise an interpretation which works well in practise, i.e. which can be implemented in a proof tool yielding convenient efficient support for proof in Z.

Interpreting one logical system in another is something which logicians do for theoretical purpose.
The kind of interpretation needed to provide proof support for one language in another is not exactly the same kind of thing.
A typical reason for a logician to interpret one system in another is to establish their relative proof theoretic strength (or obtain a relative consistency result).
For such proof theoretic motivations semantics is not important, these result are relevant even to uninterpreted formal systems.
What is essential in these proof theoretic applications is well defined deductive systems, it is the theorems which are ``interpreted''.

In the context in which ProofPower support for Z in HOL was implemented this was not the case.
The semantics of Z was known reasonably well, by extrapolation from the partial semantics provided by Mike Spivey in his doctoral dissertation, published as the book {\it Understanding Z}\cite{spivey88}. 
But there was no comparably extensive documentation of a deductive system for Z, and there were some very novel features in the language which might be expected to make the establishment of such a deductive system to be fraught with problems.
In this context a semantic embedding of Z into HOL had the great advantage that it promised sound reasoning in Z via derived rules of the well established HOL logic.

The kind of interpretation which is of interest to us here is therefore a semantic interpretation, of a kind which is now known as a semantic embedding.
The discussion which follows has more the flavour of computer science or software engineering than of mathematical logic and proof theory.

\subsection{What Kind of Embedding?}

It is possible to approach this in several different ways, and not very easy to second guess which of these is best (to some extent it depends upon the intended applications).

There are two interconnected initial choices which must be made.
Firstly, between a {\it deep} and a {\it shallow} embedding.

In a deep embedding the semantics of the embedded language is completely formalised in the supporting language, in this case, the semantics of Z would be coded up in HOL.
This would involve introducing inductive datatypes corresponding to the kinds of phrase in the abstract syntax of Z and defining valuation functions over these types yielding values in suitable semantic domains.
Each sentence in Z would then be translated into the sentence in HOL which asserts the truth of the sentence in Z.

In a shallow embedding the mapping from the interpreted to the interpreting language is defined in some suitable metalanguage rather than (as in a deep embedding) in the interpreting language.
In this case fragments of the semantics of Z are coded in HOL by a translation written in the metalanguage ML.
For each constructor in the abstract syntax of Z a constant in HOL is defined which captures the semantics of that constructor, and phrases in Z which are made with that constructor are mapped to terms in HOL which are applications of the constant which captures its semantics.
Thus, in a shallow embedding, the detail of the semantics of Z is coded into HOL constants, but the top level of the semantics where the values corresponding constructor are combined into a single semantic function for the phrase type in question are implemented in effect in the metalanguage rather than the object language.
A second important choice concerns the correspondence between types in Z and types in HOL.
Thought there are doubtless compromises which might be considered, at the extremes there are the possibility of choosing a distinct type in HOL to represent each type in Z, or the possibility of using a single type in HOL to represent the entire value space of Z.

These two choices are interconnected in that a deep embedding requires there to be at most one type in HOL for each phrase type in Z (phrase types are things like {\it formula} or {\it term} and are therefore much coarser than the types in the Z type system, which are all types of Z terms).

This leaves us with a choice among the three following alternatives:

\begin{itemize}
\item deep embedding
\item shallow embedding into small number of types
\item shallow embedding with type injection
\end{itemize}

Particular difficulties and benefits attend each of these alternatives.
Among these are:

\begin{itemize}
\item A deep embedding permits reasoning about the embedding (i.e. about the semantics of Z) in HOL, but for that reason requires a non-conservative extension to strengthen the HOL logic.
A more tangible disadvantage is that questions of type correctness in Z which are essential for sound reasoning will be pushed from the metalanguage into the object language and may make reasoning in Z more complex.
As against that, the difficulties which will be noted below in relation to use of a type injection are avoided.
\item A shallow embedding using a type injection gives a closer relationship between the type systems of Z and HOL, permits the embedding to be undertaken without strengthening the HOL logic, and may involve less reasoning about types during proofs.
A particular difficulty here is that the schema type construction does not map easily into HOL, and we end up having to use a family of type constructors to get the type injection.
\item A shallow embedding into a small number of types provides yet another combination of advantages and disadvantages, but was in fact considered at the time.
\end{itemize}

Without practical experience of the workings of these different methods with these particular languages it is not easy to know which would be best.

One advantages of a deep embedding is that it permits formalised metatheory, i.e. reasoning about the semantics of Z, whereas the shallow embedding is oriented to reasoning in Z about things specified in Z, more closely analogous to the kinds of application (e.g. in hardware verification) for which the Cambridge HOL system was developed.
Use of a type injection results in a better correspondence between type correctness in the two languages (a Z specification is type correct iff its image under the mapping is type correct HOL.
When working within {\it the LCF paradigm}, computations involving syntactic values of the object language are routine, and when a term is constructed from its constituents the typing rules are checked so that no type-incorrect terms can be constructed.
With a shallow embedding of Z into HOL in this kind of context, computations involving the HOL terms which represent Z formulae or terms are automatically type checked as a result of the checks on the underlying HOL.
More important than whether type checking comes for free, is the extent to which type checking may be pushed from the metalanguage into the object language as a result of a deep embedding.


For ProofPower the shallow embedding with type injection was chosen, this has worked pretty well, but we still don't know for sure how the other approaches would have worked out.

\subsection{The Type Injection}

The main problem in constructing a type injection is the fact that the schema type constructor in Z takes as its parameter a finite map from component names to component types, whereas type constructors in HOL take a finite sequence of types, and cannot be supplied with a map.
The Z type system is anomalous in relation to schema types and the operations over these objects, since schema operations do not have a single type in the Z type system, not even a polymorphic or generic type, but have to be considered either as having a family of types indexed by compatible operand signatures or as consisting of family of operators, each having a different type.

To deal with this in the injection into HOL a bijection between the types in Z and HOL is achieved even though there is no bijection between the type constructors, and families of constants are used for the schema operations.

The bijection is achieved using a family of constructors in HOL for the schema type constructor.
The signature of a schema type is partly coded into the name of the type constructor, which contains a canonical encoding of the names in the signature.
The types associated with the names in the signature are passed as parameters to the HOL type constructors in a canonical order determined by the names of the components.

The power set constructor is easily constructed in HOL, sets are represented by boolean valued functions.

Generic types in Z are mapped to function types.
Thinking of a Z generic type as a tuple of formal type parameters together with a Z mono-type in which these type variables may occur, the image of such a generic type is the HOL function type in which the domain type is a tuple of type variables corresponding to the formal generic variables, and the range type is the image under this injection of the Z mono-type.

\subsection{Mapping Formulae and Terms}

The broad pattern for the mapping of formulae and terms is as follows.

Where possible, a construct in Z is mapped directly to the corresponding construct in HOL.
This happens mainly for the propositional connectives.
Sometimes it is desirable to put in a dummy constant which is in effect a name for the identity function so  that tools processing a construct will be aware of the language in which it occurs even though the constant is the same in more than one language.
This happens for example, for the universal quantifier.
Though the Z universal quantifier does map to a HOL universal quantifier, the form of the body of a Z universal quantifier is special, and so it is desirable to inhibit processing the HOL universal quantifier in the normal way by wrapping it in a special constant serving only to mask it from procedures not intended for Z.

In general the pattern for the mapping involves defining a new constant or family of constants for each constructor in the abstract syntax of the Z language which correctly captures the semantics of that part of the Z language.
Complications to this arise mainly in connection with the Z constructs in which variable occurrences are implicit, and the variable binding constructions of the Z language (which also allow for these implicit variables).

The variable binding structures in Z all admit, instead of single binding occurrences of variables, an arbitrary signature, which will include set constraints and may include the use of schema expressions as declarations.
These must all be mapped down in a semantically correct way to a language in which there is only one variable binding structure (the lambda expression) which binds a single variable subject only to a type constraint.The image of a variable binding structure must include a nested lambda expression in which all the names are bound which are bound by the Z binding (explicitly or implicitly.
In the body of this expression will appear the translation of all the Z which is in the scope of the binding, in which all semantically relevant information is made explicit.
If this involves several constituents then these must be combined together in the body of the lambda expression in such a way that they can be separately accessed as necessary by the semantic constant corresponding to this kind of Z construction (which will be applied to the resulting lambda expression).
Because these variable binding constructs bind arbitrary numbers of names the type system makes it impossible to code up the semantics in a single semantic constant, and a family of constants indexed by the number of variables bound is usually required.

A good example is the Z lambda expression.
The lambda expression in Z has up to three explicit top level constituents, which are:
\begin{itemize}
\item[d] the declaration part or signature
\item[p] a predicate which further constrains the domain of the required function
\item[b] the body of the lambda expression giving the value of the function
\end{itemize}

The translation of the declaration part must yield three separate semantically significant values.
The first is the set of names which are bound by the declaration.
The second is the predicate implicit in the declaration, roughly the predicate which asserts that each of the names is a member of the set of which it was declared to be a member, or, when combined with other declared names in a binding is a member of a schema used in the declaration.
The third is a tuple of variable names formed according to the prescribed rules which indicates the structure of the required arguments to the function.
This latter is implicit in the syntactic form of the Z lambda expression, but since the syntactic form of the HOL lambda expression has no such semantic significance, this information must be made explicit in the mapping.

The method of combining constituents into a single value for use in the body of the lambda expression is to combine them as a binding.
A tuple would have done just as well, but the use of a binding allows slightly suggestive component names to be used.
In the case of the lambda expression the component names are the letter used above in enumerating the explicit constituents, together with the letter ``t'' for the tuple implicit in the Z declaration.
The bound structure is therefore a higher order function which takes values for the bound variables in turn and which then yields a binding the components of which give:

\begin{itemize}
\item[d] whether the values of the variables satisfy the predicate implicit in the declaration
\item[p] whether the values of the variables satisfy the explicit predicate
\item[t] the value in the domain of the required function which corresponds to the values of the variables
\item[b] the value of the body of the lambda expression, and hence of the function if it is defined at this point
\end{itemize}

The semantic constant which is applied to this function must convert it into a set of ordered pairs which is the graph of the required function.
An ordered pair {\it p} will be a member of this set if there exists a set of values for the variables which when supplied to the function gives a binding whose b component is the second element of p, whose t component is the first element of p and whose d and p components are {\it true}.

\subsection{Undefinedness in Z in HOL}

Z was originally supposed to be a convenient syntax for first order set theory, and its logic was considered a classical first order logic in which there are exactly two truth values and every term denotes some value from the domain of the interpretation.

There was however no systematic published account along these lines, and when the first reasonably complete accounts of the semantics of Z were published some small deviations from this position were evident.
The accounts in question were, firstly that in Michael Spivey's doctoral dissertation, published as the book {\it understanding Z}\cite{spivey88}, and then two editions of {\it The Z Notation}\cite{spivey89} by the same author.
The first edition of the latter appeared in between the availability of the doctoral dissertation and its publication in book form.

In brief the differences between these accounts in relation to the question of undefinedness are as follow
Firstly, in his doctoral dissertation Spivey broke with full conformance with a first order semantics by allowing that some terms have no denotation.
The logic remains two valued, undefinedness disappearing at the primitive relations (equality and membership) which were deemed false if either of their operands failed to denote.
In {\it The Z Notation} Spivey modifies his position, and leaves the value of the primitive predicates undetermined if either operand is undefined.

Its worth noting how this would work in a completely standard first order set theory.
In such a theory every term must always denote a value in every interpretation of the language.
Of a function such as function application is included in the language, then an application of a function to a value which is not in its domain must yield in every interpretation some value in the domain of the interpretation.
One way to fully define such a function would be to say that the value of a function when applied to an argument for which it is not single-valued is the value of the function itself (this has the advantage that because of the well-foundedness of Zermelo set theory this could not possibly be the value of the function at any point for which it is single valued).
However, it is not desirable that an arbitrary choice of this kind should be made definite so that it can be exploited in specifications.
It is preferable to leave this aspect of the application function undetermined, saying that it is indeed a function, and does indeed yield a value in every case, but declining to say what that value is in the cases where the function is not single valued (or has no value).

This looks like a fine case of hair splitting, so I had better make haste to explain why this is in fact of practical significance.

When we come to implement a proof tool for this language, especially as in our case, based on HOL which makes extensive use of rewriting with equations, the unadulterated first order logic is significant.
It is significant because it guarantees with absolutely no exceptions that for every term ``t'', ``t = t'' is true and is a theorem.
If we insist that every term denotes, then universal quantification embraces all terms, otherwise we find that when a universally quantified result is instantiated it is necessary to prove that the term to which it is instantiated denotes some value.
There are very many logical rules which can be applied without consideration of whether the terms under consideration are well defined, {\it provided that} the semantics holds fast to first order logic, but which cannot be applied without first demonstrating that the terms involved denote if non denoting terms are admitted.

\section{Implementation}


\subsection{Using Cambridge HOL for Proofs in Z}

International Computers Limited was a company put together by the British government in an attempt to rescue a declining UK computer industry.
It was formed by taking all the parts of UK industry which were involved in the design and manufacture of commercial computers and making a single manufacturer.
In the process of putting the company together a deal was struck between the various companies involved, that they would not compete with ICL in the commercial sector for a certain period of time (must have been about 20 years) and in return ICL agreed not to go for the non-commercial sector (mainly military computing).

As this period drew to a close ICL geared up to enter its new markets with some enthusiasm.
It had in its portfolio some products, such as its distributed array processor (DAP), which had clear military applications.
A Defence Technology Centre was established to undertake the research and development which was though essential to success in defence.

At this time the UK government was engaged in the promotion of capability in UK industry for the development of secure computer systems by methods similar to those advocated and under development in the USA.
To this end the government were placing contracts with UK contractors to undertake research and development in the area of highly secure computer systems.
The application of formal methods was thought to be essential, the aim being to specify formally the critical security properties of such systems, to formally specify the relevant aspects of the design of such systems, and to mathematically prove that a system implemented according to the design would meet the security requirements.
Accordingly ICL established in its Defence Technology Centre a Formal Methods Unit intended to provide the necessary technical skills for developing systems in the approved manner.

It was generally supposed that the application of mathematics to the verification of computer systems would require machine checked formal proofs, rather than the more conventional less formal proofs found in mathematical journal and checked by peer review.
Consequently, computer based proof assistants were considered essential, and as early as possible the formal methods team sought practical experience in the application of such tools in their problem domain.
At this time the government had already chosen to promote the use of a single formal specification language to be used for this kind of security work.
Having taken advice from its consultants they had selected for this purpose the Z specification language.

No proof tool was then available which supported Z, so ICL sought a tool with which experience could be gained by manually translating specifications from Z into a language supported by the tool.
Time was of the essence, and not much of it was available for surveying proof tool.
The choice was made between NQTHM, otherwise known as ``the Boyer-Moore theorem prover'' after its principle authors, and HOL.
NQTHM was thought to have a more powerful automated proof capability, but the language supported by this tool was an impoverished first order logic and the translation of Z specifications into this language would be likely to yield a specification quite different in form to the original.
The Cambridge HOL proof assistance supported higher order logic, a language much closer in expressiveness to Z, and thought the level of proof automation was not so high as NQTHM, the implementation via {\it the LCF paradigm} made the system seem more adaptable and made it seem more likely that the tool could readily be adapted to use for reasoning about specifications originally written in Z.

ICL therefore began to experiment with the application of Cambridge HOL to its Z specifications of secure systems.
This was done in a rather {\it ad hoc} way.
Firstly some facilities were coded using the metalanguage ML to facilitate use of the kinds of information structures which were common in Z.
The most pervasive of these is the schema type, which is a kind of labelled product, which could be introduced in HOL by introducing a new type represented by an iterated ordered pair in relation to which appropriate constructors and projections were automatically introduced.
The definition of schemas in Z is then readily translated into the definition of properties over these labelled product types.
A second important kind of construction to be translated was the Z free type, which in its non-recursive applications is a labelled disjoint union and could be supported in similar ways.
There was at this time no support for recursive datatypes in HOL.

The other major feature of the Z language, at the paragraph level, for which support was needed, was the axiomatic specification.
An axiomatic specification consists of the introduction of one or more new ``global variables'' (which translate into HOL as constants) together with an axiom jointly constraining these new global variables.
There is here a small clash of cultures.
The culture around Cambridge HOL reflected its ancestry as a foundation system for mathematics, in which mathematics was expected to be undertaken by the use of definitions only, not new axioms.
The more liberal attitude of the Z community to the use of axioms seemed less appropriate for the intended applications, in which a great deal of weight was to be placed on formal reasoning about the system.
In order to guarantee that the consistency of the logical system could not be compromised by errors in the specification, ICL sought to translate the axiomatic specifications by conservative means.
This was done by translating them using the choice function in HOL.
A Z axiomatic specification was translated into a definition in which a new constant was defined as {\it the} value which is a tuple whose components satisfy the required predicate.

Use of the choice function in this way was safe, so far as consistency was concerned but was not an entirely faithful translation of the Z, in two respects, one desirable and the other regrettable.
The desirable lack of faithfulness concerned the translation of non-conservative axiomatic specifications.
If the consistency of the predicate is not provable in HOL, then the desired properties of the newly defined constants cannot be established.
This is what makes it safe.
The undesirable feature is that some identities are provable which should not be.
This occurs when two different global variables are introduced using the same loose (not uniquely satisfied) property.
For example in two separate paragraphs, two distinct global variables {\it a} and {\it b} are introduced each of which is require to be an even integer but is otherwise unspecified.

When this is done in Z, one knows only that {\it a} and {\it b} are both even integers.
If these loose specifications are translated into HOL using the choice function, it becomes possible to prove that the two constants have the same value.
Support for faithful translation of such specifications was later realised by the introduction of a new facility into Cambridge HOL, called constant specification.
In ProofPower this was to be the only primitive form of constant definition.

Certain other cosmetic measure were adopted to make the correspondence between a specification translated into HOL and the original Z specification easier to see.
The first of these was simply limited presentation of the HOL specification as printed via \LaTeX in a style similar to that of Z paragraphs, which appear in various kinds of box.
The second concerns the presentation of logical symbols, which in HOL are given using ASCII characters, but in Z use non-ASCII mathematical symbols.
A new character set was devised which contained the most frequently used of the special symbols, documents were prepared using this font.
The special characters were translated for printing into appropriate \LaTeX macros, they were translated when presented to HOL into the ASCII characters required by HOL.
This hack, was later cleaned up to realise a compromise between a fully graphical interface and a purely ASCII interface which is now found in \Product.

Practical experience in reasoning about Z specifications via these experimental hacks was invaluable in securing a good practical understanding of the semantics of the Z specification language and its relationship with HOL.
Without this prior experience the FST project under which \Product was developed would not have been possible in the available timescales.

Throughout this process we were aware that in principle a completely faithful interpretation of Z in HOL was possible.
It was clear that such a mapping could not be used in practise without more radical changes to the underlying HOL system.
In general, in a systematic mapping, the resulting HOL terms would be too complex to be recognisable or intelligible, and a systematic mapping would need parsers, pretty printers and proof facilities implemented specifically to support the Z language.
This kind of undertaking could not be undertaken on the shoe-string budget under which these experiments in translating Z into HOL had been conducted.

\subsection{The FST project}

FST is an acronym for the project number 1563 assigned to the project by the Department of Trade and Industry which partially funded the work, chosen arbitrarily at a significant saving in creative effort.

The purpose of the project as a whole was to further the development of capability for formal machine checked verification of software and or digital hardware.
The objective of ICL, which was the leader and largest contributor to the project was to completely re-engineer proof support for HOL, the language supported by the proof tool of the same name developed at Cambridge University, and to use this re-engineered tool to provide best achievable support for proof in the Z specification language by semantic embedding into HOL.
The name of the software product which ICL was to produce under the project was not decided until later.
My first choice was to call the tool {\it Principia} recognising its logical system as a direct descendant of that of {\it Principia Mathematica}\cite{whitehead10}, but none else seemed enthusiastic about that name.
At that time ICL had a range of software products with names such as {\it OfficePower}, {\it DecisionPower} and so on, so for a new ICL product something with {\it Power} at the end was an easy choice, and a tool for doing proofs might as well be called {\it ProofPower}.
This proposal met with little enthusiasm or resistance, so that was it.

The other participants in the project were the Universities of Cambridge and Kent, and Program Validation Limited.

The University of Cambridge was to undertake under the supervision of Martin Hyland, some theoretical investigations into various type theories, which ICL was to work with an established type theory, Cambridge would look into more elaborate type theories which might at some time in the future provide a superior base for this kind of undertaking.

The University of Kent, under the lead of Keith Hanna was to continue its work on the development and application of tools supporting formal specification and verification of digital hardware using classical dependent type theories,
Keith Hanna's positive achievements with his VERITAS system had already been instrumental in persuading Mike Gordan to take up a higher order logic for his work on hardware verification, but no clear consensus had yet emerged on the relative merits of the simple type theory which Gordon adopted against the dependent type theories in use by Hanna.

Program Validation Limited (PVL) had extensive experience in development of software to support static analysis and formal verification of programs written in the SPARK subset of Ada.
PVL were to undertake further development of their own proof tool.

My concern in this essay is exclusively with the ICL part of the project leading to proof support for Z in HOL.

\subsection{Some Preliminary Decisions}
We had three years to produce an effective proof tool for the Z specification language, so we didn't really have time to debate the main features of the approach, which had already been laid out in the proposal.

\subsection{The Implementation of ProofPower HOL}

\subsection{Implementing Z in HOL}

\section{DAZ and CLAWZ}

\appendix

\chapter{An Interview}

\section{Introduction}
The following interview was never intended for publication%
\footnote{
The editing of the transcript has been done by me.
In doing this I have made modest changes to make it easier to read and to correct definite errors in the transcription, removing only the most conspicuously superfluous material.
These include, removal of multiple ``I mean''s, substitution of ``yes'' for ``yeah''!
I have also added footnotes, sometimes to explain what I {\it really} meant at the time or to give a bit more relevant information, sometimes to give my present view.

}, but was undertaken as part of a research project directed by Professor Donald MacKenzie of the University of Edinburgh%
\footnote{
and was later cited in his book ``Mechanising Proof''\cite{mackenzie}}.

\section{The Interview}

\ \\

Interview with Roger Jones, ICL, 8.3.94.

Interviewer: Tony Dale 

\subsection{Academic Background} 

I: So, first of all, can you tell me about your own academic background from university onwards, please?

RJ: Sure. Now, I've got a rather peculiar university thing, because I went to Cambridge from school to do engineering, but very rapidly decided that I wasn't interested in engineering. So that was long before Cambridge did computer science for undergraduates, and in fact, there was only... I think there was one university in the country that did computer science at the time.

I: What year was that? What year are we talking about?

RJ: That was 1966.

I: '66, right.

RJ: And I left at the end of the first year, and took a job with English Electric. And I worked in the computer industry for five years, and then I went back to university at Keele.

I: So, just a minute: you left Cambridge after one year, went into the computer industry. Where?

RJ: English Electric. Nelson Research Labs at Stafford.

I: And what were you doing with computers, then?

RJ: We were doing research into compilers, and compiler compilers. That was one thread. And we were also doing work on software for analysis of power transmission systems.

I: All right, so you were doing... you were working... you were actually producing software?

RJ: Yes. Well, that was my first job, and I transferred to ICL%
\footnote{
International Computers Limited, now absorbed into Fujitsu}%
after about eighteen months, something like that, and initially continued that work%
\footnote{
the work on power transmission systems analysis}%
, and did more work with compilers after that, and worked with compilers for producing compilers%
\footnote{
I wasn't working on Compiler Compilers at ICL, so I don't know what this means!}%
, mainly, until I went back to university in 1972.

I: And you went back to university to do... ?

RJ: Well, I intended to read maths and education.
I intended to do some teaching.
But I went to Keele which had a four-year degree course with a foundation year first, and I was going to do a teaching certificate concurrently.
But during the first year I changed my mind about what I wanted to do - which was one of the advantages of going to Keele - and so I did maths and philosophy, joint maths and philosophy.

I: Oh right, gosh. Who taught you the philosophy there?

RJ: Well, the professor was Swinburne.

I: Oh, yes, he's a colleague - ex-colleague - of mine.

RJ: And the logician was Treherne%
\footnote{
Alan Treherne, still at Keele as of 2010.}%
.

I: Treherne. I don't know him.

RJ: Well, he's still at Keele. He's transferred. He was at Edinburgh as well, actually. He was at Edinburgh before. He's been at Edinburgh; he's been at Imperial College; he's been in the philosophy department at Keele, and the last I heard he was in the computer science department at Keele. And I think he did... well, he did computer science at Keele and at Edinburgh, I think. So he's got a similar... he's one of these people who hovers around between maths and philosophy and computer science.

I: I know the sort. And did he teach you standard first-order logic at that point?

RJ: I don't remember whether he taught the logic courses earlier on in the course, but my special subject in the final year was philosophy and mathematics, and he did that. Philosophy: a foundation course, or something like that.

I: And the maths you did was what, pure maths, or applied?

RJ: Well, a mixture.

I: Oh, you have a mixture at Keele, do you?

RJ: Yes, a mixture of pure and applied.

I: Right. So you were given a fair spread of mathematics, and presumably a fair spread of philosophy too?

RJ: Yes, though only half a degree's worth of either, so...

I: Was it concentrated more or less on the logic/philosophy of science end in the philosophy?

RJ: No, I don't think so, no. It was the standard philosophy course they do at Keele, most of which students are doing other subjects, like generally Arts subjects rather than Science subjects.
I don't know what the mix is, but at the time everybody at Keele did two subjects%
\footnote{
Two {\it principle} subjects leading to joint honours, after a foundation year of very general coverage.}%
, and so... I mean, I guess I've forgotten most of the stuff that I don't really use that was done there, so I don't remember all the stuff we were taught in the philosophy.%
\footnote{I specialised in mathematics in the sixth form partly because I was good at it, but even more because I have an appalling memory and wanted to avoid Chemistry.
Its a miracle that I ended up with a good joint honours degree with Philosophy.}

I: It was quite a change from computers, which you had been doing previous years. Why did you give up the computers?

RJ: I didn't think I was getting where I wanted to be. I wanted to change, at the time.

I: And after your degree, you didn't go back into teaching, as you'd thought, but...

RJ: Well, I knew I wasn't going to do that after the first year there. I changed my mind about teaching rather rapidly.

I: A wise decision!

RJ: Yes. I went to Warwick.

I: To do what?

RJ: To do a PhD.

I: To do?

RJ: I went to the computer science department at Warwick, which had - and probably still does have, though I don't hear much about them nowadays - had a strong theoretical group under David Park. The research topic that I identified and sought a suitable university for was basically about mechanisation of mathematics%
\footnote{
See Chapter \ref{ResearchTopics}.}%
.

I: The mechanisation of maths. Had you been thinking about this topic, then, during your degree, or... ?

RJ: Yes.

\subsection{Logicism and Formal Proof}

I: What had led you to think about that? Was it the previous experience in the computer industry?

RJ: Well, it was a combination of the fact that I came from a computing background prior to going to university, and the fact that I was interested in the foundations of mathematics and logicism, and things like that, so it just seemed... it just seemed the right way to do maths.

I: From a logical buildup of step-by-step... you mentioned logicism itself.
You were talking about a particular philosophy of maths.
That particular philosophy appealed to you?

RJ: Oh yes, certainly.
I'm a dyed-in-the-wool logicist to this day.

I: Are you?

RJ: Yes.

I: You actually believe that mathematics is nought but logic?

RJ: Well, I guess that's an extremely ambiguous phrase.
Of course mathematicians do lots of things other than logic, but I think it's an important characteristic of mathematics that it is, effectively, reducible to logic in an appropriate sense.

I: Right.
The appropriate sense being?

RJ: Well, I think the key issue, I think the key thing is, that it's all... it is all a priori, and in practice you can derive all of... or you can derive all of mathematics in a suitable foundation system into a set of... I mean, we have to strengthen it when you want to take your mathematics a bit further, but in practical terms you can... not only do you have very hard criteria for truth and falsity of mathematical conjectures, but they're mechanisable, and you can build tools which will basically tell you whether you're right or not in your claims about the truth, not in the sense that they will tell you whether an arbitrary conjecture is true or false, but if you claimed to have proven something, then that's a testable claim, a mechanically-testable claim.%
\footnote{As you can see from the floundering here, I was not expecting quite such tricky questions in this interview.
What I should have said then, was ``in the sense that you can do most mathematics with a tool like ProofPower\index{ProofPower}''.
What I would say today is simply that is obvious (pace Quine, Boolos, ...) that mathematics is analytic (which was my belief even at that time). 
}

I: So these are proof checkers you're talking about now, that you can actually run... put it in a suitable form for... ?

RJ: You're always talking in the realm of hybrids if you're talking in the realm of practicality.
No practical system is purely a proof checker.
You've got something that's just a proof checker, then it's not going to... you're not going to see its problems.
On the other hand, you know, you're not going to get the tool to do all the proofs for you for the kind of things you want to prove.
It's got to be a collaboration.
Well, in terms of actually verifying that the proof is done... the thing you end up constructing together is a good proof, that's completely... that can be completely recognisable.
I guess, thinking back, when I went to university I was... I've always had a problem with the notion of mathematical proof - what counts as proof and what doesn't count as a proof - and I've never felt very inclined to actually get a good understanding of what mathematicians call a proof.

I: Why?

RJ: Because I knew, from the point of view of formal logic, what a proof was.
And it seemed to me that that was a viable... in the days of... in the era that I was living, you know, where in principle you could have tools which would assist you in constructing completely formal proofs and checking them, I wasn't... I really wasn't terribly interested in learning the new answers.
Well, I was also rather infuriated by it, because lecturers play games with you, you know? They will go through one proof and jump huge steps to each stage, and then all of a sudden they'll start turning round and berating you, and say... and asking you to prove the obvious. And, you know, I find that a... If I weren't already annoyed by the vagueness of what constitutes a mathematics proof, I would still have been utterly infuriated by the fact that they'd changed the rules of the game at every turn.

I: These were your lecturers at Keele?

RJ: Yes, though I should say that I didn't go to very many lectures, so I'm not a very good source of information about how good lecturers are, since I think the idea... I mean, I was also diametrically opposed to the idea that you can teach mathematics in lectures, since what has turned out to be the practice is generally that the lecturer goes along and spends an hour writing his notes on the board while you spend an hour copying them down onto a piece of paper. That seems to me to be a complete waste of time. So I didn't actually go to many maths lectures at Keele.

I: Right. And yet you had this view that somehow they were cheating because they skipped important bits and would sometimes...

RJ: Not cheating, because I don't think they had a hard and fast notion of proof there against which they could be claimed to be cheating, and clearly they don't.
It's a sociological thing.
What's acceptable is a mathematical proof for them.
But it need not be.

I: You had a formal conception of proof at this time, did you, a sequence of steps which follows by rule from the previous member or members of the sequence: I mean, that was the notion you were operating with?

RJ: Yes, I was at the time.
In my first year at Keele I was working through by hand proofs of conjectures from propositional logic.
I spent a certain amount of time working through in a different logical system the elementary results in Principia Mathematica, and so I definitely at that time knew what a real formal proof was.

I: That was an axiomatic proof you were using, or natural deduction?

RJ: Yes.

I: Axiomatic?
You actually had your axiom starting, and you came down through hardware doing proofs?

RJ: Yes, yes.
If you try and interest me in the proof of a propositional conjecture these days and I'm just not interested.
I don't want to see them.
A special proof tool doesn't show... tell you anything about those things.
And I did do... when I was still in the realm at the beginning of the maths course when you could do these things, I did much more formal proofs than they would have done, or expected normally.
But you don't get very far before that turns out to be impractical.

I: Was that because of the logic you were studying in the philosophy at the time led you to this notion of 'proof'?

RJ: No.

I: This was independently thought up?

RJ: I don't think the logic you do in the early part of the philosophy course very significant, it was prior studies of my own.

I: Right.
So how did you get this notion early on?

RJ: Well, I guess by reading, reading books.%
\footnote{I would add now, that it wasn't particularly ``early on'', since I was 24 when I went to Keele.
My Logicism wasn't precocious, I was just late doing my degree.
}%

I: You were reading?
You were reading outside the course?
People like Russell, or formalists, perhaps.
Did you read any formalists? Hilbert or Carnap?

\footnote{
The preceding section of the interview is very muddled.
Because I was a mature student I had various ideas and attitudes (and even competencies) which were unusual for a first year undergraduate.
Alan is here probing what those ideas were and how I came by them.
The answers are unclear so I will try a better explanation here.

Between my year at Cambridge and my arrival at Keele I had five years of work in the computer industry and a fair amount of exploration in my own time of diverse topics.
My knowledge of and acquaintance with logic began very soon after leaving Cambridge.
Even at Cambridge I had come across Turing, the sight of his name written on my jeans provoked the senior tutor at Churchill into reminiscences about Turing's bicycle and the unique skills required to ride it (this is the only reason I remember having known about Turing at that time).
Soon after leaving Cambridge, my acquaintance with logic and computability began with a texts by Jeffreys \cite{jeffreys1967} and Minsky \cite{minsky1967} both published and acquired in 67/68, but I believe that my knowledge of logic at this point was quite superficial.
A few of years later (1971) I read Russell's ``Introduction to Mathematical Philosophy'' \cite{russell1919}, which was probably my first exposure to Logicism.
I had read Russell's autobiography\cite{russell}, probably in 1968, and sometime later his ``History of Western Philosophy'' (probably between 1969 and 1972).
In the year before going to Keele I was using the company library to retrieve classic papers by Kleene, Turing, Church which I didn't much understand, around this time (not before 1972) I also read Ayer's ``Language Truth and Logic''\cite{ayer1936}.
This is all small beer, I am not a scholarly or prolific reader, but I did start as an undergraduate with an unusual prejudice in favour of formal proof.
}

RJ: I've not actually read much Hilbert, no.
I think most of my knowledge of formalism is second-hand.%
\footnote{And I was a logicist not a formalist.
Also I did not read Carnap before going to Keele, and probably at Keele the only bit of Carnap I read was ``Empiricism, Semantics and Ontology''\cite{carnap50}.
It was not until much later that I discovered how much of Carnap's work was devoted to formal methods.}%

I: From introductory guides to the philosophy of maths, or K\"orner%
\footnote{I think he is referring here to Stephan K\"orner's book ``The Philosophy of Mathematics'' \cite{korner1960}, which probably was the first general account of the Philosophy of Mathematics which I read.
However, I don't know when I read this, whether it was before I went to Keele or not.
}%
, or... ?

RJ: That sort of thing, and what people claim when they're writing philosophical papers, which is usually, even for somebody with only second-hand knowledge, rather doubtful.

I: OK. So you ... moved to Warwick, and you had this notion in mind of mechanising mathematical proofs along the lines of how a formal system would work?
Is that correct?

RJ: Yes.
I was certainly always dissatisfied with the notion of proof that was used by mathematicians.%
\footnote{
as opposed to mathematical logicians}%
... I don't have any recollection of ever thinking it practicable to do proofs, formal proofs, manually....

I: How optimistic were you at that stage that you could in fact mechanise mathematical proofs?

RJ: It was obvious you could.
It had already been done anyway.

I: What were you aware of at the time?
Were you aware?

RJ: I don't know, actually.
Probably at that stage... at the beginning of the degree there I probably wasn't aware of any mechanical proof systems that were available.
But it was obvious you could.
... in principle.
You could certainly code up proof checkers.

I: Right, right.
Proof checkers I can see, but proof finders would be much more difficult, possibly.

RJ: Well, ...  if you start from the viewpoint of Principia Mathematica where you're talking about it in fact being possible to do something pretty formal, albeit at a large amount of labour, it's natural to presume that if you've got a computer to help you, that you can achieve more.
And I guess until you get further into it, you don't realise how difficult and complicated it is.
Even having looked at Principia, for example, which is pretty complicated.

I: The Law of Excluded Middle comes out as 500 lines, or something, I remember.%
\footnote{
In fact it is proposition *2.1, which appears about half way down the third page of the formal proofs.
Scanning back through what is essential to get there from the axioms it looks like the proof is about 10 steps.
On the other hand, the entire paperback volume ``Principia Mathematica to *56''\cite{russell1970} only gets as far as the ordinal 2 in 400 pages, so there is no doubt that it is heavy going.
}

The Law of Excluded Middle's quite clearly a proof, isn't it, in the Principia system.
So I mean, to actually ape such a system you virtually require a heuristics of kind, ...

RJ: ... I don't generally think that using computers to ape other systems is a good scheme.
You generally find that there are better ways of doing things.

I: Can we just come back to ... what you were doing at Warwick, because we're obviously straying onto ...

RJ: OK.
So ... I looked for a university.
... I had some short description ... I couldn't dig it out%
\footnote{
I have dug it out, and it is in Section \ref{KeeleProposal} (page \pageref{KeeleProposal}).
}%
 - which was basically mechanisation of mathematics.
But it's basically about trying to get together the kind of tools you needed to do mathematics properly - what I thought was properly - but mathematicians to this day still write...
I ended up at Warwick to do that, but unfortunately I ran aground at Warwick for purely personal reasons: I got rather entangled in my relationships in a way which prevented me from doing any good intellectual work, and I left Warwick after the first year, and I figured I wasn't going to get any research done, so I might as well get a job and do something, and losing the game.
So that changed the course of my career somewhat.
And it took me completely away from the area for many years.

I: So you went out and got a job independently of computing?

RJ: ... No ... I went back to computing.
I rejoined ICL in 1977 after spending a year getting a bit of background, a bit more background, but not actually making any real progress in terms of research.
And at that time, of course, there were proof tools around.
Automath had been around for quite a while, and LCF.

I: You knew of LCF and Automath at that time, did you?

RJ: Yes, but I didn't know a lot about them.

I: Right.
And how did you come across those?
Did you come across them at Warwick, or just in general reading?

RJ: Yes.

I: At Warwick.%
\footnote{
Reading at Warwick, I don't think I tried them out.
}%

RJ: Because it was a direct development of my intended research.
So it's part of the background...

\subsection{ICL, Formal Methods}

I then spent about five years working on something completely different, which was microcoded emulation of mainframes, making 2900s%
\footnote{
ICL's ``New Range'' of mainframe computers.
}
behave like 1900s%
\footnote{
One of ICL's old ranges.
}
And then I got a bit fidgety with that, so I decided that I wanted to get back into doing something which was relevant to my interest in logic, and I started moving around within the company trying to find the right place for that.

I: They would allow you to do that, would they?

RJ: Well, if you could find the opportunity.
... you've got to find the jobs.
So I did a bit of knowledge engineering, and I did a bit of working on relational database technology.
And then I was invited to join the Defence Technology Centre, where they were forming a formal methods group.
So that was the beginning of 1986. I finally found a job where you really did use logic.

I: And this was... ?

RJ: This was in the formal methods group. In 1985 they set up...

(Interruption)

I: You were just going to tell where you would need logic in...

RJ: 
When ICL was formed - just a bit of history - ICL was engineered by the government out of lots of computer companies.
At the time, in order to give it a good chance of succeeding, there was an arrangement whereby ICL was barred from entering defence, and the people who were in the defence computing business were barred from entering commercial computing.
So that was a period to give us a good chance to get going in commercial computing where we were protected from the bits of the UK computer industry that hadn't been put into ICL, but we weren't allowed to do defence work.

...%
\footnote{
What I should have said here is that this period of agreed constraint came to an end, probably on the 20th anniversary of the formation of ICL, and so ICL began to gear up to enter defence markets.
}

There was thought to be a commercial opportunity to make a vast profit in defence.
We%
\footnote{
ICL.
}%
 set up this Defence Technology Centre, and one of the things they were doing was R\&D contracts with GCHQ\index{GCHQ} in the area of secure systems, and GCHQ\index{GCHQ} had a requirement for formal methods to be used on these contracts.
And so a formal methods group was started up.

I: At ICL?

\subsection{Z\index{Z} and HOL}

RJ: Yes.
I joined the unit at the beginning of 1986, and this was to use formal methods in the development of secure systems.
It was Roger Stokes who founded that unit in 1985, and he had this little motto or guiding principle that he wanted us to be engaged in using real tools on real problems, and ... the real problem we were ostensibly interested in was proving that certain systems were secure.
So we did take an interest in improved tools from the beginning, and we started to use them as soon as we had the opportunity to do so.
When I arrived, Roger had already been looking at the Cambridge HOL\index{HOL!Cambridge} system, and the Boyer-Moore system.
GCHQ\index{GCHQ}, or CESG\index{CESG}, had already, on advice from other consultants, decided that they wanted to go with the Z\index{Z} language, Z\index{Z} specification language.
So we knew that what it looked like was that we should be aiming to be able to do proofs in Z\index{Z}.

I: Proofs in Z\index{Z}, right, yes.
\footnote{
At that time (maybe still today) many people in formal methods thought that to reason about a formal specification, you needed something else, a {\it logic}.
Any formal language in which you can express propositions can be made into a formal logic, by codifying sound deductive rules for that language.
Cambridge HOL\index{HOL!Cambridge} illustrates this point the other way round really, it is ``Higher Order Logic'', but by some pragmatic enrichment of syntax the logic can be made into a usable specification language, so you end up with a specification language which {\it is} a logic, not a specification language {\it and} a logic.
The Z\index{Z} specification language was rather like HOL\index{HOL} in being rooted in a deductive system anyway (Zermelo's axioms for set theory), though in the case of Z\index{Z} the distance was too large for the deductive system to be obvious.
}

RJ: But there weren't any tools for that, ... and we didn't want to sit around and twiddle our thumbs until Z\index{Z} proof tools came along.

I: Presumably from PRG\index{PRG}.

RJ: Well, in fact from us%
\footnote{The Oxford Programming Research Group (PRG) were at that time advocates of paper and pencil.}%
, ... we decided we were going to choose the most appropriate tool and if necessary transcribe from one language to another in order to able to do some proof work.

I: Which languages are you going to and from?
Sorry, Z\index{Z} and what?%
\footnote{The answer here is HOL\index{HOL}, but it comes out rather tortuously.}%

RJ: Well, at the time we were looking at those tools with a view to which tools would be most likely to be helpful in reasoning about specifications that were originally written in Z\index{Z}.

I: Right.
Were you going to try doing the whole thing within Z\index{Z}, or come outside of it?

RJ: Well, we couldn't expect to do that without us having a Z\index{Z} proof tool, so it was a question of what was the best compromise we could achieve at the time, and we didn't have a lot of time to go around surveying all the possible tools.
In any case it's enormously expensive to get to understand a proof tool properly anyway.

I: To understand... ?

RJ: A proof tool.

I: To understand a proof tool.

RJ: Yes.
And if you've got a nice big research grant, then you can afford to do a thorough study, but even in those circumstances people very often do shallow studies and come up with all the right conclusions.
So we did... we made a judgement in a fairly rapid way on the basis of a fairly modest amount of experience, and we went for the Cambridge HOL\index{HOL!Cambridge} system.
The two main reasons for doing that were, first of all, the actual logic supported by it was about the closest you could get to Z\index{Z}.%
\footnote{In a proof tool available at that time.}%
It's a higher-order type theory which is logically not very dissimilar to a typed set theory, which is what Z\index{Z} is.
And also, because it was implemented using the LCF paradigm it was fairly customisable, so you could have extra layers of functionality on the top, and they were [good for] orienting towards doing things for Z\index{Z}.
We spent several years doing work with Cambridge HOL\index{HOL!Cambridge}, a couple of years maybe, before we did... and we operated at that time on the basis of supplemented Cambridge HOL\index{HOL!Cambridge} with a superstructure which helped us to do proofs about specifications we did. And that was a kind of semi-manual, semi-automatic process.

I: You'd have to put some input into the machine occasionally to direct it?

RJ: Well, in terms of the actual transcription of specifications, that was the first hurdle, to get them into a form which the tool would accept.
And that was essentially a manual process, but the superstructure that we had on the HOL\index{HOL} system made it possible for us to write HOL specs in ways which looked closest to the original Z\index{Z}.
... 
And there were certain technical problems that this superstructure solved for us as well.
There's a big cultural difference between what happens in the HOL\index{HOL} community and certainly what was happening then in the Z\index{Z} community, where the Z\index{Z} community made a virtue of pencil and paper, and freedom with the language, and where the style was a rather freewheeling axiomatic style you didn't worry about consistency very much.

I: This was before the advent of W, or whatever logic they designed for it?

RJ: Yes.
It was before there was a published semantics for Z\index{Z}, you know, before there was much by way of definition of the language.
It was sort of taught by example.%
\footnote{
It wasn't quite that bad.
There was some documentation which came out of an Alvey collaboration involving the PRG\index{PRG} and SD\index{SD}.
}

I: No Spivey reference manual.

RJ: No.
But Spivey's reference manual was published in '89.

I: And the semantics about the same time, I guess.

\subsection{Conservative Extension}

RJ: The semantics was published about a year earlier.
That was his PhD thesis, in '88.
So... and the HOL\index{HOL} community was oriented around formal proof and culturally indisposed to non-conservative extensions.
It was sort of frowned-upon to use axioms, and so...

I: Why was that?
Why was it frowned on?

RJ: Well, I guess basically because it was unsafe.
If you just ran the axioms around, then you'll end up reasoning in an incoherent context, and then that makes the whole thing limitless.
But I don't think... I mean, that's the objective reason for it.
I'm sure to some extent it's just a cultural accident, because it's not where they came from, and they got into that cultural attitude.%
\footnote{
I suspect that Russell, {\it Principia Mathematica} and logicism have a lot to do with this.
HOL\index{HOL} is the closest modern descendant of the Theory of Types used by Russell and Whitehead in {\it Principia Mathematica}, and the idea that you adopt a logical foundation system and then work in it using only definitions has a lot of attraction.
So at a time when logicism was supposed by philosophers to have been discredited, its practical consequences were being taken up (by some) at the theoretical end of Computing.}%
Nevertheless, I think the thing that wasn't accidental was that they were into formal proof and rigour as opposed to more freewheeling approaches to formality.%
\footnote{
There is a middle ground in which specifications are formal, but proofs are informal (and selective).
}%
I think one of the contributions we've made is to show how you can integrate these two cultures together and get something good out of the combination.
And I think that... the most tangible evidence of that is the reconciliation of these two different views about whether you use axioms or whether you don't use axioms.

I: How do you reconcile those two seemingly opposite points of view?

RJ: Well, I guess you don't quite reconcile the two, ... what you can reconcile is the rather freewheeling axiomatic style of Z\index{Z} ... with conservative extension.
And to do that, you need to have some slightly different principles of conservative extension, and you have to accept the discipline of conservative extension.
...
The scheme is that when you do axiomatic extensions in Z\index{Z}, you're normally defining constants axiomatically.
...
Historically, there was no constraint on what you wrote there%
\footnote{
Apart from syntactic well-formedness and type-correctness!}%
, and not even much tradition of mentioning that these might be problematic%
\footnote{
The axiom might self-contradictory, or be inconsistent with the rest of the specification.
}.
Now, you can do pretty much exactly the same thing subject to the constraint that the extension you make is conservative.%
\footnote{
Does not permit the proof of any new theorems other than those mentioning constants introduced by the extension, and hence not compromising the logical consistency of the specification.}
And you can do that mechanically with the HOL\index{HOL} system, which we originally did using the choice functions, which, however, doesn't give you quite the right semantics.
That is to say, you can behind the scenes treat an axiomatic definition as saying this new object you're introducing is {\it the}%
\footnote{I should have said ``an'', an indefinite description.}%
 object having a defining property, which means that it would have the property if and only if the property is consistent.
So it's in practice, provided that you're prepared to accept the discipline of conservative extension, or even if you're prepared to accept the slightly weaker discipline that you use conservative extension as a norm, and you very explicitly depart from it where you think that's justified.

I: Can you say that again?
I hope we missed it.

RJ: And you depart from that rather explicitly where you think that's justified, on a rather more limited number of cases than... or you can make it hang together.
I think this not very well-understood, and not very well-publicised.
But it has got embodied both in the culture of proof for Z\index{Z}, where largely through our influence, I would claim - not only in our proof tool for Z\index{Z}, but also the one that CESG\index{CESG} have funded - recognises that consistency proofs are important elements of how you develop specifications.

It's also been reflected in the Cambridge HOL\index{HOL!Cambridge} system, which has had a new form of conservative extension subsequent to our work in this area, which enables you more flexibility in the form of your definitions.

I: This was something that pre-existed your work?

RJ: No.
Historically, the sequence is something like this: that we took the Cambridge HOL\index{HOL!Cambridge} system in, and we started using it for Z\index{Z}.
And we did that originally by mapping axiomatic definitions of Z\index{Z} down to ordinary definitions in HOL\index{HOL} using the choice function, and that's slightly unsatisfactory.
It differs from the originally intended scenario in two ways.
One is that it requires conservative extension, which is... you know, limits you to conservative extension, which was an intended delimitation; and there's a non-intended deviation which arises from using the choice function, which is that any two constants which are introduced using the same property are provable equal.

I: Are they?

RJ: So if you say, ``A is the\footnote{an} object having property P,'' and then you say, ``B is the\footnote{an} object having property P,'' you can prove that A equals B even though there may be more than one object having property P.%
\footnote{The inference would have been OK if it had been ``the'' rather than ``an''}
And so what you wanted was a definitional principle which was looser.

I: So you could distinguish A and B, even if they had the same property?

RJ: Well, not that you could distinguish them, but that you couldn't prove that they were equal.
And so we did a... we did actually do a slight modification to the Cambridge HOL\index{HOL!Cambridge} system for the purposes of one of the contracts that we engaged in, and the changes we did there were first of all to seal off the loopholes which are present in the Cambridge HOL\index{HOL!Cambridge} system, but you can't just do {\tt mk\_thm}, and make a theorem, and to make it so that... and we'd cut out the new axiom thing from it. And we also modified it so that after you'd introduced one of these definitions using the choice function that you could derive the less definite principle from it. You just searched that the object you've introduced has the required properties subject to consistency. Then you delete the original definition, which is the thing that allowed you to prove the equalities you don't want, and that threw away the extra strength.
And at the same time we talked to Michael, and we suggested that they added a new principle.

I: What it might say?

RJ: Well, they did it.
They did something which was equivalent to what it was.
It wasn't exactly the same as we were asked for, but it... that principle is called `{\tt new\_specification}'.

I: It's called?

RJ: `{\tt new\_specification}'.
We only used to have definitional principles, where... I suppose the difference between a definition and a conservative extension is that a definition is usually more definite.
It says, ``This object is this thing,'' rather than ``Anything satisfying this property.''
They're now having a thing called {\tt new\_specification}, which, subject to a proof that a property is consistent, will introduce as a conservative extension, the action which claims that a new constant has that quality.
And that's... that makes quite a bit of difference to how you write specifications in HOL\index{HOL}. I mean, it allows you to... (This is a bit hot!) describe differences to your specifications.

I: This is the work you were doing here at ICL, altering HOL\index{HOL} to your... ?

\subsection{Loopholes}

RJ: Well, we did only minor modifications to HOL\index{HOL} ourselves, and that was just in that specific area. Normally we were just sticking things on top because we didn't think it was... we didn't want to be changing the underlying HOL\index{HOL} system. But this particular contract we were on was very high assurance, so we wanted to have the strongest guidance about the integrity of the proof tool we were using. So we sealed off the loopholes, and... 

I: What loopholes were they? You mentioned loopholes before. What loopholes?

RJ: Well, there were two basic mechanisms, one of which is... just allows you to bypass all the checks, which was a... 
It just allowed you to make a theorem with no justification.

I: With no justification?
You could have... well, it would announce, ``This is a theorem,'' with no justification with behind it?

RJ: Yes.

I: This would seem to be a major flaw to me. What... ?

RJ: Well, I mean, it's only a flaw... it depends on your culture, it depends what you think the proof tool's doing for you.
I think academics operate in an environment in which they're not pathologically suspicious of each other, and they're quite prepared to take some body's word when they come up with a machine checked proof that they haven't taken any short-cuts using {\tt mk\_thm}.
We were operating in a pathologically suspicious environment where you're required to prove that your system does what it's supposed to do and where they pay people to check your claim.
As well as having to check your claim they have evaluators who are there to make sure that you've really done it the way you're supposed to have done it, where you're not doing things on trust.
So the tool's playing an important role there in this environment that it doesn't really play in an academic environment, which is it is sort of putting itself on its heart and saying, ``Yes, that guy really did do his proof properly.''
You don't have to take his word for it, you can take my word for it.

I: And so the constraints you're working on are much greater because of this extra pressure?

RJ: Well, I think the requirement in terms of assurance is...

I: I mean, I'm still worried about this notion of... in the original system you could actually just come up with anything you like as a theorem.
And that I don't understand properly.
How is this possible?

RJ: Well, I'd suggest you... I mean, it's certainly possible.
There's no question about whether it's possible.
The interesting question is why they're happy to leave it like that, and I suggest you ask that to them.

I: I wish I'd spoken to you first!

RJ: You're already been there, have you?
Right.

I: I don't recall this ever being mentioned by them.

RJ: Well, I don't... yes.
They don't think it's a problem, so they wouldn't have mentioned it.
I mean, I don't think it's a problem for the kind of thing they're doing, but I think it is a problem...
It would be a problem for us if we could not claim that it was impossible to prove this result other than by legitimate means.

I: I can see your point.

RJ: 	... they also had {\tt new\_axiom} which is less problematic, because for the simple reason that you have exactly the same power - you get a theorem out of it - but the new axiom is then stored in the theory hierarchy, and it's better, and you can easily discover all the axioms in it, so there's no possibility of fraud with {\it new\_axiom}.
It's recorded as...

I: A new axiom.

RJ: There is a... there's still a risk, and the risk is that the axiom is... that you've introduced an axiom which is false, and therefore you've would be able to prove anything.
But at least there's no...
That's only the same as doing a slightly incomplete proof, where there are certain conjectures you haven't proved.

I: Certain conjectures that... ?

RJ: That you hadn't proved, where you've done, ``I've done the main body of the proof, and there are these other things we haven't proved, but they're uncontroversial, so we're not going to bother.''
I mean, you could be wrong.

I: ... successful, then you...

RJ: Originally in getting... in selling proofs, really.
And I think that was largely because we took the initiative and encouraged the customer.
We didn't need much encouragement, you know, but I think at that time had we not been keen to do it ourselves, then it would probably have been several years longer before they would have	to do.

I: This was due to your own work, your team's work at ICL?

RJ: And we produced... I think the most significant thing that we did in the early stages, the relatively early stages there, was the development of a hardware device which was certified and is still in the book as the most highly-assured secure system that's been developed.

I: And that was using your new theorem prover, or what?

RJ: Well, that was using Cambridge HOL\index{HOL!Cambridge} with one or two small modifications.

I: With the modifications.

RJ: And a certain amount of... and a fair bit of additional superstructure.

I: Right.
With the loopholes closed by that time?
So you closed the loopholes of being able to produce any theorem.

RJ: We closed the loopholes for that project.
We produced a special version for that.

I: And that still remains the most secure?

RJ: That ... is the only one registered at UKL6 ...
 are you familiar with ITSEC?

I: No.

RJ: Well, there are these various regulations about evaluating secure systems which are graded in terms of the level of assurance, and you have to do different things to get the higher ratings.
And 6 is at the top of the scale.

I: Does that mean `absolutely secure', or does it mean... ?

RJ: No, it just means that's the highest level of assurance.

I: So it could still be insecure, no matter how small one chance is, it still could be... ?

RJ: Oh yes.
I mean, insofar as the behaviour of any physical device is concerned, it may break.

I: Right.
So that's the constraint put upon us by the physical world.
It means things can go wrong.

RJ: But even if that [wasn't a] constraint, the models aren't perfectly precise anyway.
Not for hardware.
You can, in principle, get perfect models of bits of software that you do.

I: But hardware's different, because you've got to model the hardware, and that model is going to differ from the reality?

RJ: Yes.

I: Right.
OK, so how far have we got, then?
We've got... we're nearly up-to-date, I guess, aren't we?

RJ: Oh, let me see.
No, not really.

I: Still not?

RJ: I think round about the middle of '88 we were given the opportunity to do some of these part-funded collaborative R\&D things, and it seemed natural to us to do some work on the proof technology.

I: Some proof on... ?

RJ: Some work on the proof technology, because at the time there [was] no sign that anybody else was going to do any worthwhile work on Z\index{Z} proof technology.
And it wasn't at all clear whether it was possible to produce a good proof tool for Z\index{Z}.
At that time there wasn't any definition of [a] logic for Z\index{Z}, and it was clearly a very strange language, relative to almost anything else that anybody had ever built a theorem prover for.

I: In what way was it strange?
Tell me.

RJ: Well, for example, you choose something that's fairly fundamental logically.
It's the only language that I know of where you can't tell what the free and bound variables in an expression are by looking at the expression.
So you can introduce a whole set of variables by putting the name of the schema in there, and you have to go off and look at the definition of the schema to find out what variables you've introduced.

I: Right.
So you'd inspect inside the schema, you mean?

RJ: You'd have to look inside the definition of certain objects to tell what the available schema is ... it's not very usual even for programming, ... I certainly don't know of any other logics, logical languages, in which that's the case.
And it is very fundamental.
Even with much simpler variable-binding behaviour, very eminent logicians have made many mistakes about the rules.

I: Indeed.
It's a very tricky problem, it seems, both within logic and within the mechanisation that comes up all the time, the mistakes you could actually make with free and bound variables.

RJ: So anyway, we had these opportunities, and put together a collaboration between ourselves and Program Validation and the University of Cambridge, which was a fairly loose confederation.
...
Our part in that was the development of this proof tool, which we now call ProofPower, ... which is basically a re-engineered Cambridge LCF-based HOL\index{HOL} proof tool, with some Z\index{Z} built onto that.

I: Is it being used?
I mean, apart from you, I mean, is it being used elsewhere for Z\index{Z}?

RJ: It's being used elsewhere.

(interruptions)

I: I was asking whether Proof Power was being used for Z\index{Z} elsewhere, outside of your own work.

RJ: Yes, well, there are other people who've got it, and we occasionally get feedback on what we're doing with it.

I: Are these commercial concerns, or academic?

RJ: Both.
But I think the main use is still on our applications for our customers.

I: Applications for... ?

RJ: For our customers.

I: Your customers, right.
You use them internally for your customers, and then say, ``Right, we've run this through the Proof Power, and you're sure that this is OK?'' I mean, is that what you'd do?

RJ: Yes.
We'll deliver them the proof, in a machine re-runnable form, ... so they can run it all again if they want to, or do further development.
We spent about three years on that project.
It took eighteen months to get that project in place, and we spent three years on that project, and that finished at the end of 1992.
During 1993 we did... we took the thing through to being on general release as a commercial product.

I: Which one can now buy?

RJ: Yes.

I: And that brings us right up to date, does it?
That's it?


RJ: Yes, more or less. Last year was our first year as a real business, in the sense that  previous to last year we'd been earning external revenues in general, and also working on part-funded projects, but there'd been no requirement that we actually cover our costs or make a profit. It was fairly relaxed. But last year, we were required to start making profits, which we did, and so from there on in, we're pretty much like a business in our right.

I: Within ICL itself, a subdivision of it?
And you're going to be selling more Proof Power?

RJ: Well, I hope so.
The real problem is strategically, to find the ways of sustaining the investments for the funding, on the technology base.
And I think we had drifted away from Cambridge quite a bit as a result partly of the fact that we're using our tool instead of their tool now, and partly the fact that this co-operation has finished, and I think we need to really vitalise our contacts.

I: And presumably, I mean, you must have differed from quite a lot if you'd patched up all these loopholes and they hadn't.

RJ: The loopholes are tightening.
That's not a significant... but we had diverged insofar as we did... we have completely re-engineered the tools.
So it's quite a bit different, though the basic underlying logic is the same.
The basic technology is still the LCF paradigm, so it's...

\subsection{Choice of Technology}

I: Tell me again why you chose LCF from all the possible routes you could have chosen?

RJ: Well, it's powerful, and it's open-ended.

I: It's the flexibility of it to build in your own... ?

RJ: Nobody knows how to build the proof tool they want yet. If you go into any application area, you're going to want to do new kinds of proof automation pertinent to the kind of theories or the kind of techniques you're using. And the LCF paradigm provides you with a safe way of extending the capabilities and a safe and powerful way of extending the capabilities of the proof system. 		. And, you know, I believe that eventually it will be accepted as the only real way of doing it.

(Interruption)

I: So did you seriously consider any other approaches when you were on your way to getting into Proof Power?

RJ: Well, originally, we looked at Boyer-Moore.

I: And why did you reject that?

RJ: Well, the two main factors are, first of all, the logic that Boyer-Moore uses is right at the other end of the spectrum. You know, it's a quantifier-free logic.
So it was much less suitable for reasoning about things that originally came from Z\index{Z}.
In terms of the logic it seemed less plausible that it was going to work out.
And it hasn't got... you know, it's not the LCF paradigm, so it didn't appear to us to be as flexible and extendable.
Those were its two main factors.
Also, we have a cultural aversity to large numbers of brackets.

I: Yes, I can understand that.

RJ: Those were the only ones we looked at.%
\footnote{
By that I should have meant, the only tools we evaluated hands-on.
}%
And I think we did look at NuPrl\index{NuPrl}%
\footnote{
NuPrl (a proof development tool for a constructive type theory) was one of many other tools which we read about, but we didn't actually try it out.
I'm surprised that I mentioned it since I don't think we could have seriously considered using a tool based on constructive type theory for reasoning about Z\index{Z} specifications.
}%
 as well, but I think that's the main... the main differentiation between the HOL\index{HOL} and the New Pearl is that New Pearl is constructive, and we weren't interested in constructive mathematics.

I: Tell me why not.
Tell me why you're not interested in constructive mathematics.

RJ: Well, I'm tempted to ask you, ``Why should I be?''

I: Well, it's all right.

RJ: I think it's a red herring myself. I mean, when doing mathematical models, you know, constructive mathematics is just making life difficult for you. I mean, the touted... the claim to fame of constructive mathematical systems is, you know, ``We must be interested in constructive things, because we're doing computers,'' and therefore you want things that are going to be computable. Getting things that are computable is not a problem. Any program is computable. If you want to verify program, the last thing you have to prove is that it's computable. So that isn't a problem. Getting proofs cheap is a problem, and so anything that makes that more difficult is not a good idea.

I: Yes. Getting proofs cheaply and efficiently and quickly, less machine power and time, et cetera?

RJ: Unless you	. The fact that...

I: Yes, I was going to ask you about that - the amount of human input that has to go into Proof Power - what... how much is guided by the human controller in a proof?

RJ: Well, it varies on the domain, on the kind of thing you're trying to do. It is at the HOL\index{HOL} end of the spectrum where you're expected to have... it's intended for a professional who knows what he's doing, and that you get a higher level and more intimate degree of interaction than you would with something like Boyer-Moore. I'm not saying that Boyer-Moore isn't intended for the professionals, but I think as a research project it was aimed for high levels of automation, and not aimed at, necessarily, achieving the most cost-effective combination of human and machine input. And to some extent, it's just an accident of history how far we got in various different areas in terms of the automation, so there is linear arithmetic on natural numbers in the system, but there isn't linear arithmetic on integers. And it's only a matter of time and money before this pushes through. But there's an awful long way to go before have a really serious mathematical proof tool. Fortunately, most of computer science doesn't need you to do analysis.

I: No, no real numbers.

RJ: But, you know, that's where we have to be going.

I: I mean, there's no chance of any purely automated theorem prover?

RJ: Well, it depends what you'd expect it to be doing. It's all down to what you expect it to be doing. If you expect it to be doing... I mean, human beings don't do maths in a completely automated way, so why should we expect machines to do it? I mean, they can do all sorts of proofs automatically that would be hard for us to do, and harder for us to do for	it, but mathematics is a creative activity, so you can't expect to automate mathematics. And when it comes down to automation, if you ask, ``How do mathematicians learn to do mathematical proofs?'' they learn off other mathematicians. You know, they don't just sit there and... they don't go into a new area and get the axioms of group theory and figure out how to group theory from the axioms, they learn techniques. If you want to make any progress in mathematics, you'd better learn everything that the guys in the field have done before, and all the techniques, and then see if you can make some movement beyond that. And it's bloody tough work, you know? So you can't expect the machines to be more successful than that, except in, you know, sub-domains where it's down to hard work and not creative input.

I: But couldn't we put the techniques into the machine?

RJ: Sure, well, that's what it going to take.
You're talking about... you know, if you'd solved the machine-learning problem, then maybe you could get the machine to learn in the same way the mathematicians have. However, people haven't done that.

I: The AI venture, right.

RJ: Meanwhile, what we're basically doing and haven't yet got very far with is manually coding up the techniques.
And that works.
You know, you can do it.
It's just expensive, and there's an awful lot of them.
And you can see that much more clearly, say, in the symbolic maths tools rather than the proof tools, where they've got much further in coding up symbolic techniques.
And you can get them to do all sorts of things for you.

I: Such as?
Can you give me some examples?

RJ: Well, differentiation, those sorts of things.
I'm not as familiar with those tools as I'd like to be, but they've done an awful lot.
And all that stuff should be within the capabilities of proof tools, and a lot more. But it's just a huge amount of work to do that. It's more work to do it properly with a proof tool. Of course, you get a different kind of capability out at the end because you get something that can actually reason, and doesn't get the thing wrong too often. And it can help you over the problems that a symbolic maths tool wouldn't be able to help you with. And as far as I can see, that's a lot of hard graft. If you approach it in that way, if you don't say, ``We're going to solve the machine-learning problem in order to get these machines to learn how to do proofs,'' you say, ``We're just going to do it the hard graft way. We're going to code up all the techniques,'' then you can get a lot more 	. And then you might, at the end of that process, have a better clue about how to 	the machine	.

I: There was... Lenat actually claimed he'd got a program which could discover not only proofs but mathematical concepts.
Do you know that work,	?

RJ: I think I've seen that.

I: Some years ago.
That was '77, I think, that paper.

RJ: Yes, I'm not saying that you can't do these things on a small scale, but relatively speaking it's plausible that's it going to be more difficult to get a machine automatically to learn difficult things than it is to code up the techniques.
I'm pretty sure that there's nobody out there who says he's got a machine which has learnt how to do linear arithmetic.
And we've got linear arithmetic on the machine because we all went off to a textbook, and we learnt how to do the linear arithmetic ourselves, and then we coded it up.
And that's bound to be a lot easier than trying to get a machine to do that.

\subsection{Choice of Logic}

I: So you're in favour of classical logic, quite obviously, from what you've said.
So you don't see any point in having a theorem prover employing some other... ?

RJ: Well, there may well be a point, but I don't think it's particularly relevant to our field.

I: What do you want?
Do you want to... ?

RJ: And I think the claims that it's important in computer science, I think, are overstated.
And these claims were made by... I guess Martin-L\"off's\index{Martin-L{\ouml}ff} probably the guy.
I thought it was rather entertaining that the Computer Science fraternity should take his word for what they needed.

I: What do you think the explanation of that is?
He seems to be very influential.
I mean, Martin-L\"off seems to have been incredibly influential.

RJ: Well, the whole field is very academically-oriented, and in that context it's very understandable.
His theories were, at the time, very interesting, complicated, and there was a rationale for their relevance to computer science.
And so they're a really beautiful place to do research, you know?
I mean, if you're not interested in the applications but are interested in doing research, then it's a goldmine.
And one has to say also that many of the ideas do feed through and are potentially practically valuable.
But the kind of ideas that are more plausible, from a practical viewpoint, are the different kinds of type structure, not the constructiveness.
So dependent types are important things.
Not that Martin-L\"off invented them%
\footnote{
Similar constructors are used in a type-free context in the earlier work on Combinatory Logic by Haskell Curry and his associates.
}%
, but he certainly popularised [them].
And you can use those in a classical context, though even there it's still not clear whether they're better... whether you're better off having a complicated type system, a complicated undecidable type system of dependent types, or a simple decidable type system.
It's still not clear.
Many people think that these complex type theories are logically more expressive than simple type theories.

I: Logically more expressive.
So you could actually put more into them than you can if you restrict yourself to the decidable set?

RJ: Well, that's an interesting question.
What they mean when they think they're more expressive...
I can't think of any sense in which there is that relationship hard and fast.
Certainly it isn't the case in terms of proof-theoretic strength, which is the logician's notion of expressiveness, and it's not the case either in terms of convenience of notation.
It's not a sustainable case, that that's the only way to get good notation.
By and large, it's still the case that these notations are more difficult to understand than plain set theory, and certainly no more expressive than plain set theory.
\footnote{
Its important in understanding the significance of dependent types to bear in mind that these are used by Martin-L\"off\index{Martin-L{\ouml}ff} as part of a logical system which is based on the ``propositions as types'' paradigm.
In this case the burdon of logical expressiveness in the system falls on the type constructors.
They therefore have to be relatively complex, and the extra expressiveness delivered in this context by dependent types is essential.

By contrast, in a classical type theory such as HOL\index{HOL} the principle role of the type system was historically to constrain the deductive system (particularly in relation to abstraction) so as to avoid the logical paradoxes.
For this purpose the type system can be simple, logical strength arising by other means.
}

I: So there's no advantage to using it, as far as I can see, from your description?
It just complicates matters.

RJ: I don't see any advantages in the kind of stuff that we do.
I don't say that this is not a good area for research that has been done at	.
I think it is an important area, but I think it's still not clear what the practical...

I: Yes. From a practical point of view, you can't see any benefits yet?

RJ: I suppose... there has been feed-through in rather... I mean, for example, the dependent type stuff fed through into programming language stuff about structure, like Pebble, which has got dependent types into it, and that in turn was influential, to a degree, on the design of the module facilities in standard ML, and so one can see via very tortuous routes that these ideas have input in things which are practically useful, and standard ML is certainly practically useful.
We use that on lots of good things, but it's not necessarily in the ways one might imagine.
And certainly in terms of directly using constructive type theories, that doesn't look to us to be practical.
Mind you, we don't have a lot of choice anyway.
I mean, by and large our tools support languages that somebody else invented, not necessarily because they're the best languages, but because they're the languages that people want us to use.
And so as soon as you start getting close to the commercial things, the technical merits of the languages aren't necessarily very significant anyway.
If you get the choice, then that's great.
If you get the choice to choose a nice language rather than a nasty language, then that's great, but generally speaking the market doesn't work in that way.
I think HOL\index{HOL}'s nice.
I mean, HOL\index{HOL}'s really... HOL\index{HOL}'s nice and does seem to have to had a reasonably good level of exposure and application.
And it's nice because it really is simple, or simple in parts.
I mean, it's got a great power-to-weight ratio.

\subsection{Soundness of Logics and Proof Tools}

I: And what about the soundness problem?
Can I move up to soundness for a moment, and ask you: your theorem prover itself - have you got a proof of soundness for it, ProofPower\index{ProofPower}?

RJ: Well, the logic... soundness is a concept about logic rather a tool.
And the logic we use is the same as the one in... that the Cambridge HOL\index{HOL!Cambridge} system uses, which is relatively uncontroversial as an extension of Church's Theory of types.

I: So it's Church's Simple Theory of types?
And you don't think that a proof of soundness is important, because it's already been done in the logic?

RJ: Well, I think there is an informal proof in the Cambridge HOL\index{HOL!Cambridge} [Manual], and I certainly wouldn't put formalising of a proof very high on the priority list.

I: Right.
Why not?

RJ: Well, if you look at the kind of thing we're engaged in, and ask where the risks are, and what are the magnitude of the risks, the soundness of the logic is a tiny bit, a really tiny bit, and the correctness of the proof tool implementing the logic is slightly larger.
It's still actually, for our	, quite a small risk.
And these are many orders of magnitude less than the problems you get when you start using them as tools, like: does my specification say what it ought to say?
Is it a good model of the system we can talk about?

I: So compared to getting the specification matching the intention it's negligible?

RJ: Absolutely.

I: What happens if you're using your... ?

RJ: This doesn't apply to all logical systems.
Some people use more controversial logical systems than this, but this is one of the least controversial logical systems around.

I: Would you be happy to use it in safety-critical areas?
Are you that confident that the risks are so small that you could actually... ?

RJ: In the logic or the tool?

I: In the tool itself?

RJ: Yes, I'm not aware if there are any tools around that have got better credentials in terms of the risk of you proving something that isn't true.

I: What are those risks in general, do you think?
I mean, outside your system, when you say the risk of proving something that isn't true, do you think in general with your systems and tools there is that risk, and not just a minor risk, but... ?

RJ: Well, there is the risk.
It depends upon the system as to how serious a risk it is.
It's certainly usual that tools... it is possible to do it with the tool.
Normally people do screw up in the proof tools, so it is possible to prove false.%
\footnote{
I don't know that I would agree with that today.
Possibly back in the 80's most tools had unsoundnesses at some stage in their development.
Just as at one stage it was more common for published logical systems to be found faulty.
I am possibly now out of the loop, but its very rare that I hear of soundness problems in proof tools.
}
But unless somebody's actually deliberately going to exploit a flaw, it's not so... there's not so high a risk that that will actually accidentally result in your proof tool's truth.
And we have ourselves found problems within the Cambridge HOL\index{HOL!Cambridge} system, several of them.
And they will be fixed.

I: They're fixed when you find them?
You report them back, or you fix them?

RJ: Yes, or in some cases we've found, concurrently with...

I: You've found... ?

RJ: In some cases, more than one party has independently found them.
But all the ones that we've found we've found by thinking about the system.
We've never actually done an application proof and ended up proving something we shouldn't have proved, and discovered that that was because of a flaw in the logic.
I'm not sure I can remember the details, but... there were some problems with the polymorphism, which is the... one of the main features in which Cambridge HOL\index{HOL!Cambridge} and ProofPower\index{ProofPower} are different to Church's Simple Theory of types, and the other main feature is that it's got explicit policed rules for conservative extension, which must have interested Church.
And getting the rules for conservative extension right is, in combination with the polymorphism, doesn't ...
And that wasn't always right, I'll admit, but we've probably got it all right now.

I: So at some stage it was able to come out with non-theorems?

RJ: Well, it's always been able to through the known route-ways, but you could say that they're not problems with the logic or the tool.
People will only use those politically.
And at one time we used to define... we had a method, a method of structuring proofs, which involved giving names to all the lemmas and the main lemma structure, giving names as constants in the object languages.
And that's not actually sound, and we shouldn't have been able to do that.

I: Why isn't it sound?

RJ: It's not sound if the lemma that you're talking about is polymorphic, because a polymorphic lemma may have a different truth value at different type instantiations.
So if you define a constant to be equal to the value of that proposition, the truth value of that proposition, by instantiating that equation at different types, you get a different value for a constant.
And so when we actually figured that out when we changed the system, we had to... we couldn't put our proofs in.

I: They wouldn't go through?

RJ: The proofs using that... the proofs that used that method, in the simple way that we were using them.%
\footnote{
That's a ``yes''.
Once the definitional facilities were fixed to prevent this particular kind of non-conservative ``definition'' proof scripts exploiting that facility would not go through. 
}%

I: So you had to redo them all?

RJ: We didn't redo them, no.
I think that was after the completion of the project ..	time.
We just didn't use that [technique again].
You have to be slightly more subtle about it.
If you want to give object language names to polymorphic lemmas, then you have to give them some parameters, which have got the tool labels. 
\footnote{
I must have been mumbling a bit here, and too many words have been lost for me to know exactly what I was trying to say.
However, though we did not redo the proofs, it would not have been hard to sort them out.
We were not exploiting the logical unsoundness of this kind of polymorphic definition, the technique we were using was convenient for the presentation of partial proofs, and we would have had to modify the technique in very small ways to get the proofs through.
(Instead of defining a boolean constant whose value is the truth value of some unproven lemma, we would have to define a constant boolean valued function with one parameter whose domain type was the cartesian product of the type variables which occur in the lemma.
}

...\footnote{
There seems to be a gap in the transcript here, for the next paragraph does not seem to be on the same topic.}

With the LCF paradigm, you can give a	, which is almost as good, so... I would say the same thing about... I mean, the other area which... where people... where the people often focus when they're worried about integrity of proofs is getting independent checkers of the proofs, as opposed to the tools you use to construct them.


I: Yes, run them through two things.
You've got the constructor, and then run the proofs you've got through a proof checker, and that gives you more confidence.

RJ: But I think that's... I mean, in my view, that's a bit academic.

I: Why?

RJ: Well, for starters you've got that kind of structure built into an LCF system anyway.
You've got a logical kernel which is checking all the proofs are constructed.
And so as far as LCF's concerned, you're getting much more by doing that.

I: It's already being done, you mean?

RJ: Well, it all depends on what exactly the semantics are independent of, you know?
But I would... you know, the view I take is that an LCF system is designed to do that on the fly, and if you say,
    ``OK, well, maybe you're... It's still better to export a proof tool verifying a checker.''
Well, maybe it is, but you're really talking, you know,	here.
This is not the area at which there is any risk, but, you know, the significant risks are completely elsewhere.
That's not to say that it might still be a worthwhile bit of research for somebody to do, but in terms of the practicalities of what's significant commercially.
I don't know what...
00-55 calls for this, calls for independent checkers.
And I don't know how successful you'd be in arguing that an LCF system would have that.

I: But you haven't got a proof of soundness for your system, for your prover, have you?
I mean, that's definitely the case?

RJ: A proof of correctness for the prover?

I: Yes.

RJ: No, we haven't got a proof.
We have got... we have got some relevant proofs for it.
We've got formal specifications of the logic, and we have got... and we have done some proof work in relation to the more innovative aspects of the controls in the logical kernel.

I: These are proof work towards correctness, but you haven't got the absolute correctness?

RJ: No.

I: How difficult is your prover to work, to operate?
Would I need a... I mean, would one need a fair amount of training?

RJ: Well, for simple things it's easy.
For simple things it just does its easy.
There is a lot of it, and so you would acquire high levels of proficiency only over a long period of time.
We teach it in quite short courses, and we find that people get on fine with this, so we have a sponsored student, for example, who's an undergraduate reading maths at Cambridge, and last Easter he did a one-day ProofPower\index{ProofPower} course and then went on to develop various theories for it, and probably 	.
Of course, it's easier when you're living in amongst a group of people who understand it.
It's easier to...

I: In that environment.

RJ: ... employ the tools.
But there's lots... there's lots of training material comes with it.

I: Presumably a proof... a prover itself will depend upon the skill of the person operating it for its proof finding.

RJ: Well, it depends at what level you're talking about.
You know, I mean, in... if it's a conjecture... I mean, the basic approach is that insofar as we can afford to do so we implement automatic proof facilities for special domains, and the guy using the tool is reducing his problem to those domains and then hitting it with the automatic proof.

I: So most operators wouldn't need to know everything that you know in order to operate in their particular domain?

RJ: Well, they certainly don't need to know everything that we do.
They don't even have to know how to implement it.
There's a lot of facilities there...

I: Do you think that... ?

RJ: ... because it may be being used in developing better facilities.

\subsection{Surveyability of Proofs}

I: What about the proofs themselves?
Do you think it's important that the proofs produced by the machine should be surveyable by human beings?

RJ: Personally, I don't, no.

I: You don't know?

RJ: I don't.
Personally, I do not.

I: So you don't believe that?
Right.
Why?
A lot of people do things you could prove to be subject to survey.

RJ: Well, they are subject to survey, ... 

[pause]

RJ: It's going to depend on why - what your purposes are, as to whether the survey's important.
Some academics find the proofs interesting, and they want to be able to look at the proofs because they're interested in the proofs.
And that's not relevant to a commercial application.
At the high levels of the proof structure, then you may learn interesting things about the system by looking at the structure of the proof.
And that's at a very high level relative to the nuts and bolts of the proof.
In terms of wanting to reassure yourself that the proof is correct, I think that's almost... I would say that it's a complete waste of time.

I: OK, what about 'surveyable by machine'?

RJ: What's important... what is important to surveying is 'what is it that's been proven?' What is this proposition that this thing has proven? And your scope for error is entirely... if you've got a good proof tool, if it says it's proven this conjecture, it's proven that conjecture. Your only problem is, what does it mean? Does it mean what you think it means? And that certainly does need to match your specification, and the nature of the proposition that you're claiming to have completely proven, what it means is something which has to be surveyed and understood by people. And that also applies to any residual lemmas which you've not proven. If you've got various assumptions you haven't completely discharged, then obviously the content of those assumptions is very important, because if they're actually incoherent then the whole thing's meaningless.

I: This has actually happened?

RJ: Oh yes.
I mean, all it comes down to is you're trying to prove something about a system.
If that thing that you're trying to prove is false, then it's still going to be possible to prove it, if you don't carry it right through to the end, because you just show that the truth of it follows from some other false claims about other parts... that part of the system.
So long as your proof isn't entirely complete, then it could be entirely wrong.

(Pause)


I: Surveyability by a machine.
Do you think a proof should be surveyable by a machine?
I mean, if you don't think it should be necessarily surveyable...

RJ: What do you actually mean, 'surveyable'?
I mean, as far as LCF-type systems are concerned, the proof will have been checked by the machine as it was constructed, but then it's thrown away.

I: Yes, but you might get to a point where a theorem prover is running through steps which you couldn't get a printout from, or anything of this kind.

RJ: Well, that's the norm.

I: Yes.
So these are not surveyable?

RJ: Well, they're surveyed 'on the fly', as it were.

I: But I'm thinking about a case where you couldn't possibly get a printout, it's too much.
What about such proofs being produced?
``Yes, this is a theorem,'' and you've got no... nothing to survey at all.

RJ: One norm... well, the norm is, with the systems we operate, is if you survey anything you survey the script that produced the theorem, and parts of the log resulting from it.
You never survey the detailed structure of the proof itself, and I don't think it's a problem.
So I think it's not a risk they would... if you want to be sure that the tool is capable of checking proofs properly, then survey the tool, verify the tool, if you want... if you're that bothered.
But this is... you don't expect engineers to hand-check the calculations that the computer does for them.
Computers are good at doing these things, and the idea that a human being is going to provide a better check is a joke.

I: That's a joke?
Because the length of these things would be too long, and the number of characters per line would have been, well, sometimes thousands,  four thousand, right, on some of these theorem provers.
So that would be a red herring altogether, the idea of having a...

RJ: Well, it might.
Though... it is fairly popular to attach more significance to this than I do.

I: Why do you think that is?

RJ: Well, I can understand academics attaching more significance to it, because they don't always have the same interests and priorities, and some of them genuinely are interested in proofs, in the structure of proofs.

I: But from the point of view of soundness or correctness of the proof there's no advantage, you think?

RJ: I think that... I don't often hear... well, I don't think I've ever seen anybody make any comparative assessment of the significance, in terms of risk, of these different things.
I think what people say about these things doesn't take that into account if you're talking about risk.
But even people who advocate inspectability of proofs are normally doing so not from the point of view of that contributing to your confidence in the truth of the [theorem], it's generally more in terms of it contributing to your understanding of the problem domain, and that being a major element of what you get out of doing the proofs.
And that's a different thing.
But you don't... to get that kind of understanding you only need to look at the gross lemma structure.%
\footnote{
Its worth noting here that ``gross lemma structure'' means rather different things depending on whether you are talking about formal or informal proof.
A lemma in a formal proof may correspond more or less to a single step in a journal level proof, so I am here saying that there is not for most purposes much value in looking at a formal proof at level of detail any smaller than one would typically see in a proof in a mathematical journal.
}%
You don't need to look in too much detail.

I: And that lemma structure would probably convince you of the correctness of the proof by looking at the lemma structure in detail?

RJ: Well, the thing that convinces you that the truth has		...  you're still not going to be able to check on the detail, and there's always going to be a risk, as far as your eyeballing is concerned, that you've not understood it properly and it's not true.  getting the machine to check it really does add an assurance, there's no doubt about that, a lot more than you could ever get by looking at it yourself.

I: What about decision procedures, if you've employed those in your prover in some way, or at least your checker?
Should those decision procedures themselves have a correctness proof?

RJ: Well, the decision... the kind of thing that we would point to if you were talking about decision procedures are double-checked anyway by the logical kernel, so as far as our systems are concerned, it's only the logical kernel that you have to worry about.
For example, the thing which automatically proves things in linear arithmetic doesn't need to be checked, because if it gets it wrong it's only generating a proof that's going to check that logical kernel.
Now, there is a lot of interest in sidestepping that obligation and using decision procedures which don't bother to use any of the proof, and then many proof tools do that.
And you have got a problem there.
You've got more code that you have to worry about whether it's correct or not, and ultimately one may be able to do these things using principles of reflexivity which involve you prove correct something first, although that's a lot of hard work.
And in terms of the LCF paradigm, that doesn't sound to me like a very cost-effective place to put your effort.
There's lots of places where you could invest in improving the automation without having to sidestep the 	rules.%
\footnote{
Without having to bypass the logical kernal.}

I: Again, you're more or less saying that the emphasis on correctness is not really worth it compared to the efficiency?

RJ: No, I'm not saying the emphasis on correctness isn't worth it, I'm saying here you're talking an emphasis on efficiency, on real efficiency: that is to say, you want to avoid generating the proof, because it's going to take you too long, and that creates a worry about the correctness.
Now, in the general context of the capabilities of the proof tools, and the productivity of the process of producing proofs, it seems to me that there are plenty of places where you can make major steps forward without having to do that kind of thing, and therefore without entailing any risk, any additional risk.
And, you know, we haven't got anywhere near the end of that line, and all the time we're working on it, the machines get bigger and faster.
So the mill efficiency is not the dominant consideration, the mill efficiency is the side issue. 
The real issue, in terms of productivity, is 'how much automation is there in the tool, and what's the relationship between the tool and the man?'.
It's not whether you can get... it's not whether you can get... how fast you can get tautologies from it, though this is sensitive to the application.
So if you're talking about hardware verification, and you're trying to get very high levels of automation in hardware verification, then the picture changes a bit there.
And things like resolution proof - really sort of heavyweight proof methods - tend to end up being exponential anyway.
So if you do something to cut down to no cost there, it's not going to get you much further.
It's not going to solve much more difficult problems for you.
And the other thing is, in the LCF paradigm if you're talking about something like, say, a resolution proof, it's dominated by search.
It's dominated by the time taken to search for the proof, not by the time taken to check it.
So generating the proof is still not the obvious place to be... of course, if you get the generation of the proof too inefficient, then it is a problem.
But you can focus on... if you really think resolution proof is what's going to improve your productivity - and that's an interesting question whether... how much difference it makes, or any other kind of special area - then you can code up the proof search really efficiently without incurring any risk, provided you're prepared to just feed the proof right through the kernel when you'd found it.

I: What about hardware itself?
We touched on the survey - what about the hardware?
Do you think that has to have a verified design, a theorem prover, the hardware on which the theorem prover is running?

RJ: Yes.
Well, if you're talking about probabilities here, you know, if you ask the question, ``Somebody's sat down at a machine and developed a proof for a proposition - what's the chances of that proposition having been proved... being a false proposition which was proven only as a result of a hardware failure?''
They're negligible.

I: Negligible?

RJ: Totally negligible.
I mean...

I: Why is that?

RJ: These computers that we use very rarely have hardware failures at all.
Most of the hardware failures is things like disks.
Most of the hardware failures are immediately detected.
And the chance of a hardware failure resulting in a proof going through which shouldn't otherwise have gone through is...  I don't know what it is, but it's certainly not significant.%
\footnote{
Of course, what's significant depends upon the application, upon just how important it is to get things right.
}

I: No examples of this, obviously, from what you say.

RJ: Well, I've certainly never known that to happen.
So now... that's not to say the hardware verification is a waste of time, but if that's your reason for doing it, then it's a pretty poor reason, because if you think you're going to prove something that isn't proven\footnote{true} because the hardware's not working...

\subsection{The Future of Theorem Proving}

I: What do you think of the future of theorem proving, the automated theorem prover?
What do you think the future holds?
Can you speculate or predict?

RJ: Well, my speculation is that eventually...

I: So eventually, then, the whole...

RJ: Are you talking about commercial?
Are we interested in the commercial future, or just generally?

I: No, just in general, yes, your views about what direction automated theorem proving's likely to go in.

RJ: Well, I find it inconceivable that mathematicians won't eventually normally do mathematics using proof tools.

I: You really think that's going to be the case?

RJ: But I don't think that the proof tools that we have today are going to do that job, and it could be quite a long time.
But why on earth out of all the scientists in the universe mathematicians should be the ones that don't use computers, that's pretty odd.
And mathematical modelling is what science is all about, science and engineering is all about.
That's not necessarily the kind of mathematical logic that people in formal methods are doing, but that's what it's all about.
And you can do more with proof than you can do with... you know, you can do anything with proof technology, in terms of mathematics and logic, anything that can be done.
The kind of tools that are out there at the moment have strictly lesser capabilities.
Eventually proof tools will be able to do all these things, and that ...

I: But that's the direction it will go.
Will they be completely automated, or still need the human input?

RJ: It's never%
\footnote{Maybe not that long, not in my lifetime.
}%
 going to be completely automated, because it's a creative activity.
So you're always... there's always going to be a human being there constructing the model, and deciding what he wants to know about the model, and there's always going to be conjectures that the man and the machine together can prove that neither can by themselves, that are really tough conjectures.
You know,  it's not... it wouldn't be difficult to concoct a program and a specification of it such that the program was correct if and only if Fermat's last conjecture was true, and, you know, we may or may not have a proof of Fermat's last conjecture now, but it certainly took a hell of a long time to write.

I: We haven't got it, in fact.

RJ: It's been... ?

I: The gaps...

RJ: That's the latest view, is it?

I: Well, we think the gap is that it's a substantial gap.
It really is not the trivial gap that was first thought.

RJ: So if you write a program which depends upon the proof of Fermat's Last Theorem for its correctness, then it's not going to prove it automatically.

I: No.

RJ: It's probably not going to prove it at all.

I: OK, is there anything else you wanted to add, any views you want to express before we come to the end? Anything you think I've left out that is pertinent?

RJ: Well, I'm very interested in what it takes to make the kind of cultural change that... 
I think the only thing to say about the use of proof tools by mathematicians is that I don't believe it's going to be a case of mathematicians are going to do what they do now in future using proof tools.
I think they're going to do something different using proof tools.
You know, the proof tool makes so much of a difference, especially proof tools where they're going to be good enough for mathematicians, which we don't have any yet, or at least proof tools with sufficient maths built into them	that they're going to change the mathematics that mathematicians do. To some extent.
There's pressure, anyway, from the fact that mathematicians are... seem to be increasingly finding that they're trying to prove things which are seriously difficult and complicated to prove, and they're losing the ability to do them without computers anyway.
But it's going to change the nature of, particularly, mathematics.

I: I can see that it will do in certain areas, like, as we saw in combinatorics, or something of that area, that something like Fermat's Theorem, which depends upon some theory of elliptic functions, or whatever - do you really foresee a time in which a computer will be used there?

RJ: Well, it all comes down to whether it's easy to do, if at all.
At the moment it doesn't come down to that, even if they were good enough, because all the professional mathematicians around have grown up in a culture of doing proofs the way the way they do proofs, you know?
But at some time in the future mathematicians are going to grow up and go through university using proof tools, and they're going to have an entirely different...

I: Just as they now use calculators at school, you mean?
The same sort of thing? 
It would be, ``OK, here's your proof tool,'' in your undergraduate class, and you can use it in your exam, or whatever?

RJ: And in my view, if you have an interest in getting mathematicians to use proof tools, the best place to get that ball rolling is not to go to people who are doing research in mathematics, it's to get undergraduates to use proof tools, because the guys who are already doing proofs are going to carry on doing proofs the way they have done already.
They've learned how to do that.
It's the guys who've learned to do it different that are going to do it different.
Now...

I: New paradigm.

RJ: I'm not very familiar with exactly what happened, but I had this woolly perception, about fifteen years ago when... shortly after Lightfoot%
\footnote{
James Lighthill (1973): ``Artificial Intelligence: A General Survey'' in Artificial Intelligence: a paper symposium, Science Research Council }%
 put the clappers on AI at Edinburgh, that there was a sudden discovery that AI was the way to do psychology.
Now, I don't know whether this was because everybody left computer science departments and suddenly found themselves in psychology departments...%
\footnote{
Apparently, the theoretical group in the Edinburgh ``School of Artificial Intelligence'' was simply renamed ``theoretical psychology'' at about this time.}

I: A diaspora, yes.

RJ: ... but that surely must have been a major shift in the way people did psychology, and it seems to me that that's more the kind of model of how proof tools would eventually affect mathematics than the kind of idea that they're going to carry on what they used to do, and eventually they're going to start using proof tools for it.
The mathematicians... I would expect, not being a mathematician, I would expect that the kind of mathematics that gets done at the moment is, to a significant extent, conditioned by what is possible for a mathematician to do with a pencil and paper, to do with the kind of complexities he can cope with inside his head without any tools to help him.
And that determines what they think is interesting problems, and what's sufficiently abstract and interesting, and the dynamics of that eventually are going to change when the tools are in place.

I: How's it going to happen?
How is the change... how are the proof tools going to be introduced, say, to undergraduates, if their mentors are not using the proof tools?
How's it going to happen.

RJ: I think the confusion between computer science and mathematics is helpful, because discrete mathematics is taught with computer scientists in mind nowadays, but it's kind of a bit of elementary mathematics that everybody does. 
And that tends to be computer science-oriented now.
And I think we will certainly see tools, proof tools, moving into that patch.
I think moving much beyond that will be dependent hem being able to cope with more mathematics than that.
At the moment they're only good for discrete mathematics, really.
But we're also seeing, at the moment, that symbolic maths tools are getting more significant.
People are beginning to appreciate the merits of these, and...

I: Well, they are being used to solve differential equations.
Mathematica is one such tool which some mathematicians are certainly using to ease solution of differential equations.

RJ: They're being used as educational tools as well, and eventually proof tools will be able to do that kind of thing as well, and much more as well.
And it's a bit of hump to get over, you know, to get to that point.
It's not at all clear how you're going to get to that point, because there's a huge amount of work involved.
But it will happen, eventually.
I just hope I'm still in the game!

\chapter{Research Topics}\label{ResearchTopics}

Here are specifically the research topics I considered for a PhD dissertation.

\section{Keele}\label{KeeleProposal}

\subsection{Background}

The following is a transcription of a note that I made, probably in early 1976, while an undergraduate at Keele University looking for a University to do research towards a PhD.

The professor of mathematics at that time gave me the advice (which now seems to me rather bad advice) that I should avoid doing an MSc and get straight into working on a PhD.
So I wrote down the kinds of things I wanted to do research in and sent the note to some Universities to ask them whether I might be able to do that kind of research with them.
I think it just went to Oxford and Manchester at first, then Peter Aczel at Manchester said that the Computer Science department at Warwick would be a good place to do that kind of work and suggested I talk to David Park.
The copy to Oxford was addressed to Dana Scott, with a covering letter explaining that I didn't want to do the MSc but was looking to get straight into research.
He replied very concisely to the effect that he was not able to help me.
Later I got a more conciliatory response from Robin Gandy, who invited me to go and talk to him, but by that time I had already agreed to go to Warwick.
For more than one reason this was unfortunate, and I would have been much better doing the MSc at Oxford.

\subsection[Possible Topics for Research]{Possible Topics for Research in Mathematical Logic, Foundations of Mathematics, and Computer Science}

\subsubsection{High Level Logics}

My most consuming ambition in Mathematical Logic concerns the development of ``High Level'' formal systems of logic. By ``High Level'' I do not mean ``Higher Order'', I mean something analogous to the use in ``Higher Level Programming Language''.

The purpose of a high level programming language might be:

\begin{description}
\item[(A)]	To facilitate the precise formulation of algorithms (possibly in some special problem area, but not necessarily)
\item[(B)]	To enable people to make use of the facilities of a complex computer system without needing first to acquire detailed knowledge of the workings of the machine and its software.
\end{description}

The "sine-qua-non" of a programming language is translatability.

By analogy the purpose of a high level formal system of logic might be:
\begin{description}
\item[(A)]	To facilitate production of completely formal proofs of mathematical theorems (possibly in some particular branch of mathematics but not necessarily)
\item[(B)]	To enable mathematicians to produce completely formal mathematical proofs without needing to master all the intricacies of mathematical logic and set theory.
\end{description}

The ``sine-qua-non'' of a formal system of logic is that there be an effective method for verifying proofs (not to mention that it should be consistent!)

The development of such languages is a prerequisite of any extensive practical use of computers for proof finding.

\subsubsection{The Foundations of Mathematics}

Concurrently with work on the above some work on the foundations of mathematics would be necessary.
I could of course stick to formalising versions of the tried and tested axiomatisations of set theory, but I have traces of intuitionism lurking within me and would like to investigate other possibilities.

One possibility which interests me is that of basing mathematics on strings rather than sets. Strings however, unless infinite strings were admitted would serve only for a version of constructive analysis.

\subsubsection{Computing with Reals}

The principle source of my intuitionist leanings was the perception several years ago of the disparity between theory as embodied by real analysis and practice found in numerical analysis.
When I was familiar with the way mathematicians use numbers on computers the beautiful completeness and simplicity of the real number system seemed a rather shallow pretence.
In practice all mathematical computations were approximated using using a subset of the rational numbers.
What seemed worse than the understandable failure of numerical analysts to use real numbers was their failure to exploit the full range of computable numbers.
It seemed to me that it was just because mathematicians allowed themselves the pretence of real numbers in their theory that they failed in practice to fully exploit the computational capabilities of their machines.
It is well known that a wide range of functions are computable, it would be a good thing if computers were could be organised so that any ``computable'' function can in practice and without much difficulty be computed within any specified degree of accuracy (space and time permitting).
This is not presently the case.
A typical example of the deficiencies of the present techniques is a problem as simple as the inversion of a matrix.
Any non-singular matrix with rational entries has a computable inverse and so one could reasonably expect that any substantial computing complex would have facilities to compute any such matrix to any prescribed accuracy.
In fact, I doubt that there is any complex which will guarantee to compute any inverse to within 1\% accuracy (subject to time and space limitations only).
In practice the accuracy of the inversion will depend upon the size of the matrix, and more importantly, on how close to being singular it is.
In general we can only guess at how accurate the inversion is.

It would be interesting and valuable (and not trivial) to investigate the software and hardware necessary to render practicable the computation of any ``computable'' function to an arbitrary number of places. On the more purely mathematical side there is the problem of sorting out which of the problems of analysis known to have solutions actually have computable solutions, and the problem of arriving at practical algorithms for obtaining those that are computable. It must be emphasised that the algorithms here referred to are quite different in character to the algorithms used in numerical analysis at present, which are generally algorithms for approximating solutions. There is no way of obtaining solutions of arbitrary and known precision without abandoning present methods of representing numbers in machine memories.

%\addcontentsline{toc}{chapter}{Glossary}
\chapter*{Glossary}\label{glossary}
\addcontentsline{toc}{chapter}{Glossary}

\begin{description}
\item[CESG]{\index{CESG}} Communications and Electronic Security Group (a part of GCHQ).
\item[class]{\index{class}} A large collection.
\item[GCHQ]{\index{GCHQ}} Government Communications Headquarters, UK.
\item[HOL]{\index{HOL}} Higher Order Logic, a specification language, a formal deductive logic, an interactive proof tool.
\item[NuPrl]{\index{NuPrl}} A tool supporting proof development in a ``New Proof/Program Refinement Logic'', which is a constructive type theory.
\item[PRG]{\index{PRG}} The Programming Research Group, at the University of Oxford.
\item[set]{\index{set}} A small collection.
\end{description}

\backmatter

\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{alpha}
\bibliography{rbj}

\addcontentsline{toc}{chapter}{Index}\label{index}
\twocolumn[]
{\small\printindex}

\end{document}
