\chapter{The Short Story}\label{ShortStory}

\section{The Point of This Chapter}

In the aftermath of the release of generative AI as \emph{chatGPT} one direction of further development which has been widely advocated and rapidly implemented is that of giving Large Language Models (LLMs) direct access to tools, which, mediated by users they have demonstrated they can do effectively.

Another direction in which enhancement has been advocated is the ability of LLMs to reason.
My aim here is to sketch a tool for use by LLMs and other AI as a way of guaranteeing that the kinds of deductive reasoning which they might effectively deploy can be guaranteed sound.
The importance of this cannot be underestimated, due to the difficulty in containing infidelity.
Once a falsehood has been proven a contradiction is not far away, and armed with a contradiction all proofs whether of truths or of falsehoods become very short and completely worthless.

My description of the proposed logical tool will come in several stages, growing more complex as additional desiderata are introduced and address.

In the first instance the tool could simply be the logical kernel of en existing Interactive Theorem Prover, which for reasons which I hope to make clear should I suggest support formal deduction in the variant of the HOL logic devised at the University of Cambridge, originally for reasoning about digital hardware.

\ignore{

Writing in the aftermath of chatGPT going viral, and in the context of widespread experience of the strengths and weaknesses of Large Language Models, a few points about the LLM approach to AGI now seem apparent which might not have seemed so obvious before.

There are a number of elementary capabilities that we might not have expected LLMs to exhibit, but that we find they do surprisingly well, but nowhere near well enough.
Arithmetic for example.
It's encouraging to see am apparent consensus on how to deal with that (and other similar) weaknesses - give the LLM access to an old fashioned non-intelligent program that has the required capabilities.

It's my aim here to consider one very large group of missing capabilities, which do not require intelligence, for which I suspect we don't have entirely suitable tools available, and to sketch the features of the missing tools.

The capability I have in mind is some vital infrastructure for deductive reasoning.
Its not true that this does not required intelligence.
Some of the hardest intellectual problems which have ultimately been solved by human beings have been the construction of formal deductive proofs of tricky problems, such as the ``Four Colour Theorem'' which remained unproven for 120 years after first conjectured.

The first proof of the four colour theorem was controversial because it relied on extensive computation and could not be fully checked by a human.
Eventually the proof was widely accepted, and later completely ``formal'' proofs have been undertaken and checked by machine.

Once we get beyond the simplest examples, deductive reasoning can get very complex, the construction of proofs demands intelligence and the checking of them ceases to be straightforward unless the proofs are completely detailed formal proofs, in which case the complexity may be increased by an order of magnitude.

Nothwithstanding the greater complexity of detailed formal proofs, their checking is routine for digital computers and can therefore be conducted with extremely high levels of reliability.
Where conjectures are susceptible of deductive proof, it is therefore to be expected that their truth can be determined conclusively, which is rarely the case otherwise.
It is therefore desirable, for the sake of making GAI more trustworthy than mere humans rather than the ignominious status of LLMs as wholly untrustworthy, that we undertake the steps necessary to ensure that deductive reasoning is properly checked, and that it is used in the widest possible range of circumstances to check the conclusions of AI systems.

It is to address this here in as concise a way as possible.
First with an indication of scope, and second with an account of the conditions necessary to accomplish reliable checking of deductive reasoning.

As to scope, it is very broad and may be characterised as extending to the whole domain of analytic judgements.
To call a judgement \emp{analytic} is to say that it is tautologous (in an extended sense which will be described in greater detail below).

Godel's incompleteness theorem does show us that there are some gaps, but long established deductive systems such as the usual basis for set theory (ZFC) are strong enough that incompleteness though significant for the furthest reaches of mathematics is not known to have any consequences for science and engineering.

\section{Preconditions for Reliable Deduction}

\begin{itemize}
\item A formal language with well-defined and decidable rules of formation and truth conditional semantics.
\item A deductive system which has analytic axioms and  effectively decidable deductively sound rules of inference.
\end{itemize}

There are foundational problems in defining such a system, problems of regress in defining the syntax and semantics and in proving the soundness of the system.


Things to cover''

\begin{itemize}
\item
\end{itemize}

}%ignore
