% $Id: p038.tex $
% bibref{rbjp038} pdfname{p038}
\documentclass[10pt,titlepage]{book}
\usepackage{makeidx}
\newcommand{\ignore}[1]{}
\usepackage{graphicx}
\usepackage[unicode]{hyperref}
\pagestyle{plain}
\usepackage[paperwidth=5.25in,paperheight=8in,hmargin={0.75in,0.5in},vmargin={0.5in,0.5in},includehead,includefoot]{geometry}
\hypersetup{pdfauthor={Roger Bishop Jones}}
\hypersetup{pdftitle={Oracular AI v2}}
\hypersetup{colorlinks=true, urlcolor=red, citecolor=blue, filecolor=blue, linkcolor=blue}
%\usepackage{html}
\usepackage{paralist}
\usepackage{relsize}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{longtable}
\usepackage{url}
\newcommand{\hreg}[2]{\href{#1}{#2}\footnote{\url{#1}}}
\makeindex

\title{\bf\LARGE Synthetic  Epistemology\\ and\\ Artificial Intelligence}
\author{Roger~Bishop~Jones}
\date{\small 2024-06-02}


\begin{document}
%\frontmatter

%\begin{abstract}
% An epistemological synthesis with a positivist flavour, oriented toward the opportunities offered by Artificial Intelligence.
%
%\end{abstract}
                               
\begin{titlepage}
\maketitle

%\vfill

%\begin{centering}

%{\footnotesize
%copyright\ Roger~Bishop~Jones;
%}%footnotesize

%\end{centering}

\end{titlepage}

\ \

\ignore{
\begin{centering}
{}
\end{centering}
}%ignore

\setcounter{tocdepth}{2}
{\parskip-0pt\tableofcontents}

%\listoffigures

%\mainmatter

\chapter{Introduction}

Deduction is fundamental to modern civilisation.
Mathematics, science, technology and the prosperity of \emph{homo sapiens} depend upon it.

The ability to perform elementary deductions comes with an understanding of propositional language, likely co-eval with homo sapiens.
The terminological taxonomies typical of declarative languages include concepts semantically related by inclusion.
Anything we know of animals in general will also be true of mammals, and a competent user of such a language will readily infer from general facts about animals to similar attributions to mammals.
Deduction is at bottom reasoning rooted in the semantics of language, even though \emph{formal} deduction may rest on syntactic criteria known to be consistent with that semantics.

This does leave some room for doubt about when conceptual reasoning can properly be considered deductive, since we may doubt even today which inclusions are consequences of meaning rather than accident.
Elementary propositional reasoning is less susceptible to such scepticism, for the inference from `A and B' to `B' and thence to `B or C' is surely beyond reproach to anyone who understands the words `and' and `or'.
The elimination of doubt flows from the clarity of semantics, we know with greater certainty and clarity the meaning of those sentential connectives.

Declarative language is also implicated in the transformation of culture from a largely vertical inheritance of children from their parents into a richer and more broadly communicated oral tradition.
The passage of advantageous (or essential) skills and customs from parent to child by observation and imitation becomes a richer evolving culture, conveyed at first orally among larger groups, but ultimately through a progression of more persistent and pervasive media was to transform the fortunes of humanity and facilitate his migration across the globe and into global dominance.

Though written language and its notational elaborations for arithmetic were important milestones in evolution of culture, it is not until we come to Ionia in the 6th Century BC that deduction comes into its own.
The ancient Greek philosophers turned mathematics into a theoretical discipline involving long chains of deductive reasoning, and issuing over the course of about three centuries in Euclid's Elements \cite{euclidEL1}, a text retaining its authority for the next two and a half millennia. 


This monograph presents certain ideas about the representation of knowledge, which may be thought of either as the earliest and most abstract stages in the design of a \emph{knowledge base}, and hence a contribution to the design of software for any purpose which exploits knowledge of the world (i.e. almost any software?), or as an epistemological synthesis playing into the accelerating evolution of the theory of knowledge.

At the core of this proposal is the concept of a \emph{Quasi Universal Logical Foundation system} a neologism which I here introduce and for which I will also use the acronm \emph{QULF}.
Though any QULF is suitable for the purposes outlined, I refer specifically to one closely related to and derived from the logical system used by a small family of proof assistants of which the first was the HOL system created by Mike Gordon and his team at the University of Cambridge in the mid 1980s, with a pedigree descending from Frege's \emph{Begriffsschrift}\cite{frege79} through Russell's \emph{Theory of Types}\cite{russell1908} and Church's \emph{Simple Theory of Types}\cite{churchSTT}.

The universality suggested by this neologism is primarily semantic, and reflects the reducibility of a large class of well defined languages to the QULF.
That this is called a ``quasi'' universality results from there being no single semantic having the relevant universality, but rather a sequence of semantics which cover the ground.
The semantics for these languages determine the set of interpretations of the language which are considered models, and the ordering in the family corresponds to the relation of inclusion between the relevant classes of models with the larger collections appearing earlier in the ordering.
The later models differ from earlier ones by the elimination of all models below a certain cardinality.

The simplest QULF to describe is that in which the syntax of the language is that of first order set theory, the models are the ``standard'' models of that theory (i.e. those in which the power sets are full).


\chapter{Intellectual Autobiography}

I have considered the ideas presented here over at least three decades during which the core ideas have remained stable while I explored many different approaches to their presentation and gathered a broader appreciation of their origins and ramifications.

My own history with the ideas goes back beyond the thirty years since that moment which I think of as key to the synthesis which I attempt here.
This arises from an interest in two areas which dates back to not long after leving school (and probably have been earlier had these ideas been current in schools in the 1960s).
The first one was that of \emph{artificial intelligence}, and the second formal logic and its role in the foundations of mathematics.

\appendix
\chapter{Introduction 2024-06-23}

\emph{Epistemology} is the philosophical study of knowledge.
\emph{Artificial intelligence} is an aspiration of Computer Science concerned with the design of intelligent cognitive artefacts.
Machines which \emph{know} and \emph{understand}.

The things known by such an artefact may be spoken of as a its \emph{knowledge base}.
To \emph{design} a knowledge base it may be useful to understand what knowledge \emph{is}, and insofar as philosophy can offer a tangible understanding of knowledge, it may thereby contribute to the design of intelligent artefacts.

There is little sign today of such a relationship in practice between epistemology and artificial intelligence.




More traditional software engineering methods would demand that the construction of intelligence software would depend upon the design of a suitable knowledge base to act as the repository for that which the artificial intelligence knows.
Dominant contemporary trends, particularly example the progress in generative AI achieved by Large Language Models, inspired by the structure of neural nets, have largely superseded theories about knowledge representation (though academic research on knowledge representation continues).

Independently, and of prior origin to research in artificial intelligence, philosophers and latterly computer scientists have sought to improve the rigor of mathematics and science by the use of formal notations.
An influential figure in this was Leibniz, who sought a formal \emph{lingua characteristica} in which scientific knowledge might be represented in a form amenable to mechanical decision procedures (a \emph{calculus ratiocinator}).
Leibniz's project was unrealisable because of weaknesses in the logic of his day and limitations in the capabilities of computing machinery.

The first limitation was fundamentally breached by the work of Frege on his \emph{concept notation} (\emph{begriffsschrift}\cite{frege79}), an advance progressed through the 20th Century in the new discipline of \emph{mathematical logic}.
The second difficulty was mitigated by the invention of electronic digital computers, and the continuing escalation of their computational power.
Notwithstanding the immense computing resources now available, the available software to support formal reasoning still falls short of what is needed to facilitate its widespread adoption.

Frege was motivated to establish (contra Kant) that mathematics was properly considered a part of logic, but his influence, and that of Bertrand Russell, helped to shape the aspirations of Rudolf Carnap, the central purpose of whose philosophical programme was to facilitate more rigour in logic, mathematics and science using the new methods.

Thinking about the structure of knowledge, at least in philosophy and in the methodological thinking of scientists, motivation independently of any contribution it might make to cognitive engineering.

\section{Epistemological Precedents}

The history of Western Philosophy can be viewed though an epistemological lens.
This epistemological synthesis is conceived as a continuation of that history.

That story begins two and a half millennia ago with the ancient Greek philosopher Thales, around 600 BC.
It is then that historians of mathematics see the beginning of mathematics as a theoretical discipline, in which the `elements' of mathematical theories (notably, but not exclusively of geometry) could be established from small numbers of axioms and definitions of relevant concepts by deductive reasoning.

Those early Greek philosophers were not philosophers in the sense we now attach to that term, but more broadly, lovers of knowledge, polymaths.
In this, they were distinguished from their predecessors by their seeking knowledge by observation and reason rather than religion, tradition, authority or superstition.

There proved to be a considerable disparity between what could be achieved by reason in theoretical mathematics and what could be achieved in other domains.
Outside the clearly prescribed subject matters of mathematics, reason was too prolific.
It yielded contradictions, and could therefore not be trusted to yield truth.

This is seen conspicuously by the philosophies of Parmenides and Heraclitus.
Parmenides held that nothing changes, while Heraclitus conjured a work of pervasive and persistent flux, everything forever changing.

We may see in the philosophy of Plato an early epistemological resolution of these conflicting perceptions.

One way of thinking of this contrast is to take the flux of Heraclitus as a comment on the chaos of the world as viewed through our senses in default of any understanding of stable underlying structure which governs that flux.
The aim of philosophy is to make sense of that diversity of experience through the discovery of those stable underlying principles.
For Plato those stable underpinnings were to be found in 'ideal forms' which provided models to which the world of appearances approximate.

Plato resolved the apparent contradiction by positing those distict `worlds'.
One a stable world of which we might have certain knowledge discovered by reason.
The other, that world of fleeting appearances which in some measure approximates to those ideals, of which only opinion is possible.

This is the first major appearance of a partition which would remain central to Western Philosophy up to the present day.
Throughout that history the division between what could be known through reason and what could be discovered by observation and experiment would gradually evolve and is once again centred by the epistemological synthesis presented here.
It is perhaps most prominent in the history of philosophy after the emergence of modern scientific method with the presentation of the major philosophers as belonging to two distinct tendencies, rationalist and empiricist.

Descartes, Leibniz and Spinoza, the most celebrated rationalists, are so called for their belief that true knowledge comes from reason, while Locke, Berkeley and Hume, the prominent empiricists, held that knowledge could only come via the senses, by observation or experiment.
This is a caricature.
David Hume not only acknowledged a sphere for rational knowledge almost as broad as that envisioned by Plato, the Greek pre-cursor of rationalist philosophy.

After the philosophy of Hume, Kant's reaction against Hume was

\section{Recasting Positivism}

The synthesis which I offer may best be understood by contrast with the most recent major progression of positivism know as \emph{logical positivism}.
There was no unanimity of opinion of course (this is philosophy) so I will often be drawing the contrast with the particular views of Rudolf Carnap, but sometimes also for that English popularisation in Ayer's \emph{language Truth and Logic}\cite{ayer36}.
For Carnap's views I draw particularly from his 1934 lectures in London (for a lightweight perspective corresponding to his \emph{Logical Syntax of Language}\cite{carnap37}.






\phantomsection
\addcontentsline{toc}{section}{Bibliography}
\bibliographystyle{rbjfmu}
\bibliography{rbj2}

\addcontentsline{toc}{section}{Index}\label{index}
{\twocolumn[]
{\small\printindex}}

%\vfill

%\tiny{
%Started 2023-08-19


%\href{http://www.rbjones.com/rbjpub/www/papers/p038.pdf}{http://www.rbjones.com/rbjpub/www/papers/p038.pdf}

%}%tiny

\end{document}

% LocalWords:
