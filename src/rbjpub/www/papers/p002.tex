% $Id: p002.tex,v 1.2 2004/04/26 15:38:04 rbj Exp $
\documentclass{rbjk}

\newdisplay{guess}{Conjecture}

\begin{document}                                                                                   
\begin{article}
\begin{opening}  
\title{Analyticity and Abstraction}
\runningtitle{Analyticity and Abstraction}
\author{Roger Bishop \surname{Jones}}
\runningauthor{Roger Bishop Jones}
%\runningtitle{}

\begin{abstract}
Various set theoretic definitions of the concept of analyticity are presented and their characteristics consdered.
It is noted that certain techniques for abstraction prevent defining the synthetic as complementary to analytic, and the consequences of this problem are explored, leading to new ways of defining analyticity.
\end{abstract}
\end{opening}

\section{Introduction}

In his paper ``Two Dogmas of Empiricism'' \cite{Quine53} Quine provided the following account of the concept of analyticity:

\begin{quote}
 It is obvious that truth in general depends on both language and extra-linguistic fact. The statement `Brutus killed Caesar' would be false if the world had been different in certain ways, but it would also be false if the word `killed' happened rather to have the sense of `begat'. Thus one is tempted to suppose in general that the truth of a statement is somehow analysable into a linguistic component and a factual component.
 Given this supposition it next seems reasonable that in some statements the factual component should be null; and that these are the analytic statements.
\end{quote}

This is a prelude to his conclusion doubting the analytic/synthetic distinction, but it is also one of the best concise descriptions of that modern conception of analyticity which is of present interest, serving to distinguish it from rather differently formulated conceptions such as that of Kant.

Quine's skepticism about the analytic/synthetic distinction, at least as argued in `Two Dogmas', is rooted in skepticism about semantics, and it is doubtful that Quine would have disputed that if the semantics of a language can be precisely defined then the concept of analyticity for sentences of that language can be defined in terms of that semantics, or even that a generic definition of analyticity as a relation between semantics and sentences could be drafted.

Though I do not share Quine's skepticism about semantics, this is not my present interest, and beyond taking his description of analyticity as my starting point I will not be further concerned with `Two Dogmas'.
My present concern is with the details of the definition of analyticity {\it given a semantics} which turns out not so trivial as one might have hoped, and with what can be learned from the difficulties which arise.

The difficulties which I will be discussing have become apparent to me through my familiarity with and reflection upon details of how `formal methods' are sometimes applied in the development of security or safety critical IT systems.
The development of safety or security critical information systems now sometimes involves the construction of formal proofs about about abstract models of aspects of these systems and their operating environments.
When considering the scope of such modelling methods and the philosophical issues which arise from their use the concept of analyticity may be thought to have a role.
The truths demonstrated about such models are not truths of logic as this term is often now understood, since many the concepts on whose meaning they rest are highly application specific, but because the models are {\it abstract} the theorems are logical consequences of the definitions.

However, when we look closely at some of the logical systems employed and at some of the kinds of abstraction advocated and practiced, we find that naive formulations of the analytic/synthetic distinction fail.
In this essay I will first provide such a naive formulation of the analytic/synthetic dichotomy, then show how this fails in the context of these applications, and finally offer a more complex account of the dichotomy having more satisfactory characteristics.

\section{Application Context}

There is a high level of diversity evident in the application of formal techniques in software engineering.
Along one dimension of variation there has been considerable research and advocacy of the use of constructive logics and proofs, while other groups prefer to make use of the full strength of classical (i.e. two-valued) higher order logics and/or set theory.
Other groups have developed algebraic specification languages which realise high levels of abstraction.

The considerations which concern us here have arisen in just some of these contexts, the essential features of which are described here.
It is the use by these subcultures of the ``formal methods'' community of logical {\it foundation systems} which makes question of analyticity seem particularly relevant, and it is the adaptation of these traditional logical systems to meet novel desiderata which gives rise to difficulties in the formulation of the concept.

{\it foundation systems} arise from the foundational work of Frege and Russell and from their desire to demonstrate the logicist thesis that mathematics is logic.
Both Frege and Russell approached this by developing a logical system in which mathematical concepts could be introduced {\it by definition} and then employed in the derivation of mathematical theorems exclusively through the rules of the logical system.
Though the logicist thesis has not been widely accepted, it is the case that when the semantics of these foundation systems is made clear the soundness of the deductive systems and hence in a naive sense, the analyticity of all the theorems (i.e. that their truth is a logical consequence of their meaning) is readily demonstrated.
This makes it appealing to consider the use of the concept of analyticity in giving a philosophical account of the basis for this kind of application of formal methods.

Though these logical systems are directly descended from the systems of Frege and Russell, they have been streamlined by the work of later logicians and elaborated by computer scientists for pragmatic reasons.
A small number of these developments are relevant to our present discussion, all of which relate to the nature of definitions.

The first development is to discard the idea of definition as syntactic abbreviation, and to consider it as an act of naming.
An apparent advantage of treating definition as providing a syntactic abbreviation is its ontological neutrality.
There is no need when adopting such a definition for there to be something which is named by the expression thus abbreviated.
This was of particular value for Russell, because at the time he devised his Theory of Types, he was greatly vexed by the problem of consistency and under the influence of occams razor was eager to make do with as sparse an ontology as possible.
However, one cannot do mathematics without supposing mathematical entities to exist, and Russell was less successful than he had hoped in dispensing with entities.
If you have enough entities in your ontology, then it becomes practical to treat definition as an act of naming, or simply as the introduction of a new axiom which identifies a name not previously used with the entity denoted by some expression involving only names previously introduced.
Such an axiom is, in appropriate logical systems, ``conservative'' and serves much the same purpose as a syntactic abbreviation would have, while being in some respects technically and pragmatically superior.

\subsection{Notation}

The formal notation used in this paper is that of first order set theory, the connectives quantifiers and primitive relations are rendered:
\[ \land, \lor, \lnot, \Rightarrow, \Leftrightarrow, \forall, \exists, =, \in \]

The primary use of this formal notation is in defining properties and relations over structures represented as sets.
Set abstractions are written:
\[ \{x:A\ |\ P\} \]
where $x$ is a variable which may occur in P (and is bound in that context) and A is a term (in which occurrences of x are free).
The term denoted by the abstraction is the set of elements $x$ such that $x \in A \land P$.  
The colon ($:$) is used in this context as a alternative notation for membership, and will similarly be used in bounded quantification, for example:
\[ \forall x:B (x \in A) \]
abbreviates:
\[ \forall x (x \in B \Rightarrow x \in A) \]
A structure is represented as an ordered $n$-tuple for some $n$, where an $n$-tuple is a map whose domain is the set of natural numbers less than $n$, and we may (but need not) suppose that each natural number is the set of smaller natural numbers.
$n$-tuples are written
\[ (x_1, x_2, \ldots x_n) \] where $x_i\ (1\leq i\leq n)$ is an expressions denoting the $i$th component of the $n$-tuple.
A function is represented as the set of ordered pairs which is its extension, and function application is by juxtaposition with the function on the left and the argument on the right.
The value of a function $f$ for an argument $x$ is the value $y$ such that $(x,y) \in f$. 

Where ordered tuples are introduced for specific purposes each element of the tuple is given a name which may be thought of as denoting the numeric position of the element in the tuple and which may therefore be use as an arugument to the tuple for selecting from it the corresponding element.
Where this is done function application is rendered with a full-stop.
These component names will be unique within any kind of structured entity but the same name may be used in more than one tuple, in which case it is intended that their uses can be informally disambiguated by context.

\section{Defining Analyticity}

\subsection{Preliminary Considerations}

\subsubsection{abstract}

We propose to offer a definition of analyticity in general, defined as a relation between a sentence and the semantics for the language in which the sentence is rendered.

Just as there are some technical advantages in abandoning a syntactic notion of definition, a general notion of analyticity is more easily defined if we do not construe semantics syntactically.
The proposal is therefore not to think of a semantics as being some kind of syntactic object such as Tarsli's `T' schema, but rather as some kind of abstract object, such as a set or a function.

\subsubsection{Truth Conditional Semantics}

A full semantics must capture {\it all} the meaning in the sentences (and other syntactic categories) of a language.
This may not all be necessary for the purpose of defining the concept of analyticity.
It suffices for this purpose to have the truth conditions of sentences in the language.

A truth conditional semantics is one which tells us under what conditions a sentence is true.
To formulate such a semantics, one must determine what conditions are material to the truth of a sentence in the language under consideration.
This amounts to a determination of the subject matter of the language.
In the case of natural languages, the subject matter is not constrained, but in order to give a definition of the semantics some conception of how the universe might be as considered through the language is necessary.

Typically the sentences of a language have a meaning which is sensitive to the context in which the sentence is affirmed.
We may think of a statement as being a combination of a sentence and its context of assertion, and of a truth conditional semantics as mapping statements into truth valuation functions, each of which in turn is a function taking the state of the universe (or other domain of discourse) and yielding a truth value.
Let us allow that this function be partial.

This leaves us with an abstract model of the semantics of a language as the following as an 8-tuple (S, C, D, P, TV, T, M, V) whose components are:

\begin{enumerate}
\item a set of sentences S
\item a set of contexts C in which sentences may be asserted 
\item a domain of discourse D (a set of possibilities)
\item a set of propositions P
\item a set of truth values TV
\item a distinguished truth value T (truth)
\item a meaning function M from $(S \times C)$ to P giving the meaning of a sentence S asserted in context C
\item a valuation function V from $(M \times D)$ to T
\end{enumerate}

\subsection{Analyticity Simpliciter}

A statement $sc$ in some language $l$ is analytic if its truth can be known from its meaning alone, i.e. if $l.V(l.M sc, d) = l.TV$ for every $d \in l.D$.
i.e.:
	\[ Analytic_1(sc, (S,C,D,P,TV,T,M,V)) \Leftrightarrow \forall d:D (V(M(sc, d) = T) \]

This definition reflects the common practice of using analytic to refer specifically to $true$ statements.
Since we are interested in the analytic/synthetic $dichotomy$ this usage is something of a disadvantage, statements which are not analytic, may simply be false rather than synthetic.
We will therefore henceforth use the term $analytic$ to mean either analytically true or analytically false (or analytically of some other truth value).

	\[ Analytic_{\ 2}(sc, (S,C,D,P,TV,T,M,V)) \]
	\[\Leftrightarrow \exists t:T(\forall d:D (V(M(sc, d) = t)) \]

\section{Loose Specifications}

Occasions arise, both in pure mathematics and in engineering applications of formal methods, when it is desirable for a specification to be incomplete.
For example, if we use first order logic to reason about groups in general, the axiomatic system we use will not be categorical, since every group will be a model of the axioms.

\section{Revisiting Analyticity}


\begin{thebibliography}{}

\bibitem[\protect\citeauthoryear{MacKensie}{2001}]{MacKensie}
Donald MacKensie: 2001,
\newblock {\it Mechanising Proof - Computing, Risk and Trust},
\newblock {The MIT Press}.

\bibitem[\protect\citeauthoryear{Quine}{1953}]{Quine53}
Quine W.V.: 1953,
\newblock {Two Dogmas of Empiricism},
\newblock {in {\it From a Logical Point of View}, New York, Harper \& Row, 1963}.

\bibitem[\protect\citeauthoryear{Russell}{1908}]{Russell08}
Bertrand Russell: 1908,
\newblock {Mathematical logic as based on the theory of types},
\newblock {in van Heijenoort (ed.) {\it From Frege to Godel - A source book in mathematical logic 1879-1931}, Harvard University Press, Cambridge Massachusetts, 1967, pp. 150-182}.

\bibitem[\protect\citeauthoryear{Tarski}{1908}]{Tarski36}
Alfred Tarski: 1936,
\newblock {On the concept of logical consequence},
\newblock {in John Corocoran (ed.) {\it Logic, Semantics, Meta-mathematics}, Oxford University Press, 1956, pp. 409-420}.

\end{thebibliography}
\end{article}
\end{document}
