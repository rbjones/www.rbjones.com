% $Id: p036.tex $
% bibref{rbjp036} pdfname{p036}
\documentclass[10pt,titlepage]{book}
\usepackage{makeidx}
\newcommand{\ignore}[1]{}
\usepackage{graphicx}
\usepackage[unicode]{hyperref}
\pagestyle{plain}
\usepackage[paperwidth=5.25in,paperheight=8in,hmargin={0.75in,0.5in},vmargin={0.5in,0.5in},includehead,includefoot]{geometry}
\hypersetup{pdfauthor={Roger Bishop Jones}}
\hypersetup{pdftitle={An Epistemological Synthesis}}
\hypersetup{colorlinks=true, urlcolor=red, citecolor=blue, filecolor=blue, linkcolor=blue}
%\usepackage{html}
\usepackage{paralist}
\usepackage{relsize}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{longtable}
\usepackage{url}
\newcommand{\hreg}[2]{\href{#1}{#2}\footnote{\url{#1}}}
\makeindex

\title{\bf An Epistemological Synthesis}
\author{Roger~Bishop~Jones}
\date{\small 2023/12/17}


\begin{document}

%\begin{abstract}
% An epistemology constructed for the advancement of science and technology in the age of interstellar hybrid (human, synthetic, post human) intelligence.
% 
%\end{abstract}
                               
\begin{titlepage}
\maketitle

%\vfill

%\begin{centering}

%{\footnotesize
%copyright\ Roger~Bishop~Jones;
%}%footnotesize

%\end{centering}

\end{titlepage}

\ \

\ignore{
\begin{centering}
{}
\end{centering}
}%ignore

\setcounter{tocdepth}{2}
{\parskip-0pt\tableofcontents}

%\listoffigures

\hfill
\ 
\pagebreak

%\addcontentsline{toc}{section}{Index}\label{index}
%{\twocolumn[]
%{\small\printindex}}

%\vfill

%\tiny{
%Started 2023/07/21


%\href{http://www.rbjones.com/rbjpub/www/papers/p032.pdf}{http://www.rbjones.com/rbjpub/www/papers/p036.pdf}

%}%tiny

\section*{About}

This book attempts an epistemological synthesis.

I think of this as the kernel for a complete philosophical system which is perhaps easiest understood as a 21st century successor to Logical Positivism, particularly as exemplified by the philosophy of Rudolf Carnap, whose central project was to do for science what he supposed Russell and Whitehead had done for mathematics in their \emph{Principia Mathematica} \cite{russell1913} and to progress a conception of scientific philosophy articulated by Russell in his book ``\emph{Our Knowledge of the External World, as a Field For Scientific Method in Philosophy}.'' \cite{russell21}
That achievement was to realise Frege’s conception of mathematics as the result of deductive reasoning from the logical definition of the relevant mathematical concepts.  I will gloss over for the moment the controversy about whether this really was achieved, and the question how any such achievement could be extended to empirical sciences.

Carnap’s conception of philosophy was exclusively as ``logical analysis'' which he construed in terms of the new logical systems introduced by Frege and Russell, and he consequently regarded broad swathes of philosophy as it was and had been practiced as meaningless, particularly the whole of that most fundamental of philosophical disciplines, metaphysics. There are many differences in my own philosophical perspective, and I am disinclined to police the boundaries of the discipline, but in important ways I share Carnap’s standpoint, and one of those concerns the importance of logical rigour which depends upon clarity of language and demands strict conditions on the formulation of problems before the questions they raise can be considered meaningful.

Carnap’s concern for precision and clarity place language at the centre of his philosophical system, whereas I have chosen to place epistemology in that place, while addressing those kinds of knowledge which are dependent upon language and therefore acknowledging the foundational interplay between all four most theoretical branches of philosophy, metaphysics, language, logic and epistemology. Despite my own sense of proximity to Carnap and the vehement rejection of metaphysics which he retained throughout his life, I am relaxed about metaphysics and see in Carnap’s philosophy the roots of my own more accommodating stance.

Epistemology is that branch of philosophy which is concerned with the theory of knowledge.
In the last century there has been controversy about whether epistemology should be a part of some philosophical prelude to science, possibly called “first philosophy”, and therefore conducted a priori, or be a part of empirical science, “epistemology naturalised”.   Two developments seem to me to have shaken that dichotomy.   One is Postmodern philosophy, which denied the possibility of objective knowledge (except where politically convenient), and construed “episteme” as a cynical tool for exploitation (and hence as a matter of choice).  Another is the advance of information technology towards the engineering of intelligent artefacts.  The latter development made the representation of knowledge, and hence potentially choices about what knowledge is into an aspect in the design of synthetic cognitive systems (though most contemporary AI research does not do this).

Notwithstanding debate about how AI should be achieved, the aspiration to a systematic coherent repository of scientific knowledge, which is centuries old, is likely to be advanced with the aid of intelligent machinery, and questions about the best way to organise such a repository will, at the most abstract levels, be matters into which philosophical considerations enter.

Though the central thrust of the proposed work is an epistemological synthesis, this is a projection into the future of a process of epistemological evolution without which the proposed synthesis would be unthinkable.  An understanding of that history is an exercise in natural epistemology and a very selective account bringing out the key features which are important to making, understanding and reasoning about the proposed synthesis will be the subject matter of the first part of the book.

\section*{Prelude}

This is a barely started manuscript which is in the course of a major restructuring, so what it says about structure is likely to differ from much of the present structure, and hence finding your way around it may be difficult.
It is too early to make publicly available, but ...

\chapter{Introduction}

We are in interesting times, in many different respects.

The phenomenon of greatest interest in provoking the ideas presented here is the imminence of intelligent machines, and the possibility that these machines will enable a transformation in the ways in which knowledge is gathered and applied.
The interest to me is not so much in how intelligent machines might be devised which replicate those human capabilities which we consider to demand intelligence, but in the prospect that information processing technology might facilitate the adoption of better ways of gathering and exploiting knowledge.
The formal basis for such methods is known but is too cumbersome to be widely applicable without considerable advances in the available automation.

I was born into a world which anticipated machine intelligence, but had a limited understanding of how that might be achieved, and of machines which fell short of the brute compute power which would be needed. 
My own interest in the problem dates back to my first days as an undergraduate (though it was not a subject which undergraduates could then study).
It was then that I first learned a little of the work of Alan Turing, a famous logicism who subsequently exerted subtantial influence over the field of artificial intelligence (in ways which he probably did not intend).

That first spell as an undergraduate student of engineering proved short lived, and years later I returned to academic studies to obtain a BA in mathematics and philosophy.
My taking up of philosophy was a surprise to me.
Philosophy struck a deep chord, and gave me a sense of vocation not diminished by my perception that I was completely unsuited to progressing an academic career in philosophy.
This was the beginning of a lifetime of philosophical rumination, and a more or less continuous search for a way to turn those ruminations into substance.

My interests in formal logic, computer languages, artificial intelligence and under it all, philosophy, all date from that time.
They coloured my thinking many years later when I had the opportunity to work with the Cambridge HOL system, then recently developed by Mike Gordon and his embryonic Hardware Verification Group at Cambridge University.

The epistemological synthesis which I present below was seeded by the perception that the Cambridge HOL system possessed many of the desirable characteristics of a representation system for knowledge suitable for use in the kind Leibnizian project for the mechanisation of scientific knowledge which the advent of machine intelligence demanded and would facilitate.

The synthesis is \emph{foundational} in multiple senses, and appropriately for an epistemological enterprise, it digs deep in its response to the kind of academic scepticism to which it is inevitably subject.
Though not so intricate as G.E.Moore's defence of common sense \cite{moore1925,moore1993}, it is in its way similarly grounded, not in the good sense of the common man, but in that of the practical engineer.
Because of its inspiration in (rather exotic) engineering practices, my account begins there, before before starting afresh from a philosophical ground zero.
\section{Purpose and Structure}

\section{Varieties of Epistemology}

\part{Synthetic Epistemology}

\chapter{A Priori Truth}

This part is devoted to defining certain concepts culminating in the idea of a \emph{Universal Foundation for Logical Truth} and to two such foundation systems, first order set theory and the Cambridge HOL logic, which are shown to be equivalent as universal foundations.

As normally presented these logical systems are not equivalent.
There are more logical truths (in the sense defined below) expressible in the standard interpretation of the ZFC axioms for set theory than in the Cambridge HOL logic.
But the notion of Universal Foundation we expound consists in an unbounded sequence of increasingly expressive systems distinguished primarily by the cardinality of their minimal ontologies, i.e. in how many things they presume there to be.
The claim to equivalence is the claim that every logical truth which is expressible in some member of one of the the family is also expressible in a member of the other family.

It is a hypothesis that there are no more expressive universal foundation systems, which is treated in a manner similar to Church's thesis.
Rather than being proven, it survives until some convincing counterexample may be found.
There is of course a difficulty in any logical system which abjures semantics, as to whether an interpretation in set theory can be correct.
Formally there is little doubt that in any system formally consistent, if only in the sense of Post, can be given an interpretation in these universal foundations which is based on identifying truth with provability, but whether this would be philosophically acceptable to those who prefer constructive foundations is moot.

In defining these ideas an important element is the choice of reduction, particularly in relation to semantics and hence logical truth.
Two formal languages are considered equivalent if they a mutually interpretable by functions which preserve meaning, so a relevant notion of meaning must be articulated.

Once the definitions are complete there are two directions of further discussion, which flow from the distinct roles envisaged for the two Universal Foundations which have been chosen to exemplify the concept.

The Cambridge HOL logic is chosen because there is a natural way in which we can define the abstract semantics of some arbitrary language and then in practice be able to reason with that language in the resulting context in HOL.
This is a consequence of the abstract syntax of the simply typed lambda calculus being effectively a universal abstract syntax (aided and abetted by the ability to define new type constructors which correspond to syntactic categories in the required abstract syntax).

First order set theory on the other hand, is more convenient for establishing the semantics and proof rules of these universal foundations.
This helps in addressing sceptical arguments from regress in relation to both meaning and proof in the foundations.
In addressing the regress in meaning and justification, careful but ultimately circular arguments prevail.

\chapter{First Philosophy and Foundationalisms}

I am here engaged in something which may reasonably be considered ``First Philosophy'' and which shares something of the motivations and purposes which earlier philosophers have considered under that heading.

That kind of philosophy which us thought of above all else as a rational enterprise must surely first of all put in place the necessary pre-conditions for those kinds of knowledge which are amenable to deductive reason.
I am therefore concerned with \emph{knowing that} rather than \emph{knowing how}.

I think of \emph{knowing that} generally as consisting in the possession of some kind of model of aspects of its subject matter.
Such models or representations may be of particulars states or of patterns of change  in the form of scientific laws. 
The role of deduction is in the application of such models, which may consist in permitting future states to be inferred from present states given knowledge of the laws, or may establish the suitability of some design by demonstration of its behaviour in its intended use.

This kind of application of deduction is often mediated by mathematics, the role of which is to enable the construction of abstract models which can be applied in the above ways.

For these purposes we need knowledge represented by sentences in a declarative knowledge which has a definite semantics, i.e. in which there is a clear interpretation of the sentences of the language as to what they say about the intended subject matter.
Languages which are suitable for expressing this kind of knowledge may also be equipped with detailed syntactic rules which govern sound deductive reasoning with sentences of the language.

\section{On The Nature of First Philosophy}

To give a sense for what I here offer as ``first philosophy'', or of why I find that term appropriate I will say a few words about how ``first philosophy'' has been construed by some earlier philosophers, namely Aristotle, Descartes and Carnap.

\subsection{Aristotle's Conception of First Philosophy}

``First Philosophy'' is the phrase used by Aristotle to describe the subject of his writings which were to be labelled ``Metaphysics'' by later editors.

Aristotle's con

In Aristotle's time, the scope of ``philosophy'', the love of knowledge, embraced the science of his day, and most of Aristotle's work was concerned with the various sciences, which he classified as theoretical if

The term \emph{First Philosophy} was first used by Aristotle, who used it to describe the matter of the volume which later editors named ``Metaphysics''\cite{aristotleMetap}.

Aristotle had a hierarchic conception of knowledge, lower levels concerned with brute facts close to our practical experience of the world, and higher levels involving gradually increased abstraction.
At the highest level, completely divorced from practical applications,and  exhibiting the highest level of abstraction and degree of true wisdom, was \emph{First Philosophy}.
He saw first philosophy as concerned with `being qua being' or being \emph{in  itself}, rather than being \emph{as matter} or being \emph{as mind} or any other kind of being, and thus as a study prior to any of the sciences concerned with their special subject matters.

Descartes, much later, had a distinct emphasis and character to what he offered as first philosophy \cite{descartes2013meditations}, which connects it more directly with the foundational approach to an epistemological synthesis attempted here.
DesCartes wanted to discard all that had preceded him and start a rational reconstruction from scratch, for which first philosophy provided a starting point.

I use it here loosely, rather than in close correspondence with Aristotle's Metaphysics, because it is suggestive that philosophy must begin somewhere, and hopefully progress onward, rather than being a amorphous mass incapable of orderly presentation.

As well as being appropriately presented in a progressive way, my conception of ``First Philosophy'' includes the idea that some aspects of philosophy are also logically prior to others, and indeed, that a presentation which constitutes a logical progression is desirable (in a primordial sense of ``logical'' which is mere suggestion prior to any conception of what logic might be).

It is furthermore, here coupled with notions of foundation, and thus with something like the kind of doctrine which has been called ``foundationalism'', but which lacks some of the absolutism which is commonly associated with that term, and which makes such foundations susceptible to radical scepticism.

I do not seek an epistemology which looks for absolute certainty, but rather for one which may be an effective basis for the future prosperity and well being of humanity.

If I were to launch immediately into the most fundamental elements of this supposed logical progression, the reader might be left in a state of suspense as to how the various sceptical arguments which might be marshalled against it could be answered.
This preliminary discussion is intended to mitigate the suspense.

One modern response to various scepticisms may be found in G.E.Moore's ``A Defence of Common Sense'' \cite{moore1925,moore1993}.

\subsection{Some History of Foundations}

In contrast with Philosophy, Mathematics is that domain of intellectual endeavour which has the very highest reputation for clarity and certainty.
Since the transformation of Mathematics into a theoretical discipline (rather than a collection of practical methods), realised by deductive reasoning, its reputation has been the envy of many philosophers and has inspired a number of attempts to usurp its deductive imprimatur for the sake of methods and doctrines which fall short of deductive rigour.

When we look under the hood we find that reputation has been achieved by ideas which are broadly (but sometimes explicitly) \emph{foundational}, that there have been wide variations in the rigour of the proceedings and that, arguably, the more advanced mathematics has become, the more it has depended upon clearly articulated and elaborately constructed foundations.

The point of this section is to suggest that the interest of mathematicians and some branches of empirical science and engineering in foundational issues is distinct from that of philosophers.
Philosophers are more likely to take foundational studies as intent on achieving absolute precision of meaning and certainty of proof, making of ``foundationalism'' a strawman readily refuted.
A defence against such foundational scepticism might then be analogous to Moore's rejection of sceptical arguments on the basis of the certitudes of common sense.
It might serve such a defence to examine the practice of mathematicians and engineers, for whom foundational advances are a vital part of how their discipline may be progressed.

Sir Thomas Heath observes, in the preface to his history of Greek Mathematics \cite{heath1921}, that the foundations of mathematics are Greek, and consisted in first principles, methods and terminology.
Our knowledge of early Greek mathematics and of the rigour of its demonstrations comes to us primarily in the compilation by Euclid in his elements, which is the culmination of 300 years of Greek mathematics, and probably not representative of standards uniformly adopted \emph{ab initio}, but rather the result of gathering together 2-300 years of the results of work if varied standards of rigour into a single body of work presented to a uniformly high standard of rigour based on the same fundamental principles and methods.

What we see there, and what we will describe in greater detail concerning more modern foundational work, was not a starting point from which all the research was undertaken, but the result of a period of evolution of methods which ultimately could be codified to provide the body of work with a highly rigorous derivation from a single  foundation.

Axiomatic Euclidean geometery was a high water mark in the rigour of mathematics which was not to be surpassed for over two thousand years, probably because the need was not then felt, despite considerable further development of mathematics since the time of Euclid.

Eventually developments transpired which propelled mathematics at the same time, into its greatest practical significance and beyond the confines of rigour.
This flowed largely from the work of Newton and Leibniz.
Newtonian mechanics necessitated the kinds of operations on mathematical functions which were to be supplied independently by both Newton and Leibniz.
The procedures of differentiation and integration, which operate on functions to give on the one hand, the slope of the graph of the function and on the other the area under its curve.

The definitions of these concept made special demands upon the number system, particularly in the use by Leibniz of infinitesimal quantities, but also in Newton's account of fluxions as `last ratios'.

Doubts about the rigour of the procedure were voiced by Berkeley
who challenged the coherence of the ideas of fluxion and infinitesimal quantities\cite{berkeley2018analyst} in 1734 but it was not for another century before mathematicians became seriously embarrassed:

\begin{quotation}
  \emph{There are are very few theorems in advanced analysis which have been demonstrated in a logically tenable manner. Everywhere one finds this miserable way of concluding from the special to the general, and it is extremely peculiar that such a procedure has lead to so few of the so-called paradoxes.}

Abel, 1826
\end{quotation}

There ensued the most fertile period of foundational innovation in the history of mathematics.
This was not a search for absolute certainty, but for conceptual clarity and deductive rigour.
It occurred in several stages, most of which were instigated by Mathematicians and represented real advances in mathematics.

The first stage in this process was to show that the central notions of the calculus could be made precise without resort to infinitesimals or fluxions.
This aspect of the work had already been undertaken by Cauchy who's 1821 book ``Cours d'analyse''\cite{bradley2010cauchy}, rigorously defined the concept of convergence for sequences and series using what are now called Cauchy sequences.
Cauchy used these sequences to provide a precise epsilon-delta definition of limit and continuity for real functions which provided the means to  define differentials and integrals as limits \cite{cates2019cauchy}, giving precision and clarity to procedures which had evolved less formally over thousands of years going back to the work of Archimedes.

Having made these important advances in the rigour of analysis, there remained significant areas of uncertainty.
There was still no clear conception of the number system on which this work depended, though the position had improved with the elimination of infinitesimals from the theory.
The development of analysis had made it necessary for mathematical functions to be regarded as abstract entities rather than syntactic expressions, but the coherence of that position depended on clarification of the exactly what these abstract functions are.

From a modern perspective the clarification of these two points rests on the theory of sets which was begun by Cantor, but the reduction to set theory was a later development.

\section{Some Preconditions of Deductive Rigour}

The centrepiece of this synthesis is a formal logical system, consisting of a declarative language with an abstract denotational semantics and effectively decidable deductive rules.

The presentation here is informal, but describes formal treatments many of which are already in place in the literature, and all of which would generally be accepted as entirely feasible.
This preliminary discussion concerns the key requirements that will have to be met, with a first indication of how those requirements will be met.

Deduction can only take place in a declarative language which has a definite truth conditional semantics.
It is possible to formally reason in languages which lack such a semantics, and there are technical conceptions of consistency (e.g. consistency in the sense of Post) which give minimal criteria of correctness for such a deductive system, but they nevertheless fail to qualify as deductive if it cannot be shown to preserve truth.

\chapter{Logical Truth}

In due course I will address some of the sceptical challenges to the idea of first philosophy, and to many aspects of the synthesis which is proposed, but at this stage I am concerned only to precisely describe the first stages in the synthesis by offering some informal definitions.

The single most important concept around which all else is constructed is that of \emph{logical truth}.
Many will be inclined to contest my use of that term for the concept which I will now describe.
It is the concept which is important, not the name I use for it.

Ultimately, in the synthesis which I provide, there is a place for addressing all the forms which knowledge may take, but insofar as epistemology is concerned with subject matters belonging to the natural sciences, it will receive no special attention beyond the considerations later advanced for the empirical sciences.

Placing the epistemology of logical truths (logically) prior to that of empirical science reflects the important role which mathematics plays in the natural sciences, which we consider in its broadest sense as the interpretation of science as constructing abstract models of physical systems and phenomena.

The concept of logical truth will be defined in the following stages.
\begin{itemize}
  \item
    In the first instance it is defined as a characteristic of sentences in languages of a particular kind (declarative languages with fixed vocabulary and a well defined truth conditional semantics).
  \item
Then the definition will be augmented to admit languages with an extendable vocabulary, whose truth conditional semantics is appropriately augmented by the process of extension.
In these latter we may say that the truth conditions for sentences in the language are sensitive to a context which reflects the extensions to the vocabulary and the constraints incorporated by the extensions.
\item
At this point we may speak of relationships between these 
\item

\end{itemize}


\section{Ontology}

I divide those things which may exist into three kinds as follows:

\begin{itemize}

\item concrete objects

  Those things which have spatio-temporal location and may be causally related to other concrete objects.

\item Purely abstract objects

  These are entities which do not have spatio-temporal locations and are not causally related to other objects.

\item Hybrid objects

  These are complex entities which are built from or built into both concrete and abstract objects, for example, a set of concrete objects will be such a hybrid.
  Thus, for example, if in the Simple Theory of Types, the individuals are held to be concrete objects, and the propositions are abstract, all other types will consist of hybrid entities.
  If on the other hand, the individuals are purely abstract, then the entire ontology will be purely abstract.
\end{itemize}

For the purposes of defining the notion of ``logical truth'' abstract ontology will suffice.

Some philosophers will think it reasonable to ask whether any abstract objects exist, and how we can possibly establish that they do.
By engaging in ``first philosophy'' I arrogate to myself the right to articulate without restraint whatever concepts will provide the basis for the epistemological synthesis which will follow.
This includes the concept of existence.
It is open to us to decide what that concept means and how it will be used in our system.

The position I adopt on the matter of existence is sensitive to the kind of entity concerned, and I need only at this point speak to the purely abstract entities at stake, in relation to which I proposed a wholly conventional stance.
This may be considered analogous to a fictionalist stance, differing from it in crucial ways.

The fictionalist is going to tell a story which he acknowledges is not factual, but which he considers instructive or entertaining in some other way.
The statements he makes in his fictional narrative will sometimes be false, and may not even be mutually consistent.

The conventionalist stance I take differs from fictionalism in these two principle respects:

\begin{itemize}
\item Within the scope of the relevant convention, the consequences of the convention are true, not false.
\item It is very important in the adoption of conventions, that the principles adopted be logically consistent, even when these conventions are adopted en route to the establishment of the concept of logical consistency.
\end{itemize}

In what follows hybrid objects will have no role, and it is very likely that I will tire of prefixing ``abstract'' with ``purely'', which should therefore be understood as implicit.

Normally in what follows, the adoption of some such ontological convention is a part of the definition of a formal language, and its scope is primarily in determing the meaning and truth value of expressions in that language.
More broadly in the less formal conduct of mathematics and perhaps other sciences, a convention may be simply contextual rather than being a part of the adoption of a language.
For example, there are multiple different formal axiomatisations of set theory, which disagree among themselves about the domain of which they speak, about which sets must, may or cannot exist.
The ``standard'' conception is of a domain exclusively of well-founded sets, but no axiomatic system guarantees that this is the case.
On the other hand, the axiom of separation, which guarantees the existence of all subsets of a set obtained selecting the members satisfying a given formula, makes it possible to prove that there is no universal set which contains all other sets.
Notwithstanding that difficulty with the universal set, many consistent set theories are available in which the universal set does exist and the principle of separation is not valid.

I may as well note in explaining my adoption of this conventionalist convention, that if I were to prove mistaken in disbelieving that there is any objective truth about what purely abstract entities exist (including the possibility that there are none), that the value that I associate with this position would be unabated.
Since abstract entities are causally unconnected with us, there existenve or otherwise in any absolute sense is immaterial to the utility which attaches to adopting the convention that some appropriate collection of abstract entities does exist.

Among the very many connections between the synthesis I present here and the philosophy of Rudolf Carnap, there is a direct connection here between my ontological stance and that which Rudolf Carnap attempted unsuccessfully to explain by distinguishing ``internal'' from ``external'' questions \cite{carnap50}.
``Internal'' questions are those posed in the context of some well defined language, and which are to be answered on the basis of the principles explicit or implicit in the definition of the language, ``exernal'' questions are those ontological questions which might arise in the process of defining the semantics of such a language, and were regarded by Carnap as meaningless pseudo questions.
Of course, we can ask what is the meta-language in question (which would normally be an idiom of a natural language such as English aumented by the vocabulary of some special discipline, perhaps mathematical or philosophical logic.
This is unlikely to provide any definitive ruling on the kinds of ontology which Carnap was considering,

The ontological conventionalism with respect to abstract entities which I am adopting here is not sensitive to the ontology of the metalanguage (which is indeed, in the first instance, English).
It is possible to describe an abstract domain whether or not the existence of that domain is demonstrable in that context, in just the same way that one can write a novel about an entirely fictional population.
If we then define the meaning of existence in the object language as membership in that possibly fictional domain, then ontological questions posed in that object language will be answered according to the structure of the chosen domain rather than by reference to any ontological principles which belong to the meta-language.

Though this discussion has addressed only abstract ontology, conventionalism in respect of concrete ontology is at least implicit in what will be said later about the representation of scientific and engineering knowledge.
It should also be noted that modern logic is generally agnostic about what things are, insofar as all properties and relations are external, and so long as those externalia are preserved, the substitution of a completely different collection of individuals in the domain of discourse has no effect on the truth values of any sentence.
This apparent indifference to identity is reinforced in the logical foundations which are proposed by ontological ambiguity in type constructors, the effect of which is that though various complex types of abstract entities are constructed in way similar to the constructions which would be used for mathematical objects in a pure set theory, there is no basis for identifying these newly constructed entities with the constructions themselves.
It is a foundational scheme in which numbers are not \emph{identifieed with} sets, or any other type of universal foundational ontology.

Though fully in the spirit of positivism, this epistemology is not nominalistic, a departure from the mainstream first appearing in Rudolf Carnap.

\section{Language}

It is not impossible to reason about the world without making use of language, but as soon as we talk about truth, language is presumed.
A generic definition of logical truth therefore depends upon an explicit and general account of some class of languages of which logical truth can meaningfully be predicated.

Though linguists and mathematical logicians may have conceptions of language which are purely syntactic (e.g. ``a set of sentences'') for the purpose of defining the concept of logical truth (as it is here construed), some semantics is essential.
The kind of semantics which is necessary is \emph{truth conditions}.

The essence of declarative language is the use of sentences to make claims which discriminate between various possibilities which constitute the subject matter of the language.
For the barest account of logical truth, we therefore require of the truth conditional semantics:

\begin{enumerate}

\item that a range of ``possibilities'' be identified, which are those for which the truth conditions settle a truth value.

\item that for each sentence in the language, a subset of possibilities is given, which are the cases for which the conditions expressed by the sentence under the semantics hold, in which case the sentence is then deemed true.
\end{enumerate}
    
\section{Logical Truth as Tautology}

For the broad swathe of languages which satisfy the conditions given above, the logical truths are those sentences which are true in every possibility, hence necessary.

It may be noted that there is a special case arising when there is only one possibility.
This is exemplified by the language of first order arithmetic when given with the standard semantics which admits of just one possibility (up to isomorphism).
In that case, the idea that a sentence conveys information when asserted by eliminating possibilities is not as instructive as it might otherwise be.
That we have a clear intuitive understanding of the structure of the natural numbers clearly does not entail that we know what arbitrary sentences of arithmetic are true, and it is therefore informative to know that a sentence is true even if it has not ruled out any possible structure of the natural numbers.

The sceptically minded will further note that this conception of logical truth is just the identification of logical truth with analyticity, and that the concept is thus rendered wholly conventional and lacks the absolute character which some would wish to see in the notion of logical truth.
These consequence arise however, not from any defect in the notion of logical truth thus articulated, but rather from the fact that the syntax and semantics of languages are completely conventional, and hence that, however absolute a conception of logical truth you might have, the question of which sentences express logical truths must be determined by those conventions (though I do not deny that the relevant ``conventions'' may in the case of natural languages have been fixed, if indeed they are fixed, by biological and cultural evolution rather than by the whim of language designers).

\section{Defining Foundational Universalism}

A ``Universal Foundation'' for logical truth would be a language to which logical truth in any other language is reducible.
The idea is therefore dependent upon what concept of reducibility is intended.

My motivation in addressing this topic comes from the desire to support a linguistic pluralism which includes the ability to reason about systems whose subsystems or components may be given in distinct languages.
In that case one needs a logical environment in which claims about subsystems in different languages can all be comprehended coherently.

A natural way to consider this kind of universality is by translation from each language into the universal language  which reduces truth in the source language to that in the target language.
The weakest constraint one could put on such a translation would be that it is an ``effective'' mapping which preserves logical truth.
This introduces a constraint on languages which it was not necessary to include in the definition of logical truth above, that the syntax, i.e. the set of sentences, be countable.
The kinds of reduction which arise in this way are those which are studied in \emph{recursion theory} and the the structure created by these reductions are various hierarchies of degrees of recursive unsolvability \cite{rogers1967theory}.

Regrettably, this extensive;y researched branch of mathematical logic will not give a convincing account of how logical truth can be encompassed in one universal family of languages unless we were prepared to regard all logically true sentences as expressing the same logical truth.
In that case we should be prepared to find, when attempting to prove a logical truth which had some material bearing on the problem we are addressing, that we were actually reasoning about some other proposition which happened to have the same logical status.

To progress this agenda we need much stronger constraints on reductions so that they can reasonably (and practically) be understood to preserve the meaning of the original.
For this purpose we need a more refined semantics than sentential truth conditions.
To accomplish this we need translations which not only preserve truth value, but preserve subject matter.
To that end we need more detailed structure in the kind of semantics which is to preserved by the reductions.

Not only should the mapping preserve the truth values of sentences, but it should preserve the values of the non-sentential expressions  which are the constituents of the sentences in the language.
We therefore appeal to the idea that the semantics should be presented as a homomorphism defined over the abstract syntax considered as a many sorted algebra, in which the sorts correspond to the various syntactic categories or phrase types in the phrase structure grammar of the language.

This more elaborate conception of semantics defines a new class of languages relative to which a claim to universality of some family of languages will be judged.

The notion of ``family of languages'' is quite restrictive, it is not some arbitrary collection of languages, but a series of very closely related languages, which have the same syntax but a progressively refined semantics.

\section{Two Universal Foundations}

Two putative universal foundations serve the purpose providing intelligible foundational underpinning for practical foundation system.
There is no suggestion that either of these is unique, but the two choices are made to be appropriate in distinct ways.

The first is chosen for the kinds of simplicity which contribute to underpinning the semantics and the proof rules through the deepest penetration of formality supported by intelligible but less formal semantic refinement.
The second is chosen for the convenience with which it can provide an underpinning for the variety of formal notations which are used in practice in mathematics, science and a full variety of practical applications in all domains.

The first language is the language of first order set theory, the second is the variant of Church's \emph{Simple Theory of Types} used by the Cambridge HOL system.
In both cases the families are generated by progressive refinements of the semantics which eliminate smaller interpretations.
In the proof theory these are effected by large cardinal axioms.
Given the standard semantics which consists primarily in demanding full power sets (in the set theory) or full function spaces (in the type theory) these axioms suffice to refine the semantics appropriately.

\chapter{Digging Deeper}

In this chapter I consider the credentials of the simpler of the two foundational systems, with the intention to examine how well defined is its denotational semantics and proof rules are, how solid are our reasons for believing that the proof rules are sound with respect to the semantics (that it proves only logical truths), and in what ways possible ambiguities might be reduced and certainty of soundness improved.

First order set theory is a first order language, a concept which has been central to much of the work in Mathematical Logic in the last century.
This means that the semantics and proof theory is well understood and many result have been established to the highest standards of rigour found generally in modern mathematics.
For the purpose of this synthesis there is very little added to the standard account.

The theoretical study of logical systems often treats semantics in a strictly analytic rather than a prescriptive manner.
By this I mean, that a formal language and its deductive system is addressed by determining the full range of \emph{models}, where a model is any interpretation of the language for which the logical system is sound, rather than expecting that the definition of the language should stipulate the intended subject matter (possibly by describing one or more intended interpretations).

If the semantics is analytic rather than prescriptive, then it renders any consistent logical system sound and complete.
The point of semantics in this case may well be to establish consistency, which is achieved by furnishing any model.

If the semantics is prescriptive then it is not only possible, but likely, that the deductive system will not be complete, since all but the simplest mathematical subject matter do not have decidable truth conditions, and cannot therefore have a complete axiomatisation in relation to which proofhood is decidable.

It is not usual to talk of first order languages in terms of denotational semantics, the study of semantics in this case is done using the language of \emph{model theory}.
Languages are distinguished by their vocabulary, and meaning is assigned to the names in the vocabulary by means of a structure called an interpretation, which consists of a domain of discourse (a set of values which constitute the range of the first order quantifiers) and an assignment to each name of a value defined over that domain of discourse.
The names in question are called constants, and represent either predicates which may take as values any subset of the domain of discourse which is the set of elements which are deemed to satisfy the predicate, n-ary relation symbols which denote sets of tuples of values from the domain of discourse, or functions which represent mappings from n-tuples to values drawn from the domain.

The standard account of the semantics of a first order language determines how the value of an expression in the language can be obtained for each interpretation of the non-logical constants in the language.
Since the expression may contain free variables, it is necessary to have a value for these variables before a value for the expression can be obtained.

A denotational semantics can be determined for a first order language by stipulating a particular set of interpretations as intended interpretations.

In first order logic with equality, the relation of equality is fixed and represents equality of pairs in the domain of discourse, and therefore does not appear in the structure defining an interpretation.
In first order set theory there is just one other relation symbol, which is the binary relation of membership.
The 



\chapter{Connecting the Families}

In this chapter the equivalence of the two families is argued.
In the first case this is done by reference to the various treatments of the semantics of CHOL in FOST.
This is supplemented by a discussion of the interpretation of OFST in CHOL using a strong axiom of infinity which in effect asserts that the cardinality of the 'individuals' is inaccessible.

Both families consist of a linearly ordered set of languages indexed by cardinal numbers which represent the minimum cardinality of the ontology.
For lower cardinalities, CHOL and FOST may not be equivalent (this depends on the detail of how these ideas are defined), but once we reach a sufficiently large cardinality they will be equivalent (i.e. bi-interpretable in the relevant sense).

For practical purposes, i.e. for the purposes of science (excepting some parts of set theory), engineering and commerce, large cardinals are not needed, the relevant mathematical structures being modest in size, and the heirarchy of languages becomes irrelevant.
This is evidenced in practice by the sufficiency of CHOL with the standard axiom of infinity (ensuring only a countable infinity of individuals) for engineering purposes.

\chapter{Building Higher}

In this chapter the way in which CHOL provides universal support for the class of languages with a well defined denotational semantics is discussed.

The idea here is that no injection between the language for the application and the CHOL language is necessary, because the use of the standard mechanisms for conservative enables the denotational semantics to be renedered in CHOL in a manner which results in the abstract syntax for CHOL containing the abstract syntax of the application language, and the semantics of CHOL with those extensions which define the denotational semantics of the target language will correctly render the meaning of expressions in that abstract syntax.

\chapter{Beyond the Limits}

The universalist conjectures which I have presented have their limits and in this chapter I mention some languages and deductive systems which either are technically not within the scope of these universal foundational families or which are based on philosophical ideas which are incompatible with our perspective.

When adequate formal systems for mathematics were first devised the philosophical attitudes toward these and other foundational matters fell into three main camps known as Logicism, Formalism and Intuitionism.
The hard core logicists were Gottlob Frege and Bertrand Russell, both of whom were also \emph{universalists}, holding that there should be just one universal conception of logic which is sufficiently rich to define all the concepts of mathematics and to formally derive mathematical theorems.
It is with Russell that type theories like Cambridge HOL originate, though Frege's logical system is generally regarded as constituting a second order logic, which prefigures the more elaborate type system contructed by Russell (and later simplified en-route to HOL).
The formalists lead by David Hilbert were linguistically pluralistic.

The story I have given above attempts a reconciliation of these two approaches to foundations, following the precedent of Rudolf Carnap in whose spirit this synthesis has been constructed.
It represents a compromise, proving a reconciliation of the pluralistic and universalistic foundational perspectives.

Intuitionism is more difficult to reconcile, because of its outright rejection of much of the mathematics of its time, most particularly of Cantorian set theory.
If intuitionism were merely a weaker logical system adequate only for a part of mathematics it might still reasonably be supported in the more full blooded system which I have proposed, but it is unlikely that its advocates would be content with any such accomodation.
Philosophical reservations (to say the least) would remain.

One reason for the philosophical incompatibility of intuitionism is its rejection of formal systems of any kind, which however was not replicated across the many logical systems which shared some of its concerns.
When formalisation was admitted (as later became normal), semantics remained prohibited, for truth was then, in many systems, identified with the existence of a constructive proof.
So the idea of rooting logical foundations in the concept of logical truth defined in terms of semantics remains unacceptable.

A technical reduction might nevertheless be effected through the identification of truth and proof, but its not clear that repairs the philosophical breach.

Intuitionism, spawned a broad range of logical systems which radically departed from the classical models, often with proponents without a uniform philosophical perspective, but with technical reasons for preferring the novel logical systems which emerged.
An important innovation in these constructive logical systems appeared in the work of Martin L\"of in the shape of a dependent type theory inspired by a correspondence between the types of terms in a type theory and the structure of propositions.

\chapter{Empirical Knowledge}

Concerning the representation and application of empirical knowledge.

\chapter{The limits of Language}

Every bit of information stored in an information processing system is represented by a material structure which involves very many submicroscopic parts, the nature of which is the subject of complex ongoing research in fundamental physics.
It is therefore inconceivable that we could ever have a complete description of the material universe.

It is not impossible that there might be a finite description of what kind of state the universe might be in, and of the ways in which the state of the universe progresses from one moment to the next, but contemporary physics must surely cast doubt upon that possibility too, notwithstanding the insistence of some physicists that a ``theory of everything'' is a real possibility.

This suggests that our knowledge of the physical universe of which we are a part, or even of any part of it, must be incomplete.

It is nevertheless possible to construct theories about, or models of the universe which prove of considerable value in progressing the prosperity and well-being of ourselves and those we care about.

Isaiah Berlin has characterised ``Enlightenment Thought'' as involving the belief that all truths are logically compatible, but the reality is that we routinely make us of ideas about the way the world works which are logically incompatible.
A clear example of that is the continued use of Euclidean space time and Newtonian physics after the discovery or invention of Einstein's theories of relativity.

\chapter{Abstract Models of Physical Reality}

\appendix

\chapter{Varieties of Epistemology}


The purpose of this work is to construct, present and advocate for a \emph{synthetic epistemology}.
Notwithstanding such prior usage of that concept which there may have been, what I intend in using the term is that the ``epistemology'' which I present has been devised as suitable for future application in a world in which the corpus of systematic knowledge which includes both scientific knowledge of the principles which govern the world (including the logic and mathematics which enables those principles to be articulated) and also an ever increasing body of detail about the the world governed by those principles, is maintained and progressed in a collaboration between human beings and intelligent artefacts, the latter facilitating precise language and formal reasoning which might otherwise be too onerous for mere humans.

Though that central purpose is synthetic, the synthesis is not a fantasy constructed \emph{in vacuo}, but a stage in a historical process which has been underway for billions of years.
Some examination of that historical progression is therefore attempted for two reasons, and this I present as ``natural epistemology'', which you can imagine, may be something like but not entirely the same as ``epistemology naturalised''.

\subsection{}

The purpose of this work is to present some ideas for the representation of all those kinds of knowledge which are amenable to deductive reason.
Let us say \emph{declarative knowledge}, that kind of knowledge which is found in true declarative sentences.
My aim is to articulate certain ideas about how large bodies of knowledge can be organised in such a way as to maximise our confidence in the truth of the propositions and to facilitate deductive reasoning for the purposes of elaborating and applying the knowledge.

These ideas are \emph{foundational} in more than one respect, the most fundamental of those being very closely related to some of the important work done in relation to the logical foundations of mathematics over the last 150 years.
The focus on foundations is complemented by attention to applications.
A single logical system is identified as the pivot through which the foundational underpinnings support the full range of applications in almost every aspect of our lives.

That pivotal system is the logical system which has been adopted by Cambridge HOL and some other interactive theorem provers.
That system is itself a logical foundation system for mathematics with well established credentials, within which almost all known matheamatics can be derived by machine supported and checked deductive inference from formal definitions of the relevant mathematical concepts.

Most foundational research has historically been conducted by scientists (and sometimes engineers) seeking to improve the clarity and rigour of their work in the face of difficulties arising from perceived defects in those areas.
  It is a process of working backwards from the applications in need of support toward some more solid and stable basis on which they can be constructed, and there is usually a good practical sense of when enough has been done, though sometimes foundational advances expose deeper difficulties which are then in need of attention.
  This is the kind of process which was seen in mathematics througout the 19th Century and eventually provoked a crisis at the turn of the 20th as a result of Russell exhibiting a paradox which vitiated Frege's foundational enterprise.

  A quite different foundational enterprise occurs in philosophy when philosophers look to foundations for a response to radical scepticism.  The philosophy in this context may not be well connected with the practice of science, and the expectations of foundations may be less easily satisfied.

To be certain of the truth of a sentence, we must first establish a definite meaning for it.
The meaning of a sentence depends upon the language in which it is expressed, and on the semantics of that language.

The distinction between true belief and knowledge plays very little role in this project, despite a deep concern with the levels of assurance with which truth can be established.

**************************


I am primarily concerned with making meaning precise and enabling truth to be establish with the highest level of assurance.

Insofar as this discussion can be construed as 
The conception of epistemology as concerned specifically with the theory of \emph{knowledge} is therefore 

Either:
\begin{itemize}
  \item
Before going further I should confess that, though I will often talk about knowledge, I regard that term in much the same way that an Engineer might think of the concepts `hot' and `cold', as poor substitutes for a temperature.
In the 'epistemology' here proposed, the semantics of declarative sentences is factored into two parts.
The first of these is an abstract semantics interpreting the sentence in relation to some abstract model, and the second an indication of the concrete structure which that abstract model is intended to replicate.
Knowledge in this system consists of formal logical truths about the abstract model (usually established by machine checked formal deductive proof) and more elaborate and mixed information about how that abstract model is applicable to the concrete world, rooted in a bijection between those elements of the model which are intended to represent physical entities with the entities they represent.
\item
By contrast with much epistemology, the focus is not on the word \emph{knowledge}, which plays only a minor expository role.
Instead the emphasis is on \emph{truth} and the confidence with which truth can be established (primarily in relation to \emph{a priori} knowledge), and on utility or applicability in relation to theories about, or models of, parts or aspects of the material universe.
It is thus oriented toward scientific knowledge represented in abstract structures, rather than addressing the kinds of everyday knowledge which people acquire and exploit in their everyday lives.
\end{itemize}
- or some amalgam!


********************************

Notwithstanding this emphasis on science and engineering, the synthesis is applicable to any domain sufficiently definite that deductive reason is in principle possible, and it is expected that any practical difficulties in its use in domains less comfortable with formal notations will be mitigated by the intermediation of Large Language Models or Artificial General Intelligence.

An epistemological synthesis must begin somewhere.
A major consideration for this synthesis is the establishment of clear meanings, a necessary condition for truth to be definitively established, and the provision of contexts in which deductive reasoning can be rigorous and substantially automated.
In such matters sceptical arguments based on regress both of definitions (of concepts and languages) and of justification demand an answer for which we offer a foundational perspective.
For this reason there is a logical order in which the ideas are best presented, withstanding the inevitable circularity incurred in the most fundamental parts (for the risks in which, mitigations are offered).

There are two orderings of interest here.
There is the ordering of presentation so as to best communicate to readers the ideas.
There is also, in a foundational presentation, a logical ordering of the substance.
This is the kind of order which is required in definitions to avoid their vitiation by circularity.
As well as the ordering of definitions, we will be concerned with an ordering of languages, in which more complex languages are defined in or by reference to simpler languages.

The avoidance of circularity is desirable, since it contributes to ensuring that meaning is clear and well defined, the maintenance of coherence and the avoidance of contradictions.

\part{Natural Epistemology}

\chapter{Prebiotic Epistemology}

\chapter{Biological Knowledge and Knowing}

\chapter{Culture and the Advent of Epistemological Innovation}

\subsection{Some History of the Vision}

The ideas addressed by this epistemological synthesis seek to progress conceptions of knowledge which have developed over the last two and a half millenia, beginning perhaps with Aristotle's conception of ``demonstrative science'', a conception of empirical science modelled on the axiomatic method which had been successful in mathematics, particularly in what we now know as Euclidean Geometry.

There follows a brief sketch of some of the key development leading up to the present synthesis to provide a backdrop against which the account of purpose and the key requirements which flow from that purpose may be more clearly understood.

\subsection{Gottfried Wilhelm Leibniz}

Leibniz (1646-1716) was a polymath of the early Enlightenment, innovating in Philosophy, Logic, Mathematics, Science and Engineering. 

Leibniz conceived of and attempted to design a \emph{lingua characteristica} (a language in which all knowledge could be formally expressed) and a \emph{calculus ratiocinator} (calculus of reasoning) such that when philosophers disagreed over some problem they could say 'calculemus' (let us calculate) and agree to formulate the problem in the lingua characteristica and solve it using the calculus ratiocinator.

\paragraph{The Lingua Characteristica}
Leibniz engaged in four kinds work related to his proposed lingua characteristica, (or universal characteristic).
\begin{itemize}
\item Encyclopaedia - he sought the collaborative development of an encyclopaedia in which would be presented in non-symbolic form all that was so far known. This was to provide a basis for the lingua characteristica in which the knowledge could be formally expressed. This enterprise was not completed, but beneficial side effects were the foundation of new academies and of the journal Acta Eruditorum.
\item Universal Language - he promoted the development of a language universal in the sense of being spoken by all. There have been many such projects of which the best known today is Esperanto
\item The lingua characteristica - a formal language universal both in being understood by all, and in encompassing all knowledge.
\item The calculus ratiocinator - a method of computing the truth value of a proposition in the lingua characteristica
\end{itemize}

  The lingua characteristica was to be a language in which predicates were numerically encoded in such a way as to render the truth of subject predicate proposition (and Leibniz considered all propositions to have this form) could be obtained by arithmetical computation.

\paragraph{The Calculus Ratiocinator}
This is roughly how he proposed to do it. He believed that every predicate was either simple or complex and that complex predicates could be analysed into simple constituents. He proposed to assign prime numbers to each simple predicate and then represent a complex predicate by the two products, one of the primes representing its simple constituents and another similarly representing the simple constituents which occurred negated.
Complex predicates are therefore thought to be invariably conjunctions of finite numbers of simple predicates or their negations.
He also believed (following Aristotle) that every proposition had subject-predicate form, and that in a true proposition the predicate was contained in the subject, i.e. the set of simple predicates from which the predicate was composed was a subset of the set from which the subject was composed. This can be sorted out by numerical computation, you just check whether the first part of the predicate divides that of the subject without remainder, and that the second part (the negated constituents) of the predicate divides the second part of the subject.

His main difficulty in this was in discovering what the simple predicates are. Leibniz thought the complete analysis beyond mere mortals, but believed that a sufficient analysis (into predicates which are relatively primitive) for the purposes of the calculus ratiocinator would be realisable.

We now know much more about logic and its scope, and can see that this scheme could not work except perhaps for very simple fragments of scientific language.
This is primarily down to the limitations of Aristotle's Syllogistic logic within which Leibniz was working, but also because of limitations on what can be done even with modern logic requiring some qualification to any modern reconception of that vision, such as that in which I am here engaged.

Leibniz has nevertheless been an inspiration to many other philosophers, logicians and computer scientists and engineers who have progressed aspects of his ambition in the centuries which followed.

For my present enterprise the aspiration to completely general way of representing knowledge amenable to deductive reason and suitable for gathering together a comprehensive body of precise knowledge in a manner conducive to mechanised deductive reason is a primary aim of the proposed synthesis.

In order to tease out in simpler terms some aspects of the big picture here, its interesting to see the perspective of a modern philosophical historian of ideas on some features of Western thought up to and in the Enlightenment.

\subsection{Isaiah Berlin}

Isaiah Berlin's take on The Enlightenment\cite{berlinRR} comes in two parts.
First, three legs upon which the whole Western tradition rested:
\begin{enumerate}
  \item All genuine questions have an answer.

    In principle, by someone.
\item  The answers are knowable.
\item All the answers are compatible.
\end{enumerate}

and then, the extra twist added by the Enlightenment:
\begin{quotation}
That the knowledge is not to be obtained by revelation, tradition, dogma, introspection..., only by the correct use of reason, deductive or inductive as appropriate to the subject matter.

This extends not only to the mathematical and natural sciences, but to all other matters including ethics, aesthetics and politics.
\end{quotation}
and... that virtue is knowledge.

This is a simple description of an unattainable ideal elements of which are important to this synthesis.
It will not be expected that all questions have an answer, for it is convenient sometimes to work with entities for which we have only incomplete descriptions, but it is an aspiration that any question definite enough to be amenable to deductive reasoning, either in its establishment or its application, can be accommodated within the synthesis.

There are now strong reasons to doubt that the answer to any properly formulated question can be discovered and established.
In many aspects of the proposed synthesis, absolutes are known to be unrealisable, and it is more important to be confident in the answers which do come than for such answers to be always forthcoming.

That all the answers be compatible is possibly the most crucial requirement in a system intended for large scale deductive elaboration, for in default of coherence, no result can be trusted.

\subsection{Hume and Kant}



\subsection{Frege}

\cite{frege1980}


\chapter{The Mechanisation of Logic}

\section{Meeting Cambridge HOL}

During the 1980s Mike Gordon and others at Cambridge University began research into the formal modelling and verification of digital hardware using software supporting the interactive development and checking of formal logical proofs.
This arose from prior work at Stanford and Edinburgh Universities on reasoning about computer software using a logic called ``Logic for Computable Functions'' (LCF) devised by the logician Dana Scott \cite{scott1993type}.
For the purpose of reasoning about hardware it was decided to adopt a Higher Order Logic instead of LCF.
After a number of adaptations, a logical system and a culture stabilised, based on a derivative of Church's ``Simple Theory of Types''\cite{churchSTT}.

This logical system is sufficiently powerful for the formal development of applicable mathematics, in a similar manner to that advocated by Frege and exemplified by Russell and Whitehead in their influential \emph{Principia Mathematica}\cite{russell10}.

The Fregean prescription (expressing his logicist thesis, contra Kant, that \emph{mathematics is logic}) was:

\begin{center}
  Mathematics = Logic + Definitions
\end{center}

To formally derive mathematics it suffices to begin with a formalisation of logic, add the definitions of the concepts of mathematics, and then derive in that logical system the theorems of mathematics.

Though Frege's prescription was specifically for mathematics, he considered his logical system to be of more general application.
Inspired by the ideas of Frege and Russell, Rudolf Carnap devoted his academic life to philosophically facilitating similar methods throughout science.
This was before the invention of the digital stored program computer, and the size and complexity of the formal proofs required would have been sufficient to make those ideas impractical for scientists.
The ideas, an important thread of the anti-metaphysical \emph{logical positivism} of the Vienna Circle, were even less palatable to philosophers.
In the hands of W.V.Quine a sceptical attack on Carnap's conception of logical truth, delivered in his ``Two Dogmas of Empiricism''\cite{quine53} (among other criticisms), served to de-throne positivism from the leading role in analytic philosophy which it might otherwise have occupied.

Despite the vicissitudes of Carnap's programme, when the Hardware Verification Group at Cambridge, with the advantage of computational support, looked to reason reliably about digital hardware, they advanced the programme pioneered by Frege, Russell, Carnap, Church and others.

Among those it is perhaps only Carnap who had attempted to address the problem of applying these logical methods in the empirical sciences.
He understood that Frege's prescription for the logical derivation of mathematics would not suffice for the empirical sciences.
Frege's prescription was specific to the \emph{a priori} sciences whose conclusions could be expected to be logically necessary.
These were understood to include mathematics, but not the empirical (\emph{a posteriori}) sciences.
For this reason Carnap had moved from the ``universalist'' stance of Frege and Russell, which had sought a single logical system for all deductive reasoning, to a linguistically (and ontologically) pluralistic regime, in which each application domain (each science or discipline) had its own logical system.
In these logical system principles which were factual rather than logical were adopted and results could be derived which were true results in the relevant empirical domain, but not logical truths.

It is not probable that Mike Gordon had any knowledge of these aspects of Carnap's philosophy when he sought to apply formal reasoning to digital hardware, but when he did do so these apparent difficulties in applying purely logical formal systems to reason about physical systems did not impede the adoption of pragmatic and sound ways of formally reasoning about hardware.

Philosophical qualms about the possibility of logically reasoning about physical systems did not pass unremarked by the engineers who adopted the methods, not only in Cambridge but across the world, as the resulting tools were adopted more widely.
Doubts about the status of logical proofs concerning the properties of physical systems moved from the research labs into the courts when claims about a commercial microprocessor whose design had been partially verified by these methods were contested in court.
Avra Cohn, a member of the Cambridge Hardware Verification Group (which had been contracted to contribute to the supposedly mis-described microprocessor verification), wrote a paper clarifying the issues: ``The notion of proof in hardware verification''\cite{cohnPIHV}.

Its worth looking a bit closer into the practice in the HVG and its relation to the difficulties which Carnap perceived in applying the new logical methods to the empirical sciences.

The Fregean prescription for the logicisation of mathematics required that mathematical concepts be introduced exclusively by \emph{definition}, and Frege was particular about what constituted a definition.
Using arbitrary axioms to characterise mathematical concepts or structures was not acceptable, primarily because of the risk it posed of compromising the logical consistency of the resulting theory.

Definitions are safe in this way, for they simply name some entity which in the existing system demonstrably has the required properties, rather than baldly asserting properties which are not already known to be realisable.
Because of this characteristic this kind of extension is called \emph{conservative}.

Definition only suffices in a logical system which is sufficently rich.
This is because definitions serve only to name something which already exists rather than to introduce something which was not previously in the domain of discourse.

The LCF system, which had previously been used for reasoning about software and which was adapted at first by Gordon for use in hardware verification, was not rich enough to work by conservative extension.
New concepts had to be introduced axiomatically, with the attendent risk of compromising consistency, which would become more severe as more complex systems were addressed.
When the group moved to work with a higher order logic\footnote{A fuller account of this story is given by Gordon in ``From LCF to HOL''\cite{gordon2000lcf}.} they fell in line with the Frege/Russell universalist conception of logic, and fully accepted the discipline of working exclusively with definitional  extensions, ensuring that all the resulting theorems were logical truths in that broad sense (of ``logical truth'') which was needed for mathematics to be assimilated into logic.

Notwithstanding this apparent turn to universalism, it is hard for anyone immersed in computer science to imagine that any one language can be sufficient, and the desire to support diverse notations and languages soon made itself apparent.
The universalist fundamental paradigm in practice became the use of a single logical system in which all other notations and languages could be interpreted, or ``embedded'' in the parlance which emerged (though without any explicit doctrinal underpinning).
An early illustration of techniques for interpreting other logical system in HOL may be found in \cite{gordon1989mechanizing}.
An account of a more elaborate example is given in Arthan's ``Z in HOL in ProofPower''\cite{arthan2005}.

The possibility of using HOL to support reasoning in other languages can said to arise from two features of the language.
The first is that the \emph{abstract syntax} of HOL is universal for a large class of languages, and the second is that the HOL language is sufficiently expressive that an abstract semantics of these languages can be rendered in HOL.
The concepts of abstract syntax and semantics arise from abstracting away from certain details of the syntax and the semantics, and from the possibility that they can be rendered entirely within the abstract ontology which is available in a purely abstract logical foundation such as HOL.

In \emph{abstract syntax} it is details of presentation which are discarded in favour of a simple representation of the structure of an expression or sentence which reflects the ways in which meanings of the elements of a syntactic category are compounded from the meanings of its constituents.

In \emph{abstract semantics} it is the details of how the expressions denote things in the real world (if the language does indeed do that), on the basis that the semantics has been rendered in the first instance in terms of an abstract model, a model in which all the entities involved are purely abstract entities.

With one further caveat we may then talk about HOL providing a language which is universal in practically important ways.
That caveat comes from two important results from mathematical logic and philosophy.
The first is the demonstration by Godel\cite{godel31a} of the incompleteness of arithmetic, and the second is Tarski's result on the arithmetic undefinability of arthmetic truth\cite{tarski31}.
The implication of these results is that no single logical system can be universal in its semantic expressiveness or complete in its deductive system.

We nevertheless conjecture that the HOL language and its deductive system can be indefinitely extended, both in its semantics and in its deductive system, and that it is universal in the sense that any other language (in a large class which will be defined) can be in practice reduced to a sufficiently strong version of HOL, sufficient strength being obtainable by progressively stronger axioms of infinity without other modification to the system.
HOL is in this sense universal, and it is also, I suggest (but cannot prove) practically complete in the sense that the probability of there being any practically applicable result which is not a theorem at close to the base level of this heirarchy.

Though the claim to universality is clearly refutable unless made for the heirarchy, the informal notion of \emph{practical universality} can probably reasonably be claimed for any single member of the heirarchy above a certain level.
This can be related to the default axiomatisation of first order set theory, generally regarded as a practically adequate foundation for mathematics.
HOL with a strong axiom of infinity which states or entails the existence of inaccessible ordinals.

This claim to practical universality extends beyond the provision of an adequate semantic and proof theoretic foundation, it includes a claim that the resulting support for languages is efficient.

The ideas presented here can be seen as providing a unification of the universalist and the pluralist attitudes to logical foundations.
Though we assert the existence of a universal foundation, we do not claim that it is unique.
I accept that there are many alternatives, but note that if they do have the required characteristics, then they will be equivalent.
I also believe that this provides a natural basis for the support of plurality of languages.
In fleshing out this account, I will abstract from the concept of language, completely divorcing it from concrete syntax, with the result that it becomes natural to identify a \emph{language} with a logical context which determines a vocabulary and its meaning, retaining the deductive system which extends that of the HOL system with the formal constraints which define the meaning of the vocabulary.


\footnote{Some references for future use:
\cite{arthan1991formal}
\cite{beeson2012foundations}
\cite{centrone2019reflections}
\cite{dzamonja2019}
\cite{gettier1963justified}
\cite{jones1992a,jones1992b}
\cite{kline1990mathematical1}
\cite{kline1990mathematical2}
\cite{kline1990mathematical3}
\cite{kumar2016self}
\cite{oliveira2006unifying}
\cite{shapiro1991foundations}
\cite{shapiroHPML}
\cite{tarski31}
\cite{tarski56}
}

\phantomsection
\addcontentsline{toc}{section}{Bibliography}
\bibliographystyle{rbjfmu}
\bibliography{rbj2}

\end{document}

% LocalWords:
