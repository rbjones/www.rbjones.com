% $Id: p036.tex $
% bibref{rbjp036} pdfname{p036}
\documentclass[10pt,titlepage]{article}
\usepackage{makeidx}
\newcommand{\ignore}[1]{}
\usepackage{graphicx}
\usepackage[unicode]{hyperref}
\pagestyle{plain}
\usepackage[paperwidth=5.25in,paperheight=8in,hmargin={0.75in,0.5in},vmargin={0.5in,0.5in},includehead,includefoot]{geometry}
\hypersetup{pdfauthor={Roger Bishop Jones}}
\hypersetup{pdftitle={Formal Abstract Modelling for General Artificial Intelligence}}
\hypersetup{colorlinks=true, urlcolor=red, citecolor=blue, filecolor=blue, linkcolor=blue}
%\usepackage{html}
\usepackage{paralist}
\usepackage{relsize}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{longtable}
\usepackage{url}
\newcommand{\hreg}[2]{\href{#1}{#2}\footnote{\url{#1}}}
\makeindex

\title{\bf{\LARGE Formal Abstract Modelling\\for\\General Artificial Intelligence}}
\author{Roger~Bishop~Jones}
\date{\small 2021/04/16}


\begin{document}

%\begin{abstract}
% A philosophical perspective on evolution, and an evolutionary approach to philosophy.
% 
%\end{abstract}
                               
\begin{titlepage}
\maketitle

%\vfill

%\begin{centering}

%{\footnotesize
%copyright\ Roger~Bishop~Jones;
%}%footnotesize

%\end{centering}

\end{titlepage}

\ \

\ignore{
\begin{centering}
{}
\end{centering}
}%ignore

\setcounter{tocdepth}{2}
{\parskip-0pt\tableofcontents}

%\listoffigures

\pagebreak

\section*{Preface}
\phantomsection

\addcontentsline{toc}{section}{Preface}

``General Artificial Intelligence'', as is now termed since the ``Artificial Intelligence'' lost its cachet, ought we hope, if not at first, to surpass humanity in the reliability of its memory and the certainty of its reasoning.
The latest wave of excellence fails miserably on both.

Large Language Models (LLMs) have certainly cracked language, which is, as some modern philosophers have pointed out, complicit in the incoherence of common discourse.
Can we now assume that when natural language discourse is required, an LLM technology will supply it, and seek deductive proficiency through simpler more precise notations, or relegate concrete syntax to intermediaries and approach deductive closure through exclusively abstract structures?

That is the possibility which this essay explores.

Despite their present ascendency, Large Language Models have not been the star achievers in recent AI R\&D.
The mantra that proficiency in AI is crucially dependent on the use of very large bodies of data for training neural nets has been disregarded in several of the most impressive achievements of Deep Mind the London AI startup now acquired by Google's parent Alphabet.

First with Chess, then Go and latterly alphafold, Deep Mind showed that in in simple worlds governed by a modest collection of unambiguous rules, neural nets relegated to heuristics could achieve proficiency as the system learns by playing itself, without needing role models to mimic.
Though their complexity is unbounded, the abstract structures of mathematics and their use in the more elaborate mathematical models created by science and the vastly more complex constructions of engineering (especially in digital semiconductors), are susceptible to the same methods, which place Monte-Carlo tree search in pole position with crucial subordinate roles for neural nets (which need no linguistic competence).

Thinking of deductive inference about abstract models as being a precise and simple game, it is merely a speculation on my part that it will be susceptible to similar methods as chess, go and protein folding, and the realisation of that aspiration is not something to which I can contribute, except by the attempt which I offer here to clarify the nature of the game.

There are many different ways in which a broadly universal game of deductive modelling can be framed, and what I offer here is just one of them.

I have been trying to tell this story for a long time, without success.
I don't know whether the story can be told without talking about the history of the ideas in which it appears, and, not being a historian I am likely to be scornful of my attempts to do that.
But i have attempted to tell the story without going into the history, and I cannot see how it can be understood.

The fact is that the ideas which I want to present under this heading are convincing to me only because of the history of ideas which has lead me to them, though my own particular perspective on them us substantially influenced by some of the most recent developments in them (by historical standards, not by the hectic standards of AI research) and by my own professional experience in trying to apply these ideas in the development of highly assured information processing systems in relation to security and safety concerns.

This essay is an attempted compromise, in which I try again to present the ideas in the context of a minimal presentation of those histotical precedent which seem most crucial in illuminating and motivating the approach which I am suggesting.

\section{Introduction}


Nearly half a century ago John McCarthy published a paper \cite{mccarthy1981} that divided the problem of artificial intelligence into two components or domains: an epistemological component, which concerned the representation of knowledge and the rules of reasoning, and a heuristic component that optimised search and pattern matching to permit the solution of  otherwise intractable problems.

In this essay I discuss a further division of the epistemological side of McCarthy's division.

McCarthy's ``epistemological part of AI'' can, I claim, profitably be advanced by separating our that part which concerns purely deductive reasoning.
In the established epistemological terminology, this is the problem of the \emph{a priori}.
The contention is that all deductive reasoning, even in relation to knowledge \emph{a posteriori} can be undertaken and validated in an abstract model with the standards of soundness and consistency which are realisable in that domain, and that an approach which separates out  abstract modelling as a fundamental shared component of all empirically applied AI is not only theoretically elegant but practically effective.

The separation of the whole of this domain yields a general facility for \emph{abstract modelling}, in which any aspect of empirical concern can be addressed by the use of an abstract model.
This is a particular way of thinking about and implementing the use of formalised mathematics and logic for modelling the phenomena in the real world, and thus a critical element of the problem of knowledge representation, and a means to very high levels of rigour and generality in the automation of deductive reasoning.

\section{A First Sketch of The Foundation}

The formalisation and mechanisation of logic and thence of mathematics was anticipated by Aristotle and Leibniz, but only became plausible after Frege devised his \emph{concept script} and attempted to demonstrate the reduction of mathematics to logic.
His own system was shown to be flawed by ``Russell's paradox'', but Russell then went on to devise and apply a type theory for that same purpose.

Frege's mantra was that:

\begin{center}
Mathematics = Logic + Definitions
\end{center}

Which has fallen foul of much philosophical disagreement about exactly logic is, but it was generally accepted that \emph{in practice}:

\begin{center}
Mathematics = Set-theory + Definitions
\end{center}

Where \emph{in practice} means that almost all of mathematics can be derived in the axiomatisation of set theory known as ZFC provided that definitions of the concepts of mathematics in terms of sets are accepted, and that such definitions can be supplied which render the resulting formal theory strictly the same as accepted but less formally conducted mathematics.
We have to say \emph{almost} all here because of the well known incompleteness results of G\"{o}del, but I have been unable to locate any incompleteness which is relevant to any but the most esoteric studies in logic or mathematics.

Henceforth any logical system (and there are many of them) which is capable of formalising logic and mathematics in a similar way I call a \emph{logical foundation system} (often eliding the `logical'), and I note here the importance of the possibility of proceeding in such a foundation system exclusively by the use of definitions of concepts rather than by the introduction of additional axioms or rules of inference as crucial to the integrity of the system, ensuring as it does, that if the foundation system itself is logically consistent the so will be the extensions for modelling purposes undertakem exclusively by definition.\footnote{
Though later we will admit a useful generalisation of the concept of definition as that of \emph{conservative extension}.}

Russell's \emph{Theory of Types} was not the most elegant foundation system, and it was not long before Ramsey suggested its simplification into the \emph{Simple Theory of Types}.
\footnote{First published as a fully worked out formal system in 1928 by Hilbert and Ackermann "\cite{hilbert1928}.}

In the 1930's logicians found it necessary to clarify the concept of \emph{effective method}, and in 1936 four simple but general ways of describing computational methods (algorithms) were published.
These were Stephen Kleene's ``recursive functions'', Alan Turings's machines, Post's production systems, and Church's lambda calculus.
\footnote{The relevant papers may all be found in ``The Undecidable'' \cite{davis65}}
These originally devised to demonstrate the existence of unsolvable problems, but Church's lambda calculus was to prove influential in the design of computer programming languages and as the basis of foundations for mathematics which through their simplicity and power would be embraced by those computer scientists and engineers who were interested in reasoning about computer programs and digital hardware.

Closely related to the lambda calculus, in which functions are defined using a variable to represent the argument, and an expression involving that variable for the value of the function at that point, an even simpler but similarly expressive notation was devised which made no use of these ``bound variables'', and there was much research in how these systems might form the basis for mathematical foundation systems.

A major figure in this research was Haskell Curry, whose names were to be enshrined in the terminolgy of functional programming languages (which are closely related to the lambda-calculus), but his work on mathematical foundation systems based on combinators was to prove less influential.
For our story it is the ideas of Church which are more significant, and after proposing a foundation system based on the simple (type-free) lambda-calculus which proved to be logically inconsistent, he added to the lambda-calculus a type system similar to that of the Simple Theory of Types resulting in a variant of that system which would prove influential in computer science.

Church's formulation of the Simple Theory of Types \cite{churchSTT} was to be taken up in research into the formal verification of computer programs and digital hardware.
The resulting elaboration of Church's system was the product of a line of development which began with a logical system devised by Dana Scott for reasoning about programs and programming languages
\footnote{First circulated in 1969 but not published until much later as \cite{scott1993type}}.
This became the logical basis for a line of software systems for computer assisted proof construction and checking intended for reasoning about programming languages and programmes.
The first system was called LCF for ``Logic for Computable Functions'' and after several steps, including migration from Stanford to Edinburgh and then Cambridge (UK) was re-oriented toward hardware verification and adapted to support a variant of Church's STT by Mike Gordon.%
\footnote{For a fuller account of this story see Gordon\cite{gordon2000lcf,plotkin2000proof}}

\cite{gordon1989mechanizing,birtwistle2012current}

\section{A History of Modelling}
\subsection{What is a Model?}

The aim of this document is to talk about formal abstract models, since it seems possible that they will be the next important development in the evolution of models and their use.

In order to give a sense of the importance of such models I aim to sketch the history behind them.
This might involve a bit of stretching of the term, but it already serves a great variety of purposes so I'm sure it will cope.

It may be useful to characterise the sort of model which are of interest here, which are those which consist in something representing information about some other thing of interest.
This encompasses linguistic descriptions as well as information strucures which long pre-date the evolution of language.

Since the beginning of life on earth, even the most simple forms of life have found it necessary to sense their environment, and to adapt their behaviour according to what those senses reveal.
In the simplest cases, the senses will serve to detect potential foods and suspected predators, in order to secure the former and avoid the latter.

This involves some kind of representation of the environment being created by the sense organs, to be transformed into that kind of representation of necessary effect which will secure the best interests of the organism.
In those organisms which possess a nervous system the representation may be as patterns of firing in neurons, and the transformation will be undertaken by a neural net before some other pattern of firings initiates appropriate motor functions.
In yet more primitive organisms, the necessary presentations and processes are likely to be chemical, and the representations less likely to be thought of as models even though serving similar purposes.

\subsection{}

My aim here is to describe a game, which I believe to be susceptible to automation, but which because of its algorithmic complexity depends upon the use of heuristics best supplied by intensively trained neural nets.

The game is the prediction of real world events by the use deduction in the context of abstract models of the relevant real world phenomena.
It also includes the elaboration of the pre-requisite logical and mathematical theories which provide a basis for the construction of the relevant scientific and engineering models.
The whole provides potentially a substrate for the automation of engineering design.

A bare presentation of the game as proposed would I expect be unconvincing and opaque, so I propose to come to the details via some discussion of the history of abstract modelling, or of those kinds of process which have lead the particular conception of abstract modelleing which is presented here.

This leads to a minimal presentation, followed by the consideration of a certain number of desirable elaborations.
Throughout, an understanding will be supported by the presentation of certain philosophical positions, which are offered primarily as useful in supporting this kind of enterprise (i.e. on a pragmatic basis) not as demonstrably superior to the many alternatives.

\section{Some History of Modelling}


\subsection{Pre-History}

Throughout the evolution of life on earth it has been necessary for living organisms to take cognisance of their surroundings in order to take those actions which will secure nourishment, avoid predators, and effect reproduction.
When an organism takes in information about its surroundings and uses it as a basis for its actions in the near future, it is not wholly unreasonable to consider that it has thereby acquired some sort of useful model of its environment, and that is will use that model to predict which courses of action will best serve its need to grow and replicate.
For organisms with a nervoud system we might call these neural models, for single celled organisms perhaps chemical ``models'' (stretching the point beyond its limits perhaps).

As the nervous systems of organisms evolve to greater complexity we may imagine that the complexity of the models grows with that of the nervous systems.

Though these models serve a vital purpose for the organisms, we cannot reasonably say at this point that the models are used deductively.

Deduction can only be considered in the context of \emph{propositional language}, which is that kind of language in which claims can be expressed which will be either true or false according to how things are, and whose assertion may thereby communicate knowledge (about how things are) from one to another.
Such language is probably coeval with homo sapiens, who appears at the end of a million years in which the size of the brain grew quite rapidly.
Any faculty which demands special mental faculties must evolve alongside both the physical and mental adaptations necessary to support it.
Those regions of the brain which are adapted to support language could not have appeared without the languages faculties they support.

With propositional language comes deduction, for even the most elementary language must embrace taxonomic classifications.
If your language has a word for horse and one for animal, then you cannot be said to understand the language unless you know that horses are a kind of animal and are able to deduce from the knowledge that something is a horse the ``conclusion'' that it is also an animal.
Admittedly this kind of deduction is neither formal nor self-conscious.

The advent of language enabled oral culture which would ultimately empower mankind in the transformation of the world.
The evolution of culture must at first have been quite slow, since the first evidence of animal husbandry and then agriculture date back only 13,000 years, 200-300 thousand years after the evolution of homo sapiens.

The first stage in accelerating cultural evolution was the invention of the written word, which, in the form of clay tablets, was probably of modest impact, but the technology of writing and communication has itself evolved and many important cultural advances have been facilitated by new media for storing, and sharing knowledge.

Papyrus was invented by the Egyptians around 3000 BC, and was a medium for the preservation of Egyptian culture, including geometry and arithmetic.
It does not appear to have been adopted in ancient Greece until about the 8th century BC, but then catalysed the flourishing of Greek culture which then shaped the development of the Western and ultimately the whole world.
This began with literary works such as the Odyssey, but of greater interest to us here is the transformation of Mathematics which began around 600BC.

\subsection{Greek Mathematics and Logic}

The period between 600 and 300BC was one in which major advances were made in sophistication with which reason could be applied to modelling the world.

It is this period that mathematics is first systematised as a deductive science, and in which we first see deduction talked and theorised about.
The systematisation of Geometry now known as Euclidean Geometry set a new standard of rigour allowing more elaborate deductive reasoning to be undertaken with great reliability.
This standard was not to be improved upon for the next two millenia, and was to be the envy many philosophers who wished for the emprimateur of logical certainty to be attached to their conclusions.

\subsection{The Organon and Demonstrative Science}

It is in Aristotle's ``Organon'' \cite{aristotleL325,aristotleL391} that the applicability of logic to the understanding of reality was first to be made explicit, and it was in Aristotle that we see the first attempts at formally codifying the rules of sound deductive reasoning.

In his Organon, six books forming his contribution to logic, Aristotle presents the idea of Demonstrative Science in which necessary conlcusions about the world are deductively derived from the fundamental principles of each science.
In these volumes the nature of deductive reason is analysed using the idea of the syllogism.
This is the first known metatheory concerning deduction, and it was to remain dominant until the end of the 19th century.

\subsection{Bacon's Novum Organum and Empiricist Science}

The next major development in the methodology of science, the ways in which we build models of the world, came in the Renaissance and is presented in writings of Francis Bacon, notably his \emph{Novum Organum} \cite{bacon2017novum}.

Whereas Aristotle sought to adapt the axiomatic methods of mathematics to Science, looking in greater depth at the process of deriving conclusions from the principles than that of establishing the principles, Francis Bacon was more concerned with how the principles could be established, and in this the differences between mathematics and science wer more marked.

He described a four stage process as follows:

\begin{itemize}
\item Collect data about the phenema of interest.
\item Look for patterns in the data.
\item Formulate a hypothesis.
\item Test the hypothesis by observation and experiment.
\end{itemize}

This is the new nomologico-deductive method, in which the novelty is primarily in the establishment of the laws rather than their application (though testing hypotheses is similar to application, as involving deductive inference from the hypotheses).

Almost contemporary with Bacon, and with more substantial credentials as a scientist and engineer (as well as a philosopher), Galileo Galilei (1564–1642) spent most of his life trying to give structure to the new science, in which Aristotelian science was largely cast aside.
Gone was the teleological emphasis on ``final cause'', and replace the five fold ontology of ether, fire, air, water and earth with a system built exclusively on matter, introducing a mechanical system which came to dominate modern science.

Modern philosophy from this point divides into two camps, empiricist like Bacon who emphasised observation and experiment as the source of worldly knowledge, and those who sought knowledge through reason.

\subsection{Leibniz and The Mechanisation of Science}

\subsection{Hume's Forks and The Synthetic A Priori}


\phantomsection
\addcontentsline{toc}{section}{Bibliography}
\bibliographystyle{rbjfmu}
\bibliography{rbj2}

%\addcontentsline{toc}{section}{Index}\label{index}
%{\twocolumn[]
%{\small\printindex}}

%\vfill

%\tiny{
%Started 2023/07/21


%\href{http://www.rbjones.com/rbjpub/www/papers/p032.pdf}{http://www.rbjones.com/rbjpub/www/papers/p036.pdf}

%}%tiny

\end{document}

% LocalWords:
