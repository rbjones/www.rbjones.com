% $Id: p036.tex $
% bibref{rbjp036} pdfname{p036}
\documentclass[10pt,titlepage]{book}
\usepackage{makeidx}
\newcommand{\ignore}[1]{}
\usepackage{graphicx}
\usepackage[unicode]{hyperref}
\pagestyle{plain}
\usepackage[paperwidth=5.25in,paperheight=8in,hmargin={0.75in,0.5in},vmargin={0.5in,0.5in},includehead,includefoot]{geometry}
\hypersetup{pdfauthor={Roger Bishop Jones}}
\hypersetup{pdftitle={An Epistemological Synthesis}}
\hypersetup{colorlinks=true, urlcolor=red, citecolor=blue, filecolor=blue, linkcolor=blue}
%\usepackage{html}
\usepackage{paralist}
\usepackage{relsize}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{longtable}
\usepackage{url}
\newcommand{\hreg}[2]{\href{#1}{#2}\footnote{\url{#1}}}
\makeindex

\title{\bf An Epistemological Synthesis}
\author{Roger~Bishop~Jones}
\date{\small 2023/10/16}


\begin{document}

%\begin{abstract}
% An epistemology constructed for the advancement of science and technology in the age of interstellar hybrid (human, synthetic, post human) intelligence.
% 
%\end{abstract}
                               
\begin{titlepage}
\maketitle

%\vfill

%\begin{centering}

%{\footnotesize
%copyright\ Roger~Bishop~Jones;
%}%footnotesize

%\end{centering}

\end{titlepage}

\ \

\ignore{
\begin{centering}
{}
\end{centering}
}%ignore

\setcounter{tocdepth}{2}
{\parskip-0pt\tableofcontents}

%\listoffigures

\pagebreak

%\addcontentsline{toc}{section}{Index}\label{index}
%{\twocolumn[]
%{\small\printindex}}

%\vfill

%\tiny{
%Started 2023/07/21


%\href{http://www.rbjones.com/rbjpub/www/papers/p032.pdf}{http://www.rbjones.com/rbjpub/www/papers/p036.pdf}

%}%tiny


\chapter{Introduction}

We are in interesting times, in many different respects.

The phenomenon of greatest interest in provoking the ideas presented here is the imminence of intelligent machines, and the possibility that these machines will enable a transformation in the ways in which knowledge is gathered and applied.

I was born into a world which anticipated machine intelligence, but had a limited understanding of how that might be achieved and machines which fell short of the brute compute power which would be needed. 
My own interest in the problem dates back to my years as an undergraduate (though it was not a subject which undergraduates could then study).
It was then that I first learned a little of the work of Alan Turing, and the word ``Turing'', written in biro on my jeans, provoked my tutor into reminiscences from his own personal acquaintance with that famous logician and philosopher of AI.

That first spell as an undergraduate student of engineering proved short lived, and years later I returned to academic studies to obtain a BA in mathematics and philosophy.
My taking up of philosophy was a surprise to me.
Philosophy struck a deep chord, and gave me a sense of vocation not diminished by my perception that I was completely unsuited to progressing an academic career in philosophy.
This was the beginning of a lifetime of philosophical rumination, and a more or less continuous search for a way to turn those ruminations into substance.

My interests in formal logic, computer languages, artificial intelligence and under it all, philosophy, all date from that time.
They coloured my thinking many years later when I had the opportunity to work with the Cambridge HOL system, then recently developed by Mike Gordon and his embryonic Hardware Verification Group at Cambridge University.

The epistemological synthesis which I present below was seeded by the perception that the Cambridge HOL system possessed many of the desirable characteristics of a representation system for knowledge suitable for use in the kind Leibnizian project for the mechanisation of scientific knowledge which the advent of machine intelligence demanded and would facilitate.

The synthesis is \emph{foundational} in multiple senses, and appropriately for an epistemological enterprise, it digs deep in its response to the kind of academic scepticism to which it is inevitably subject.
Though not so intricate as G.E.Moore's defence of common sense\cite{moore1925,moore1993}, it is in its way similarly grounded, not in the good sense of the common man, but in that of the practical engineer.
Because of its inspiration in (rather exotic) engineering practices, my account begins there, before before starting afresh from a philosophical ground zero.

\section{Meeting Cambridge HOL}

During the 1980s Mike Gordon and others at Cambridge University began research into the formal modelling and verification of digital hardware using software supporting the interactive development and checking of formal logical proofs.
This arose from prior work at Stanford and Edinburgh Universities on reasoning about computer software using a logic called ``Logic for Computable Functions'' (LCF) devised by the logician Dana Scott \cite{scott1993type}.
For the purpose of reasoning about hardware it was decided to adopt a Higher Order Logic instead of LCF.
After a number of adaptations, a logical system and a culture stabilised, based on a derivative of Church's ``Simple Theory of Types''\cite{churchSTT}.

This logical system is sufficiently powerful for the formal development of applicable mathematics, in a similar manner to that advocated by Frege and exemplified by Russell and Whitehead in their influential \emph{Principia Mathematica}\cite{russell10}.

The Fregean prescription (expressing his logicist thesis, contra Kant, that \emph{mathematics is logic}) was:

\begin{center}
  Mathematics = Logic + Definitions
\end{center}

To formally derive mathematics it suffices to begin with a formalisation of logic, add the definitions of the concepts of mathematics, and then derive in that logical system the theorems of mathematics.

Though Frege's prescription was specifically for mathematics, he considered his logical system to be of more general application.
Inspired by the ideas of Frege and Russell, Rudolf Carnap devoted his academic life to philosophically facilitating similar methods throughout science.
This was before the invention of the digital stored program computer, and the size and complexity of the formal proofs required would have been sufficient to make those ideas impractical for scientists.
The ideas, an important thread of the anti-metaphysical \emph{logical positivism} of the Vienna Circle, were even less palatable to philosophers.
In the hands of W.V.Quine a sceptical attack on Carnap's conception of logical truth, delivered in his ``Two Dogmas of Empiricism''\cite{quine53} (among other criticisms), served to de-throne positivism from the leading role in analytic philosophy which it might otherwise have occupied.

Despite the vicissitudes of Carnap's programme, when the Hardware Verification Group at Cambridge, with the advantage of computational support, looked to reason reliably about digital hardware, they advanced the programme pioneered by Frege, Russell, Carnap, Church and others.

Among those it is perhaps only Carnap who had attempted to address the problem of applying these logical methods in the empirical sciences.
He understood that Frege's prescription for the logical derivation of mathematics would not suffice for the empirical sciences.
Frege's prescription was specific to the \emph{a priori} sciences whose conclusions could be expected to be logically necessary.
These were understood to include mathematics, but not the empirical (\emph{a posteriori}) sciences.
For this reason Carnap had moved from the ``universalist'' stance of Frege and Russell, which had sought a single logical system for all deductive reasoning, to a linguistically (and ontologically) pluralistic regime, in which each application domain (each science or discipline) had its own logical system.
In these logical system principles which were factual rather than logical were adopted and results could be derived which were true results in the relevant empirical domain, but not logical truths.

It is not probable that Mike Gordon had any knowledge of these aspects of Carnap's philosophy when he sought to apply formal reasoning to digital hardware, but when he did do so these apparent difficulties in applying purely logical formal systems to reason about physical systems did not impede the adoption of pragmatic and sound ways of formally reasoning about hardware.

Philosophical qualms about the possibility of logically reasoning about physical systems did not pass unremarked by the engineers who adopted the methods, not only in Cambridge but across the world, as the resulting tools were adopted more widely.
Doubts about the status of logical proofs concerning the properties of physical systems moved from the research labs into the courts when claims about a commercial microprocessor whose design had been partially verified by these methods were contested in court.
Avra Cohn, a member of the Cambridge Hardware Verification Group (which had been contracted to contribute to the supposedly mis-described microprocessor verification), wrote a paper clarifying the issues: ``The notion of proof in hardware verification''\cite{cohnPIHV}.

Its worth looking a bit closer into the practice in the HVG and its relation to the difficulties which Carnap perceived in applying the new logical methods to the empirical sciences.

The Fregean prescription for the logicisation of mathematics required that mathematical concepts be introduced exclusively by \emph{definition}, and Frege was particular about what constituted a definition.
Using arbitrary axioms to characterise mathematical concepts or structures was not acceptable, primarily because of the risk it posed of compromising the logical consistency of the resulting theory.

Definitions are safe in this way, for they simply name some entity which in the existing system demonstrably has the required properties, rather than baldly asserting properties which are not already known to be realisable.
Because of this characteristic this kind of extension is called \emph{conservative}.

Definition only suffices in a logical system which is sufficently rich.
This is because definitions serve only to name something which already exists rather than to introduce something which was not previously in the domain of discourse.

The LCF system, which had previously been used for reasoning about software and which was adapted at first by Gordon for use in hardware verification, was not rich enough to work by conservative extension.
New concepts had to be introduced axiomatically, with the attendent risk of compromising consistency, which would become more severe as more complex systems were addressed.
When the group moved to work with a higher order logic\footnote{A fuller account of this story is given by Gordon in ``From LCF to HOL''\cite{gordon2000lcf}.} they fell in line with the Frege/Russell universalist conception of logic, and fully accepted the discipline of working exclusively with definitional  extensions, ensuring that all the resulting theorems were logical truths in that broad sense (of ``logical truth'') which was needed for mathematics to be assimilated into logic.

Notwithstanding this apparent turn to universalism, it is hard for anyone immersed in computer science to imagine that any one language can be sufficient, and the desire to support diverse notations and languages soon made itself apparent.
The universalist fundamental paradigm in practice became the use of a single logical system in which all other notations and languages could be interpreted, or ``embedded'' in the parlance which emerged (though without any explicit doctrinal underpinning).
An early illustration of techniques for interpreting other logical system in HOL may be found in \cite{gordon1989mechanizing}.
An account of a more elaborate example is given in Arthan's ``Z in HOL in ProofPower''\cite{arthan2005}.

The possibility of using HOL to support reasoning in other languages can said to arise from two features of the language.
The first is that the \emph{abstract syntax} of HOL is universal for a large class of languages, and the second is that the HOL language is sufficiently expressive that an abstract semantics of these languages can be rendered in HOL.
The concepts of abstract syntax and semantics arise from abstracting away from certain details of the syntax and the semantics, and from the possibility that they can be rendered entirely within the abstract ontology which is available in a purely abstract logical foundation such as HOL.

In \emph{abstract syntax} it is details of presentation which are discarded in favour of a simple representation of the structure of an expression or sentence which reflects the ways in which meanings of the elements of a syntactic category are compounded from the meanings of its constituents.

In \emph{abstract semantics} it is the details of how the expressions denote things in the real world (if the language does indeed do that), on the basis that the semantics has been rendered in the first instance in terms of an abstract model, a model in which all the entities involved are purely abstract entities.

With one further caveat we may then talk about HOL providing a language which is universal in practically important ways.
That caveat comes from two important results from mathematical logic and philosophy.
The first is the demonstration by Godel\cite{godel31a} of the incompleteness of arithmetic, and the second is Tarski's result on the arithmetic undefinability of arthmetic truth\cite{tarski31}.
The implication of these results is that no single logical system can be universal in its semantic expressiveness or complete in its deductive system.

We nevertheless conjecture that the HOL language and its deductive system can be indefinitely extended, both in its semantics and in its deductive system, and that it is universal in the sense that any other language (in a large class which will be defined) can be in practice reduced to a sufficiently strong version of HOL, sufficient strength being obtainable by progressively stronger axioms of infinity without other modification to the system.
HOL is in this sense universal, and it is also, I suggest (but cannot prove) practically complete in the sense that the probability of there being any practically applicable result which is not a theorem at close to the base level of this heirarchy.

Though the claim to universality is clearly refutable unless made for the heirarchy, the informal notion of \emph{practical universality} can probably reasonably be claimed for any single member of the heirarchy above a certain level.
This can be related to the default axiomatisation of first order set theory, generally regarded as a practically adequate foundation for mathematics.
HOL with a strong axiom of infinity which states or entails the existence of inaccessible ordinals.

This claim to practical universality extends beyond the provision of an adequate semantic and proof theoretic foundation, it includes a claim that the resulting support for languages is efficient.

The ideas presented here can be seen as providing a unification of the universalist and the pluralist attitudes to logical foundations.
Though we assert the existence of a universal foundation, we do not claim that it is unique.
I accept that there are many alternatives, but note that if they do have the required characteristics, then they will be equivalent.
I also believe that this provides a natural basis for the support of plurality of languages.
In fleshing out this account, I will abstract from the concept of language, completely divorcing it from concrete syntax, with the result that it becomes natural to identify a \emph{language} with a logical context which determines a vocabulary and its meaning, retaining the deductive system which extends that of the HOL system with the formal constraints which define the meaning of the vocabulary.

\section{Purpose and Structure}

The purpose of this work is to present some ideas for the representation of all those kinds of knowledge which are amenable to deductive reason.
Let us say \emph{declarative knowledge}, that kind of knowledge which is found in true declarative sentences.

By contrast with much epistemology, the focus is not on the word \emph{knowledge}, which plays only a minor expository role.
Instead the emphasis is on \emph{truth} and the confidence with which truth can be established (primarily in relation to \emph{a priori} knowledge), and on utility or applicability in relation to theories about, or models of, parts or aspects of the material universe.
It is thus oriented toward scientific knowledge represented in abstract structures, rather than addressing the kinds of everyday knowledge which people acquire and exploit in their everyday lives.

Notwithstanding this emphasis on science and engineering, the synthesis is applicable to any domain sufficiently definite that deductive reason is in principle possible, and it is expected that any practical difficulties in its use in domains less comfortable with formal notations will be mitigated by the intermediation of Large Language Models or Artificial General Intelligence.

An epistemological synthesis must begin somewhere.
A major consideration for this synthesis is the establishment of clear meanings, a necessary condition for truth to be definitively established, and the provision of contexts in which deductive reasoning can be rigorous and substantially automated.
In such matters sceptical arguments based on regress both of definitions (of concepts and languages) and of justification demand an answer for which we offer a foundational perspective.
For this reason there is a logical order in which the ideas are best presented, withstanding the inevitable circularity incurred in the most fundamental parts (for the risks in which, mitigations are offered).

There are two orderings of interest here.
There is the ordering of presentation so as to best communicate to readers the ideas.
There is also, in a foundational presentation, a logical ordering of the substance.
This is the kind of order which is required in definitions to avoid their vitiation by circularity.
As well as the ordering of definitions, we will be concerned with an ordering of languages, in which more complex languages are defined in or by reference to simpler languages.

The avoidance of circularity is desirable, since it contributes to ensuring that meaning is clear and well defined, the maintenance of coherence and the avoidance of contradictions.

\part{A Priori Knowledge}

This part is devoted to defining certain concepts culminating in the idea of a \emph{Universal Foundation for Logical Truth} and to two such foundation systems, first order set theory and the Cambridge HOL logic, which are shown to be equivalent as universal foundations.

As normally presented these logical systems are not equivalent.
There are more logical truths (in the sense defined below) expressible in the standard interpretation of the ZFC axioms for set theory than in the Cambridge HOL logic.
But the notion of Universal Foundation we expound consists in an unbounded sequence of increasingly expressive systems distinguished primarily by the cardinality of their minimal ontologies, i.e. in how many things they presume there to be.
The claim to equivalence is the claim that every logical truth which is expressible in some member of one of the the family is also expressible in a member of the other family.

It is a hypothesis that there are no more expressive universal foundation systems, which is treated in a manner similar to Church's thesis.
Rather than being proven, it survives until some convincing counterexample may be found.
There is of course a difficulty in any logical system which abjures semantics, as to whether an interpretation in set theory can be correct.
Formally there is little doubt that in any system formally consistent, if only in the sense of Post, can be given an interpretation in these universal foundations which is based on identifying truth with provability, but whether this would be philosophically acceptable to those who prefer constructive foundations is moot.

In defining these ideas an important element is the choice of reduction, particularly in relation to semantics and hence logical truth.
Two formal languages are considered equivalent if they a mutually interpretable by functions which preserve meaning, so a relevant notion of meaning must be articulated.

Once the definitions are complete there are two directions of further discussion, which flow from the distinct roles envisaged for the two Universal Foundations which have been chosen to exemplify the concept.

The Cambridge HOL logic is chosen because there is a natural way in which we can define the abstract semantics of some arbitrary language and then in practice be able to reason with that language in the resulting context in HOL.
This is a consequence of the abstract syntax of the simply typed lambda calculus being effectively a universal abstract syntax (aided and abetted by the ability to define new type constructors which correspond to syntactic categories in the required abstract syntax).

First order set theory on the other hand, is more convenient for establishing the semantics and proof rules of these universal foundations.
This helps in addressing sceptical arguments from regress in relation to both meaning and proof in the foundations.
In addressing the regress in meaning and justification, careful but ultimately circular arguments prevail.

\chapter{First Philosophy}

\section{Preliminary Discussion}

The term ``First Philosophy'' comes from Aristotle, who used it to describe the matter of the volume which later editors named ``Metaphysics'' on the basis of no more compelling case than that it appeared after the volume on Physics.

I use it here loosely, rather than in close correspondence with Aristotle's Metaphysics, because it is suggestive that philosophy must begin somewhere, and hopefully progress onward, rather than being a amorphous mass incapable of orderly presentation.

As well as being appropriately presented in a progressive way, my conception of ``First Philosophy'' includes the idea that some aspects of philosophy are also logically prior to others, and indeed, that a presentation which constitutes a logical progression is desirable (in a primordial sense of ``logical'' which is mere suggestion prior to any conception of what logic might be).

It is furthermore, here coupled with notions of foundation, and thus with something like the kind of doctrine which has been called ``foundationalism'', but which lacks some of the absolutism which is commonly associated with that term, and which makes such foundations susceptible to radical scepticism.

I do not seek an epistemology which looks for absolute certainty, but rather for one which may be an effective basis for the future prosperity and well being of humanity.

If I were to launch immediately into the most fundamental elements of this supposed logical progression, the reader might be left in a state of suspense as to how the various sceptical arguments which might be marshalled against it could be answered.
This preliminary discussion is intended to mitigate the suspense.

One modern response to various scepticisms may be found in G.E.Moore's ``A Defence of Common Sense'' \cite{moore1925,moore1993}.

\subsection{Some History of Foundations}

In contrast with Philosophy, Mathematics is that domain of intellectual endeavour which has the very highest reputation for clarity and certainty.
Since the transformation of Mathematics into a theoretical discipline (rather than a collection of practical methods), realised by deductive reasoning, its reputation has been the envy of many philosophers and has inspired a number of attempts to usurp its deductive imprimatur for the sake of methods and doctrines which fall short of deductive rigour.

When we look under the hood we find that reputation has been achieved by ideas which are broadly (but sometimes explicitly) \emph{foundational}, that there have been wide variations in the rigour of the proceedings and that, arguably, the more advanced mathematics has become, the more it has depended upon clearly articulated and elaborately constructed foundations.

The point of this section is to suggest that the interest of mathematicians and some branches of empirical science and engineering to foundational issues is distinct from that of philosophers who may take foundational studies as intent on achieving absolute precision of meaning and certainty of proof, making in the process, a strawman readily refuted.
There may be an analogy here between Moore's rejection of sceptical arguments on the basis of the certitudes of common sense and the kind of response to scepticism about foundations to be seen in mathematicians and engineers, for whom foundational advances are a vital part of how their discipline may be progressed.

Sir Thomas Heath observes, in the preface to his history of Greek Mathematics \cite{heath1921}, that the foundations of mathematics are Greek, and consisted in first principles, methods and terminology.
Our knowledge of early Greek mathematics and of rigour of its demonstrations comes to us primarily in the compilation by Euclid in his elements, which is the culmination of 300 years of Greek mathematics, and probably not representative of standards uniformly adopted \emph{ab initio}, but rather the result of gathering together 2-300 years of the results of work if varied standards of rigour into a single body of work presented to a uniformly high standard of rigour based on the same fundamental principles and methods.

What we see there, and what we will describe in greater detail concerning more modern foundational work, was not a starting point from which all the research was undertaken, but the result of a period of evolution of methods which ultimately could be codified to provide the body of work with a highly rigorous derivation from a single  foundation.
The importance of those foundations is confirmed by Aristotle's observation that 

Axiomatic Euclidean geometery was a high water mark in the rigour of mathematics which was not to be surpassed for over two thousand years, probably because the need was not then felt, despite considerable further development of mathematics since the time of Euclid.

Eventually developments transpired which propelled mathematics at the same time, into its greatest practical significance and beyond the confines of rigour.
This flowed the work of Newton and Leibniz.
Newtonian mechanics necessitated the kinds of operations on mathematical functions which were to be supplied independently by both Newton and Leibniz.
The procedures of differentiation and integration, which operate on functions to give on the one hand, the slope of the graph of the function and on the other the area under its curve.

The definitions of these concept made special demands upon the number system, particularly in the use by Leibniz of infinitesimal quantities, but also in Newton's account of fluxions as `last ratios'.

Doubts about the rigour of the procedure were voiced by Berkeley
who challenged the coherence of the ideas of fluxion and infinitesimal quantities\cite{berkeley2018analyst} in 1734 but it was not for another century before mathematicians became seriously embarrassed:

\begin{quotation}
  \emph{There are are very few theorems in advanced analysis which have been demonstrated in a logically tenable manner. Everywhere one finds this miserable way of concluding from the special to the general, and it is extremely peculiar that such a procedure has lead to so few of the so-called paradoxes.}

Abel, 1826
\end{quotation}

There ensued the most fertile period of foundational innovation in the history of mathematics.
This was not a search for absolute certainty, but for conceptual clarity and deductive rigour.
It occurred in several stages, most of which were instigated by Mathematicians and represented real advances in mathematics.

The first stage in this process was to show that the central notions of the calculus could be made precise without resort to infinitesimals or fluxions.
This aspect of the work had already been undertaken by Cauchy who's 1821 book ``Cours d'analyse''\cite{bradley2010cauchy}, rigorously defined the concept of convergence for sequences and series using what are now called Cauchy sequences.
Cauchy used these sequences to provide a precise epsilon-delta definition of limit and continuity for real functions which provided the means to  define differentials and integrals as limits \cite{cates2019cauchy}, giving precision and clarity to procedures which had evolved less formally over thousands of years going back to the work of Archimedes.

Having made these important advances in the rigour of analysis, there remained significant areas of uncertainty.
There was still no clear conception of the number system on which this work depended, though the position had improved with the elimination of infinitesimals from the theory.
The development of analysis had made it necessary for mathematical functions to be regarded as abstract entities rather than syntactic expressions, but the coherence of that position depended on clarification of the exactly what these abstract functions are.

From a modern perspective the clarification of these two points rests on the theory of sets which was begun by Cantor, but the reduction to set theory was a later development.

\subsection{Logical Truth}

In due course I will address some of the sceptical challenges to the idea of first philosophy, and to many aspects of the synthesis which is proposed, but at this stage I am concerned only to precisely describe the first stages in the synthesis by offering some informal definitions.

The single most important concept around which all else is constructed is that of \emph{logical truth}.
Many will be inclined to contest my use of that term for the concept which I will now describe.
It is the concept which is important, not the name I use for it.

Ultimately, in the synthesis which I provide, there is a place for addressing all the forms which knowledge may take, but insofar as epistemology is concerned with subject matters belonging to the natural sciences, it will receive no special attention beyond the considerations later advanced for the empirical sciences.

Placing the epistemology of logical truths (logically) prior to that of empirical science reflects the important role which mathematics plays in the natural sciences, which we consider in its broadest sense as the interpretation of science as constructing abstract models of physical systems and phenomena.

The concept of logical truth will be defined in the following stages.
\begin{itemize}
  \item
    In the first instance it is defined as a characteristic of sentences in languages of a particular kind (declarative languages with fixed vocabulary and a well defined truth conditional semantics).
  \item
Then the definition will be augmented to admit languages with an extendable vocabulary, whose truth conditional semantics is appropriately augmented by the process of extension.
In these latter we may say that the truth conditions for sentences in the language are sensitive to a context which reflects the extensions to the vocabulary and the constraints incorporated by the extensions.
\item
At this point we may speak of relationships between these 
\item

\end{itemize}


\section{Ontology}

I divide those things which may exist into three kinds as follows:

\begin{itemize}

\item concrete objects

  Those things which have spatio-temporal location and may be causally related to other concrete objects.

\item Purely abstract objects

  These are entities which do not have spatio-temporal locations and are not causally related to other objects.

\item Hybrid objects

  These are complex entities which are built from or built into both concrete and abstract objects, for example, a set of concrete objects will be such a hybrid.
  Thus, for example, if in the Simple Theory of Types, the individuals are held to be concrete objects, and the propositions are abstract, all other types will consist of hybrid entities.
  If on the other hand, the individuals are purely abstract, then the entire ontology will be purely abstract.
\end{itemize}

For the purposes of defining the notion of ``logical truth'' abstract ontology will suffice.

Some philosophers will think it reasonable to ask whether any abstract objects exist, and how we can possibly establish that they do.
By engaging in ``first philosophy'' I arrogate to myself the right to articulate without restraint whatever concepts will provide the basis for the epistemological synthesis which will follow.
This includes the concept of existence.
It is open to us to decide what that concept means and how it will be used in our system.

The position I adopt on the matter of existence is sensitive to the kind of entity concerned, and I need only at this point speak to the purely abstract entities at stake, in relation to which I proposed a wholly conventional stance.
This may be considered analogous to a fictionalist stance, differing from it in crucial ways.

The fictionalist is going to tell a story which he acknowledges is not factual, but which he considers instructive or entertaining in some other way.
The statements he makes in his fictional narrative will sometimes be false, and may not even be mutually consistent.

The conventionalist stance I take differs from fictionalism in these two principle respects:

\begin{itemize}
\item Within the scope of the relevant convention, the consequences of the convention are true, not false.
\item It is very important in the adoption of conventions, that the principles adopted be logically consistent, even when these conventions are adopted en route to the establishment of the concept of logical consistency.
\end{itemize}

In what follows hybrid objects will have no role, and it is very likely that I will tire of prefixing ``abstract'' with ``purely'', which should therefore be understood as implicit.

Normally in what follows, the adoption of some such ontological convention is a part of the definition of a formal language, and its scope is primarily in determing the meaning and truth value of expressions in that language.
More broadly in the less formal conduct of mathematics and perhaps other sciences, a convention may be simply contextual rather than being a part of the adoption of a language.
For example, there are multiple different formal axiomatisations of set theory, which disagree among themselves about the domain of which they speak, about which sets must, may or cannot exist.
The ``standard'' conception is of a domain exclusively of well-founded sets, but no axiomatic system guarantees that this is the case.
On the other hand, the axiom of separation, which guarantees the existence of all subsets of a set obtained selecting the members satisfying a given formula, makes it possible to prove that there is no universal set which contains all other sets.
Notwithstanding that difficulty with the universal set, many consistent set theories are available in which the universal set does exist and the principle of separation is not valid.

I may as well note in explaining my adoption of this conventionalist convention, that if I were to prove mistaken in disbelieving that there is any objective truth about what purely abstract entities exist (including the possibility that there are none), that the value that I associate with this position would be unabated.
Since abstract entities are causally unconnected with us, there existenve or otherwise in any absolute sense is immaterial to the utility which attaches to adopting the convention that some appropriate collection of abstract entities does exist.

Among the very many connections between the synthesis I present here and the philosophy of Rudolf Carnap, there is a direct connection here between my ontological stance and that which Rudolf Carnap attempted unsuccessfully to explain by distinguishing ``internal'' from ``external'' questions \cite{carnap50}.
``Internal'' questions are those posed in the context of some well defined language, and which are to be answered on the basis of the principles explicit or implicit in the definition of the language, ``exernal'' questions are those ontological questions which might arise in the process of defining the semantics of such a language, and were regarded by Carnap as meaningless pseudo questions.
Of course, we can ask what is the meta-language in question (which would normally be an idiom of a natural language such as English aumented by the vocabulary of some special discipline, perhaps mathematical or philosophical logic.
This is unlikely to provide any definitive ruling on the kinds of ontology which Carnap was considering,

The ontological conventionalism with respect to abstract entities which I am adopting here is not sensitive to the ontology of the metalanguage (which is indeed, in the first instance, English).
It is possible to describe an abstract domain whether or not the existence of that domain is demonstrable in that context, in just the same way that one can write a novel about an entirely fictional population.
If we then define the meaning of existence in the object language as membership in that possibly fictional domain, then ontological questions posed in that object language will be answered according to the structure of the chosen domain rather than by reference to any ontological principles which belong to the meta-language.

Though this discussion has addressed only abstract ontology, conventionalism in respect of concrete ontology is at least implicit in what will be said later about the representation of scientific and engineering knowledge.
It should also be noted that modern logic is generally agnostic about what things are, insofar as all properties and relations are external, and so long as those externalia are preserved, the substitution of a completely different collection of individuals in the domain of discourse has no effect on the truth values of any sentence.
This apparent indifference to identity is reinforced in the logical foundations which are proposed by ontological ambiguity in type constructors, the effect of which is that though various complex types of abstract entities are constructed in way similar to the constructions which would be used for mathematical objects in a pure set theory, there is no basis for identifying these newly constructed entities with the constructions themselves.
It is a foundational scheme in which numbers are not \emph{identifieed with} sets, or any other type of universal foundational ontology.

Though fully in the spirit of positivism, this epistemology is not nominalistic, a departure from the mainstream first appearing in Rudolf Carnap.

\section{Language}

It is not impossible to reason about the world without making use of language, but as soon as we talk about truth, language is presumed.
A generic definition of logical truth therefore depends upon an explicit and general account of some class of languages of which logical truth can meaningfully be predicated.

Though linguists and mathematical logicians may have conceptions of language which are purely syntactic (e.g. ``a set of sentences'') for the purpose of defining the concept of logical truth (as it is here construed), some semantics is essential.
The kind of semantics which is necessary is \emph{truth conditions}.

The essence of declarative language is the use of sentences to make claims which discriminate between various possibilities which constitute the subject matter of the language.
For the barest account of logical truth, we therefore require of the truth conditional semantics:

\begin{enumerate}

\item that a range of ``possibilities'' be identified, which are those for which the truth conditions settle a truth value.

\item that for each sentence in the language, a subset of possibilities is given, which are the cases for which the conditions expressed by the sentence under the semantics hold, in which case the sentence is then deemed true.
\end{enumerate}
    
\section{Logical Truth}

For the broad swathe of languages which satisfy the conditions given above, the logical truths are those sentences which are true in every possibility, hence necessary.

It may be noted that there is a special case arising when there is only one possibility.
This is exemplified by the language of first order arithmetic when given with the standard semantics which admits of just one possibility (up to isomorphism).
In that case, the idea that a sentence conveys information when asserted by eliminating possibilities is not as instructive as it might otherwise be.
That we have a clear intuitive understanding of the structure of the natural numbers clearly does not entail that we know what arbitrary sentences of arithmetic are true, and it is therefore informative to know that a sentence is true even if it has not ruled out any possible structure of the natural numbers.

The sceptically minded will further note that this conception of logical truth is just the identification of logical truth with analyticity, and that the concept is thus rendered wholly conventional and lacks the absolute character which some would wish to see in the notion of logical truth.
These consequence arise however, not from any defect in the notion of logical truth thus articulated, but rather from the fact that the syntax and semantics of languages are completely conventional, and hence that, however absolute a conception of logical truth you might have, the question of which sentences express logical truths must be determined by those conventions (though I do not deny that the relevant ``conventions'' may in the case of natural languages have been fixed, if indeed they are fixed, by biological and cultural evolution rather than by the whim of language designers).

\section{Universal Foundations}

A ``Universal Foundation'' for logical truth would be a language to which logical truth in any other language is reducible.
The idea is therefore dependent upon what concept of reducibility is intended.

My motivation in addressing this topic comes from the desire to support a linguistic pluralism while retaining the ability to reason about systems whose complete description is given in multiple languages.
In that case one needs a logical environment in which claims about subsystems in different languages can all be comprehended coherently.

A natural way to consider is by translation from one language to another, which is a kind of reduction, a correct translation permits truth in the source language to be reduced to truth in the target language.

The weakest constraint one could put on such a translation would be that it is an ``effective'' mapping which preserves logical truth.
This introduces a constraint on languages which it was not necessary to include in the definition of logical truth above, that the syntax, i.e. the set of sentences,  be countable.
We are now in recursion theory, and we can see that this kind of reducibility is that known in recursion theory as 1-reducibility of the set of logical truths in the source language to that in the destination language.
  
\footnote{Some references for future use:
\cite{oliveira2006unifying}
\cite{kumar2016self}
\cite{centrone2019reflections,dzamonja2019}
\cite{arthan1991formal}
\cite{jones1992a,jones1992b}
\cite{kumar2016self}
\cite{tarski31,tarski56}
}

\phantomsection
\addcontentsline{toc}{section}{Bibliography}
\bibliographystyle{rbjfmu}
\bibliography{rbj2}

\end{document}

% LocalWords:
