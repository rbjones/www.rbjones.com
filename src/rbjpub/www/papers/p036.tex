% $Id: p036.tex $
% bibref{rbjp036} pdfname{p036}
\documentclass[10pt,titlepage]{book}
\usepackage{makeidx}
\newcommand{\ignore}[1]{}
\usepackage{graphicx}
\usepackage[unicode]{hyperref}
\pagestyle{plain}
\usepackage[paperwidth=5.25in,paperheight=8in,hmargin={0.75in,0.5in},vmargin={0.5in,0.5in},includehead,includefoot]{geometry}
\hypersetup{pdfauthor={Roger Bishop Jones}}
\hypersetup{pdftitle={An Epistemological Synthesis}}
\hypersetup{colorlinks=true, urlcolor=red, citecolor=blue, filecolor=blue, linkcolor=blue}
%\usepackage{html}
\usepackage{paralist}
\usepackage{relsize}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{longtable}
\usepackage{url}
\newcommand{\hreg}[2]{\href{#1}{#2}\footnote{\url{#1}}}
\makeindex

\title{\bf An Epistemological Synthesis}
\author{Roger~Bishop~Jones}
\date{\small 2024/03/02}


\begin{document}

%\begin{abstract}
% An epistemology constructed for the advancement of science and technology in the age of interstellar hybrid (human, synthetic, post human) intelligence.
% 
%\end{abstract}

                               
\begin{titlepage}
\maketitle

%\vfill

%\begin{centering}

%{\footnotesize
%copyright\ Roger~Bishop~Jones;
%}%footnotesize

%\end{centering}

\end{titlepage}

\ \

\ignore{
\begin{centering}
{}
\end{centering}
}%ignore

\setcounter{tocdepth}{2}
{\parskip-0pt\tableofcontents}

%\listoffigures

\hfill
\ 
\pagebreak

%\addcontentsline{toc}{section}{Index}\label{index}
%{\twocolumn[]
%{\small\printindex}}

%\vfill

%\tiny{
%Started 2023/07/21


%\href{http://www.rbjones.com/rbjpub/www/papers/p032.pdf}{http://www.rbjones.com/rbjpub/www/papers/p036.pdf}

%}%tiny

\section*{About}

\section*{Prelude}

\part{Current Draft}

\chapter{Introduction}

Epistemology, the philosophical study of knowledge, has often been anthropocentric and psychologistic.
This may be seen in the not so distant past in the debate around the conception of knowledge as `justified true belief', which if accepted would place knowledge exclusively in the mind or the brain of human beings.

The epistemological synthesis which I offer here is neither.
It is a conception of knowledge compatible with inorganic intelligence, operating with knowledge often held in widely accessible shared repositories.
In it knowledge is not a psychological phenomenon, and is not the exclusive prerogative of homo sapiens, or even of the living.

In detaching my synthesis from human psychology I take knowledge to encompass all those cases where information about some entity or system is represented or modelled in a distinct structure.
In a more traditional epistemological perspective the locus for this representation would be the human mind or brain, but in the proposed perspective it need not be.
There are clear examples of such knowledge representation which probably are not controversial, such as the knowledge to be found in printed books, and that we now find in electronic data processing systems, sometimes as different physical renderings (\emph{in silico} perhaps) of books also published on paper, sometimes as purely electronic publications.

You might therefore suppose that I have drifted from epistemology into information science, but I affirm here that semantics, truth, and assurance (related to `justification' but not so binary) are essential ingredients in the proposed synthesis; bits and bytes are all we need for information.
I am positioned at the philosophical end of cognitive science, despite the difficulty in detaching the word `cognition' from its mental connotations and associations.
A more easy association is with that field of Artificial Intelligence known as \emph{Knowledge Representation an Reasoning}, with its 35 year history of international conferences\footnote{See \href{http://kr.org/}{kr.org}}, relative to which, styling this work \emph{epistemology} reflects a broader and more fundamental philosophical perspective.

My epistemological synthesis is not just a perspective on what knowledge \emph{is}, and how it may be acquired managed and applied.
It is a suggestion about what knowledge \emph{might be}, and how we might evaluate, manage and apply it in the future.
Though not one intended for universal application, not for everyday use, but rather for science, engineering, commerce and any other domain in which deductive reasoning might sometimes seem appropriate.

\section{Epistemic Evolution}

The possibility (or inevitability) of change in these matters is underpinned by evolutionary thinking.
The particular synthesis offered will be presented as a natural continuation of the evolution of knowledge, of language, culture and of evolution itself over the last four billion years.

In that long evolutionary history there are a small number of significant landmarks which will help me sketch the central features of my synthesis.
These are major advances in the ways in which knowledge can be represented, in which a novel manner of representation becomes available  because of the achievements of evolution.
Sometimes these new methods are sufficiently advanced that they subsume and expand everything which has gone before.
It is just such an advance which I believe to be upon us, as the world teeters toward the realisation of self-reproducing artificial super-intelligent systems.

\begin{itemize}
\item{The Genome}
  
  A repository of detailed knowledge of the chemical structure of proteins essential to each life form, a blueprint for the construction of self replicators.
\item{Memory}
  
  A dynamic store of knowledge about each individuals environment essential for survival, evolving to provide special memory facilities for each new capability of these life forms.
\item{Language}
  
  A means to share knowledge facilitating the growth of culture as a shared survival resource moving faster than biological evolution.
  
\item{Axiomatics}
  
\item{Foundations}

\item{The Deductive Paradigm}
  
\end{itemize}


\paragraph{The Coding of The Genome}
The first of those milestones in the evolution of knowledge is the origin of life, abiogenesis.
Not only does pre-biotic evolution craft an organism capable of surviving and reproducing in the primordial bioshpere, it devises a specific and precise way or recording the exact chemical composition of the complex proteins necessary for those living organisms to survive and for their self-replication.
This is the coding of protein structure as DNA molecules, a coding with a clear semantics which is implemented operationally by ribosomes which in the context of a living organism construct the proteins coded by these DNA sequences.
The complete hereditary information which determines the structure and development of an organism (the genome) is not confined to the DNA coding sequences.
Other aspects of the genome are not all understood as well, and may not have quite so precise and intelligible a semantics as the protein codings, but they may nevertheless be thought of as integral parts of a body of knowledge which enables the construction and development of viable organisms.

The encoding of protein structure in DNA has some of the key ingredients which linguists identify as distinguishing human language from the interactions of other species, notably that arbitrarily complex expressions can be formed, there being infinitely many possible chemical structures which a protein might take, and a DNA sequence corresponding to each one.
The codings are also discrete or \emph{digital}, a characteristic which comes to prominence again much later in the evolution of knowledge when information technology is invented.

In that extended conception of knowledge, evolution at that time begins to evolve life through the evolution of the genomes of living species, and it is those genomes which constitute a body of knowledge which is gradually accumulated as life evolves.
The clearest aspect of this is the representation in the genomes of the proteins necessary to the development of the individuals of the species and to their life and reproduction, encoded in DNA.
This includes the proteins which form the ribosomes which manufacture the protein's according to their specification in the DNA.
Full details of how the genome determines the construction and operation of the individuals of the species are not yet known, but advances continue to be made, and are being accelerated by AI as evidenced by Deep Mind's \emph{alphafold} which has automated the previously highly labour intensive process of discovering how a protein folds, a determinant of the properties of the protein.


\paragraph{The Evolution of Memory}
The knowledge which evolution encapsulates in genomes is very slow moving.
It serves the evolution of species adapted to particular environmental niches, but does not enable those species or individuals to adapt to the particular opportunities and threats in their own special nook in that niche as it changes from day to day or on any timescale smaller than the lifetime of an individual.

That kind of adaptive behaviour depends upon individual organisms learning about their own particular situation so that their behaviour can be adapted to it in ways which maximise their reproductive success.
The evolution of neurons and of the central nervous system serves that purpose and creates a new form of knowledge which progresses on much smaller timescales than the evolution of the genome.
Among its many roles in the regulation of the organism, the central nervous system gathers information from the sense organs about the environment of the organism which it makes use of in its struggle to survive and reproduce.

The knowledge in this case is physically represented by the structure of the nervous system, the synaptic connections between neurons and the synaptic weights which determine the significance of each connection.
Whereas in more primitive life forms, behaviour patterns would be largely `hard wired' by the slowly evolving knowledge in the genome, in these more sophisticated organisms could adapt more rapidly by storing important knowledge in the more volatile repository of the nervous system.
It is now thought that, as the central nervous system evolved supporting the adaptation of species to new environmental opportunities and challenges, new kinds of memory were an integral part of those cerebral adaptations.
The history of the evolution of memory, is also part of the evolution of knowledge, for memory must be memory of something and constitutes a kind of knowledge.

\paragraph{Language and Culture} are the next stop on our journey.
The knowledge which an individual garners during his lifetime is valuable not only to that individual, but also to those around him, but is not readily passed to them, and disappears when that individual dies.
It is a part of the adaptability of that individual, but not part of the evolutionary progression of the species (though the capability is).
There is some limited ability to transfer such knowledge, which is evident in precursors to the next major step in the evolution of knowledge.
Culture begins, in all probability, before language, as tools development begins perhaps a million years before language.
for it is likely language co-evolved with homo sapiens a process in which rapid growth of brain size is conspicuous, suggesting an end point (homo sapiens).

\paragraph{Axiomatic Precision}

In the accelerating evolution of Western Culture, the civilisation of ancient Greece marked an inflection point, in which multiple aspects of the evolution of knowledge advanced including the first known systematic thinking about knowledge which can be thought of as the beginnings of epistemology.

For the present history it is the transformation of mathematics from a body of numerical and geometric methods into a theoretical discipline which is most significant.
That transformation depended upon more elaborate deductive reasoning than had been hitherto undertaken, which in turn, in defence against the ubiquitous fallacy of equivocation required precision of language which could only be fully realised through what became known as the \emph{axiomatic method}, perfected after its gradual establishment in 300 years of mathematical reasearch in the compilation known as Euclid's \emph{Elements}\cite{euclidEL1}.

\paragraph{Foundational Universality}

\paragraph{The Deductive Paradigm Shift}

\section{Prospectus}

In the chapters which follow I aim to fill out that preliminary sketch in various ways.
My first objective is to add detail and clarity to the ideas of foundational universality and the deductive paradigm shift, which I will approach through a more detailed historical account of the more recent stages in the development of the key ideas.

A principle result of that more detailed account is reliable methods for making certain aspects of semantics precise, and of then establishing conclusively when sentences in the relevant languages are true.
It is not part of the thesis that absolutes are ever achievable, that one can ever make meaning absolutely unambiguous or truth absolutely certain, but that standards of clarity which are exceptional are possible in a very special if narrow semantic domain, and that within this same small but crucial domain unprecedented reliability of truth can be achieved.

My next aim is to underpin the fundamentals of these ideas, by offering by describing how those claims to clarity and truth can be supported.
These are arguments against philosophical scepticisms in relation to meaning and truth, the most important of which are those which deal with the problem of regress in semantics and justification.

Having in this way underpinned the proposed foundations, my effort will then turn to exposition of how these foundations can serve to underpin knowledge in a very broad sense, arguing that knowledge in general would benefit from being seen through the deductive lense which the foundations provide.

\chapter{Deduction}\label{ChapDeduction}

Deductive inference and the related concept of logical truth are central to the ideas presented here, and play a fundamental role in the epistemological synthesis.

The word `deduction' or its Latin predecessor only began to be used to describe logical inference during the period of the Medieval Scholastic philosophers, but the practice of deduction is coeval with propositional language, perhaps 300,000 years ago, and the study of this kind of reason dates back at least as far as Aristotle in the 4th Century BC.

Deduction can be formal and syntactic (concerned with the structure of linguistic expressions)  or informal and semantic (concerned with the meanings of linguistic expressions).
Informal deduction, has been around as long as \emph{homo sapiens}, but formal deduction was unknown until about 150 years ago.

Deductive inference is peculiar to declarative or propositional language, so an account of deduction must begin with a few words about this kind of language.

\paragraph{Declarative Language}

Declarative or propositional language is the kind of language which makes use of sentences to make claims (or express propositions) about some domain of interest (sometimes, the world, more often some part of it or some fictions about it).

These propositions are sometimes true, depending on how the world is and whether the world conforms to the claim expressed.
We may therefore consider an important part of the meaning of a sentence to lie in the collection of possibilities under which the sentence is true.
This aspect of meaning is called its semantics, and this way of accounting for the semantics of a propositional language is called \emph{truth conditional}.

When we think of semantics in this way an important relationship between sentences or propositions can be made precise, which is called the relationship of \emph{entailment}.
A sentence A entails some other sentence B if the truth conditions of A are all truth conditions of B, so that whenever A is true, so is B.
This is to say, that any possible state of the subject matter under which A is true is also one in which B is true, whenever A is true, so is B.
This legitimises an inference from the truth of A to that of B, and under these circumstances, i.e. when A entails B, we call the inference from A to B \emph{deductive} (in the informal semantic sense which which exclusively characterised deduction for its first quarter of a millenium.

We will see later that often the term deduction is reserved for a kind of inference which is syntactically rather than semantically characterised, which characterisation is only possible in a formal language.
However, the invention of formal languages in which these criteria can be defined is very recent, and it is generally recognised that mathematical proofs for the two millenia prior to the invention of formal languages were deductive but informal, and it is convenient therefore for the purposes of this monograph to work with a semantic conception of deduction, while recognising the enormous importance of the transition from informal deductive reasoning to strictly formal deductive systems.

A brief historical narrative may help to make clear the development of these ideas up to the present, as a basis for the epistemological synthesis which follows.

\ignore{
Deduction is often now considered a syntactic feature of formal deductive systems.
These are however very modern, dating back less than 200 years, whereas the use of deductive reasoning in Greek mathematics dates backmore than two millenia before the first formal logics, and similar if less rigorous and elaborate use of deductive inference may date back as far as the origins of human language.

An explanation of deduction depends upon an idealisation of important features of natural languages, in particular on the supposition that indicative sentences have definite truth conditions, and that competence in a language necessarily yields insights into a special relationship between sentences which we call `entailment'.

This kind of language is presented here as an idealisation of important aspects of the kinds of natural language whose origin coincides with that of our species homo sapiens, and is found in no other known biological species.

First a few words about the nature of deduction and the importance it has had in the advancement of \emph{homo sapiens} to the levels of well-being and prosperity which we now enjoy.

Deduction is a form of reasoning which is available in declarative or propositional language.

Ludwig Wittgenstein spoke in his philosophy of the variety of language, going beyond Augustinian conception of language as working by the use of sentences which express relationships between things signified by the constituent words and phrases of the sentence.
His broadened conception of language likened the variety of linguistic discourse to the idea of games, in which rules govern the conduct of the game in which the moves do not necessarily have any significance beyond the role they play in the game.

While accepting this broadened conception of language, we are here concerned specifically with the kind of language in which deductive reasoning can take place, which has a very important place in human history and in the betterment of humanity.
That kind of language is called declarative or propositional language, the process of deduction being one in which from one or more propositions which are supposed true, further propositions may be inferred which will of necessity be true if the premises are indeed true.

@@@@@@@

The chasm between the linguistic abilities of humans and those known in any other species is so great that it is likely that the evolution of language went hand in hand with the rapid growth of brain size in genus homo leading up to the emergence of anatomically modern homo sapiens.
At this point the innate capabilities of homo sapiens must have laregly sufficed for the kind of complex propositional language which is now pervasive among humans.
There is close coupling between competence in such language and the ability to perform elementary deductive reasoning, because the most elementary deductions flow from an understanding of the meaning of taxonomically related concepts.
One cannot be said to understand the concept of horse if one does not know that horses are mammals, and that knowledge yields the natural (deductive) inference from knowledge about mammals to knowledge about horses.

}

\paragraph{Some History of Deduction}

Deduction has a long history.
This introductory account will describe it in terms of four stages in the advancement of our understanding of its practice and theory.
It is convenient to consider deductive inference under the following four headings:

\begin{itemize}
\item informal
  
  The informal practice of deductive dates back to the origins of language early in the history of homo sapiens, and continues to the present day.
  It requires no understanding of the nature and significance of deduction, but is simply a part of what constitutes a good understanding and competence in a natural language.
  It is practically useful, but will fail if pressed too far, and can be manipulated to yield whatever results we find convenient.
  
\item systematic

  Elaborate application of informal deductive inference is perilous, but subject to certain rules of hygene concerning the context in which deduction is conducted, can be made safe and productive.
  The first documented use of systematic deduction subject to appropriate and effective guardrails appears in the development of mathematics as a theoretical discipline in ancient Greece, beginning with the philosopher Thales, round about 600 BC.
  Over a period of approximately 300 years, mathematics and the axiomatic deductive method were expanded and refined, culminating the Euclid's compilation of his ``Elements'' \cite{euclidEL1}, in which the axiomatic method was clearly documented and 300 years of productive research were gathered together and demonstrated by that method.

  The key features of the `systematic' deduction found in Euclid's Elements serve in the first instance:
  \begin{itemize}
  \item to limit the language to some clearly intelligible domain
  \item to limit the premises from which deduction may proceed to a fixed set whose truth is clearly warranted in that chosen subject matter, and
    \item to ensure that every deductive step proceeds from an identified small set of these premises or from theorems previously proven from them
  \end{itemize}
  This was done using explicit \emph{definitions}, \emph{postulates} and \emph{axioms} which characterise the subject matter and provide the premises from which all theorems are ultimately derived, and by ensuring that in each step of a proof the premises from which the conclusion is derived are identified and directly entail the conclusion.

  Systematic deductive reasoning, with varying degrees of rigour in the observation of the kind of strict axiomatic structure adopted by Euclid, have a long history of success in the continued development of mathematics since Euclid down to the present day.
  They have also been attempted occasionally in other domains, generally with much less reliable and convincing results.
 Factors mitigating against success include the difficulty in providing good definitions or axiomatic characterisations of the key concepts, and the dependence of reasoning in the field upon wider considerations (e.g. an understanding of the empirical world
   
\item formal 

  The axiomatic method provided a context in which informal deduction could be applied safely and productively in much more elaborate and complex derivations than would otherwise be reliable, primarily by ensuring very definite subject matters and strictly limited premises from which deductions could proceed.
  The nature of the deduction which is involved was nevertheless, just that informal deductive inference which is intuitively practised by any competent speaker of a natural language.
  This remains to this day the predominant way of demonstrating mathematical theorems.

  The advent of modern science, most particularly in the theories of mechanics and gravitation of Newton, opened up a plethora of practical applications as the industrial revolution approached and gathered steam.
  Those applications depended on new mathematical techniques, notably on the differential and integral calculi and the whole new mathematical discipline called \emph{analysis}.
  These seemed to depend on innovations in the concept of number which were poorly understood, of doubtful coherence, and which were more difficult to characterise in coherent principles from which mathematical results could be deduced.
  At the beginning of the 19th Century mathematicians set about putting their house in order, first of all reformulating the required mathematics in clear terms which avoided the controversial idea of infinitesimal numbers, and proceeding from there to the clarification of the system of real numbers and of the mathematical concept of \emph{function} which had assumed greater significance as functions morphed from certain kinds of mathematical expression to bona-fide mathematical entities upon which operations like differentiation and integration could be performed.

  This progression ever deeper into the foundations of mathematics then took a more philosophical turn, as the philosopher Gottlob Frege entered into a disagreement about the status of mathematics which had appeared in the previous century as a claim by David Hume, rejected (in different language) by Immanual Kant, which was re-asserted by Frege to the effect that mathematics was the body of knowledge derivable from the definitions of mathematical concepts by purely deductive inference, and consequently that the truths of mathematics are \emph{logical} truths.
  
\item foundational

  A formal language with a sound formal definition of proof rules provides a reliable account of deductive inference, but deductive inference is only as good as the context in which it is conducted.
  If the premises from which deductions are made are logically inconsistent, then valid deductive reasoning does not yield true results.

  A logical foundation system solves the problem of ensuring that the context in which deduction takes place is coherent, that deduction is undertaken from true premises or definitions constructed in a safe context.
  Inconsistency is thereby prevented, and the truth of the results is assured.

  A further important feature is practical universality.
  An abstractly codified foundation system provides deductive capability over an abstract syntax suitable for the representation of any sufficiently well-defined finitary language in which deductive reasoning is in principle possible.
  This makes possible a comprehensive knowledge base and permits reasoning in the context of systems or subsystems defined each defined using the most suitable language for the type of system, hence facilitating (for example) the verification of designs of highly complex systems.
  
\end{itemize}

\section{Informal Deductive Reasoning}

There is no evidence of an appreciation of the distinction between deductive and other ways of reasoning until about 600 BC, but the ability to undertake elementary deductive reasoning may be seen to flow from or be implicit in competence in the use of propositional languge.

This has its limits, an appreciation of which motivates the later stages in the development of deductive methods.

The elements of language comprehension which are closely related to deductive competence can be illustrated under three headings:
\begin{itemize}
\item \emph{propositional} reasoning, which flows from an understanding of the propositional connectives, \emph{and}, \emph{or}, and \emph{not}.
\item \emph{taxonomical} reasoning in which an understanding of taxonomical heirarchies yields inference between semantically related classifications.
\item \emph{generality} and \emph{specialisation}, in which inference from general truths to particular cases or general principles are inferred from indefinite truths.
\end{itemize}

The practical impact of deductive reasoning has been effected so far only under the first two headings.
Formal deductive systems have only recently been invented (within the last two centuries) and have so far proved too onerous for widespread use.
Informal deductive reasoning is however ubiquitous and dates back as far as declarative language itself (perhaps 300,000 years).

\subsection{Propositional Reasoning}

The propositional connectives `and', `or' and `not' may be used to form complex propositions by combining or modifying simpler propositions, thus from the propositions `grass is green', `roses are red' and `black is white' we can infer `grass is green and roses are red', and conversely, from the latter, the two former propositions.
We can also conclude from `grass is green' that `grass is green and black is white', and from that compound truth together with the fact that black is not white, we infer that `grass is green'.
These words could not serve their normal purpose in language if the hearers were not capable of making these elementary deductions.

It should nevertheless be noted that the word are not confined in their use to these purely truth functional applications, and in some contexts may carry additional information.
This `and' may sometimes connote temporal sequence, having a sense similar to `and then', and `or' may be exclusive, indicating that exactly one of the two alternatives which it connects is true.
These are some of the pitfalls of informal reasoning which have lead to the progressive refinement of deductive reasoning which is our present theme.

\subsection{Taxonomic Heirarchies}

Closely related to propositional reasoning is reasoning about taxonomical heirarchies.
It is biology which provides the best source of examples of such taxonomic heirarchies.
Thus in biology animals are classified into species, which are classified in turn into genus, family, order, class and so on up to domain.
Within this heirarchy distinct members at each level are required to be disjoint, and their defining characteristics must therefore be logically incompatible.
The relationship between one classification and the classification at the next level to which it belongs is that of conceptual inclusion.
Any member of the lower classification must satisfy all the criteria of the higher level as well as some additional criteria which characterise the lower level classification.
Consequently, a deduction may be made from membership of the lower to membership of the upper.

For example, the domestic horse (equus caballus) and the zebra (equus grevyi) are species of the genus equus.
Thus being a zebra entails being equine, and justifies the deduction from the former to the latter.
It also contradicts being a domestic horse (i.e. entails \emph{not} being a domestic horse) rendering that further deduction sound.

These kinds of deductive reasoning justified by a good understanding of these conceptual heirarchies also apply to relational inclusion.
Examples of these are likely to be more complex and will be less common in everyday discourse, but may be significant in the more elaborate informal reason found in mathematics and science.

\subsection{Generality and Specialisation}

An understanding of the words \emph{all} and \emph{some} also comes with some elementary deductive reasoning.
If something is true of some collection, e.g. all men are moral, then it is true of any member of that group, for example, if socrates is a man, then he is mortal.
The two premises together entail the conclusion and the inference is therefore deductive.

One might also infer from the mortality of all humans that there are some mortal humans, but this depends on there being humans.
We can also establish general principles by reasoning about indefinite individuals of some group, or by transitive reasoning with generalities.

Thus, if all rodents are mammals and all mammals are vertebrates, then we may infer that all rodents are vertebrates.
The premises entail the conclusion and the inference is deductive.
This kind of inference is immediate to those who understand the language, whether or not it is understood that an inference is involved.
This was the case long before we had the concept of or a word for deductive inference.

\section{Systematic Deduction}

There may have been, since the beginnings of declarative language, more than a quarter of a million years of elementary deductive reasoning on the part of homo sapiens before anyone reflected on what was going on.
The first clear evidence of self conscious application of deductive reasoning in a systematic way is drawn from the earliest stages of the development of mathematics by philosophers of ancient Greece, in whose hands a theoretical discipline emerged from what had previously been mostly an aggregation of arithmetic and geometric techniques in Mesopotamia and Egypt.
Thales was the first of those philosophers, working around 600 BC.

The evidence of those first developments is scant, no writings having survived, but as the body of mathematical knowledge grew, and the methods of proof were refined and documented (notably by Aristotle in his Organon), consolidations of key results (``Elements'') began to appear culminating in the Elements of Euclid \cite{EuclidEL1}, a classic text whose contents have weathered the test of time and were used in teaching right through into the 20th Century.

The method articulated and practised in the Elements was \emph{the axiomatic method}, and remained for the next two thousand years the highest standard of rigour in the deductive derivation of the principles of mathematics.

Rigour to that standard was to be probably the exception rather than the rule.
For the most hectic periods in the subsequent development of mathematics, the work was driven by the opportunities and needs of scientists and engineers, and more practical criteria than the demand for rigour determined the acceptability of the mathematical discoveries and innovations.

Nevertheless, the ideal was in place, and from time to time, often provoked by conceptual muddle, the more philosophical mathematicians or or mathematical philosophers would offer their criticisms and sometimes provoke or contribute to a retreat from confusion and reinforcement to the foundations.

It was in the wake of one of the most important periods of retrenchment that the next important advancement in the story of deduction.

\section{Formal Deduction}

The success of systematically organised informal deduction in furthering the development of mathematics in the two thousand years after Euclid was remarkable, but it is not the end of our story.

The next important step in that story is the \emph{formalisation} of
deduction.
The formalisation of deduction involved the discovery of ways of checking the correctness of the inferences which made up a deductive proof.

The idea is that without recourse to semantic insights, but merely by the use of effective, well defined, checks on the syntactic form of the inference, its soundness could be conclusively established.
In order for this to be possible, the syntax of the language in which the mathematics is presented must be itself formally defined, and then for that particular formal language, the permitted rules of inference could also be defined in such a manner as to ensure that every inference thus permitted was deductively sound.

Aristotle had made some first steps toward formality in the description of his syllogistic logic, but up until the beginning of the Scientific Revolution in the 16th Century, mathematics was conducted in natural languages, albeit supplemented by a growing lexicon of mathematical concepts.

Three key developments paved the way for a formal conception of deduction adequate for the kinds of informal deductive reasoning required for mathematical proofs.

\begin{itemize}
\item the introduction of symbolic notations
\item the introduction of the concept of a mathematical \emph{function}
\item the interpretation of sentences as \emph{truth valued} expressions, and the mathematical treatment of sentential connectives as forming a boolean algebra
\end{itemize}

The first was the transformation of mathematical discourse through the introduction of symbolic notation, which served mainly to make complex expressions more concise and hence more easily grasped and manipulated by the intellect.
Early steps included the use of symbols for the various operations on numbers, and for equality.
In the the development of algebraic methods, the use of letters for variable quantities or for fixed but arbitrary quantities permitted the articulation of general principles, which could subsequently be used to transform expressions in applications of mathematics to science engineering or commerce.

Symbols for roots, notation for exponents, and quotients, combined with the use of letters for variables or arbitrary constants. 


This required a regimentation of mathematical notation (syntax) sufficient to admit precise syntactic rules for what constituted an elementary deductive inference.

This was first done by the Philosopher Gottlob Frege who devised a notation which he called \emph{begriffsschrift} \cite{Frege79}, which he conceived as a completely general system in which deductive reasoning about arbitrary (not necessarily mathematical) concepts could be undertaken.

\begin{quote}
  During the scientific revolution, in the 16th and 17th century, considerable advance in the language of mathematics took place including the first use of symbols for mathematical concepts and entities permitting more concise arithmetic expressions and the transition to algebraic expressions in the work of Viète.
  Early in this period, the study of mechanics and other natural phenomena brought out the concept of function which became central to the developments leading to the differential and integral calculus ultimately perfected by Newton and Leibniz.
  These developments to the language of mathematics paved the way to a reconceptualisation of logic through the perception of sentences as truth valued expressions, and logical connectives as truth functional sentential operators.
  The use of variables in construing expressions as functions then made the treatment of "all" and "some" as quantifying operators over truth valued functions natural and powerful overcoming the many weaknesses in Aristotelian syllogistic which rendered it impotent for the kinds of deductive proof required in mathematics.
\end{quote}

\chapter{Logical Foundation Systems}\label{ChapLFS}

\section{The Inconsistency of Frege's System}

Frege's mission to show that mathematics is logic was underpinned by the slogan:

\begin{quote}
{\it Mathematics = Logic + Definitions}
\end{quote}

This connects with the systematic use of informal deduction for the derivation of mathematics in the following way.
The Logic which Frege had in mind was the formalisation of the notion of deduction which he had published as his \emph{Begriffsschrift} \cite{frege79}.
The main effect of the systematisation offered by adoption of an axiomatic method such as that found in Euclid is to ensure that the context in which deduction takes place is coherent, by ensuring that the principles from which the deductions begin are principles which are true of some subject matter.
This is reflected in Frege's dictum through the requirement that the formal deductions for the derivation of mathematics begins exclusively with the definitions of the mathematical concepts.

Frege undertook the derivation of mathematics using his Begriffsschrift over the next two decades, which was to appear in a multi-volume work entitled \emph{Die Grundgesetze der Arithmetik}\cite{frege93}.
But towards the end of this period, just as the second volume work was in the final stages, the philosopher Bertrand Russell, who was independently working toward similar ends, became acquainted with the work and noticed that a contradiction could be derived in it.

Though Frege's Begriffsschrift was a game changer which ultimately would herald the transformation of logic through the discipline of \emph{mathematical} or \emph{symbolic logic}, its use in Frege's |emph{Grundgesetse} inconsistency 
The lesson learned from the inconsistency of Frege's system

\section{Two Resolutions of the Paradoxes}

By 1908 two quite different resolutions to the inconsistency of Frege's system were published.
Ernst Zermelo's published an axiomatisation of set theory \cite{zermelo08}, and Bertrand Russell published his \emph{Theory of Types}\cite{heijenoort67,russell1908}.

These were very different resolutions, the first belonged to the extand tradition of mathematics as systematic informal deduction, the second was a revised assault on the problem of basing mathematics on a formal deductive system.

\subsection{Zermelo's Set Theory}

The key requirement for a systematically deductive mathematical theory is clarity about the subject matter delivered through a fixed set of axioms and definitions.

In a way Frege's ambition had been a departure from that tradition insofar as he envisaged one axiomatic system in which all the diverse parts of mathematics could be derived by adding only the definitions of the concepts peculiar to that branch of mathematics.
The foundational work on analysis had shown that analysis could be reduced to arithmetic, and Frege's aim was to show that arithmetic could be reduced to logic.

That Frege had chosen to do this via a formalisation of logic was incidental to this central purpose.
It served that purpose by making it easier to see in such a formalisation that the required mathematical theorems really were logically derived from nothing but the necessary definitions.

Zermelo's set theory, as published in 1908 was not then known to be adequate as a foundation for mathematics, and was criticised at the time for the lack of clarity about what `properties' could be used in the formation of subsets using the separation axiom.
It would later be made precise by formulation in what became first order logic, and more powerful through the addition of the replacement axiom due to Fraenkel.

\subsection{Russell's Theory of Types}

Zermelo's axiomatisation of set theory would eventually provide an alternative foundation for mathematics, as it was in due course discovered how more or less arbitrary structures could be modelled as pure sets, but at this stage the motivation was rather different.

Russell, on the other hand, was fully engaged in an enterprise very similar to that of Frege, and at the time he discovered his `paradox' was well advanced in writing \emph{The Principles of Mathematics}\cite{russellPRM} an informal account of the ideas which intended to  supplement with a formal account in a second volume.
After his discovery of the paradoxes, and coming to an appreciation of the scale and complexity of the task, he entered into the collaboration with A.N.~Whitehead which bore fruit as \emph{Principia Mathematica} \cite{russell1913}, a formalisation of mathematics based on Russell's \emph{Theory of Types} \cite{russell1908}.

\section{The Simple Theory of Types}

\section{Church's Simple Theory of Types}

\section{Gordon's HOL}

\section{Definition as Conservative Extension in HOL}

\chapter{Foundational Responses to Scepticisms}\label{ChapFRS}

Scepticism has an important role to play in motivating a focus on rigour which enables us to separate scientific knowledge from psudo-scientific fantasy.
Absolute certainty is nevertheless neither necessary nor achievable, and the kind of radical scepticism which discards the possibility of knowledge because there is always room for doubt, however slender, is corrosive and counterproductive.

In this defence against scepticism I begin by acknowledging that absolutes are unachievable.
The two particulars which I will look more closely at are scepticism about the possibility of precise language, scepticism about meaning, and scepticism about the establishing with certainty the truth of any proposition.

My defence will be qualified.
I will argue that:
\begin{itemize}
\item a certain fraction or aspect of meaning can be made very precise in important languages,
\item that truth in such languages can then be established with very high degrees of certainty,
\item that the certainty extends to the validity of deductive reasoning in any domain in which the language is adequate to support such reasoning,
\item and that the benefits of these certainties spill over into every aspect of the establishment, management and exploitation of knowledge.
\end{itemize}

A relevant phenomenon which I mention in support of these ideas is the difference in attitude which is observable between the attitude to these matters on the one hand of philosophers and on the other among mathematicians, scientists and engineers.

This can be illuminated by observing the work done on foundations by mathematicians in the 19th Century, and the research on ``full stack verification'' by Computer Scientists in the 20th Century.

\subsection{The Rigorisation of Analysis}

After a period of stagnation during the middle ages, mathematics once more began to progress under the stimulus of the Scientific Revolution.
The late 15th and 16th Centuries saw unprecedented levels of original work in mathematics.
Among the stimulae for this were military applications during a period of proligfic warfare in Europe after the invention of gunpower had made an understanding of ballistics crucial to the effectiveness of artillary.
There was also great demand for improved methods of navigation for shipping, at a time when measuring longitude was neither reliable nor precise.
Understanding and predicting the motions of the moon and the planets was crucial to improving navigation.


Mathematical notation became symbolic, the concept of mathematical function originated, and a century of research by many prominent mathematician progressed methods for manipulating functions which were finally perfected by Newton and Leibniz in the differential and integral calcului.




\chapter{Knowledge Through a Deductive Lens}\label{ChapKTDL}

\part{Attic}



\chapter{Introduction}

Epistemology is the philosophy of knowledge.

Various kinds of knowledge have appeared during the course of the evolution of the ecosystem on this planet.
Evolution itself may be considered as a process which creates knowledge of how to construct organisms which can flourish in the various environmental niches provided by the ecosystem as it develops.
That knowledge is encoded in the genome of each biological species.
Science is gradually discovering how to read the language in which this knowledge is represented, having achieved the first step of recognising how the protein molecules essential to the organism are coded as DNA sequences and the ribosomes which interpret the DNA and construct the specified proteins.

Many forms of knowledge form a part of the evolution of life, and these forms of knowledge may therefore be thought of as having co-evolved with life on earth.
For example, the representation of protein's in DNA was a prerequisite for life to emerge.
Subsequently, the evolution of the central nervous system provided a new substrate for knowledge, and we may think of the evolution of memory over the 500 Million years since vertebrates appeared as a continuing evolution in the nature of knowledge.
When oral and then written language appeared, they provided new kinds of knowledge, which for the first time could be aggregated and propagated throughout a culture and across large geographic and temporal expanses.
The invention of information technology created new media for and new kinds of knowledge.


\chapter{Introduction}

My aim in this monograph is to present some ideas about the future of knowledge.
Epistemology is the discipline which engages in philosophical theories about knowledge.
In offering an epistemological \emph{synthesis} I am putting forward some ideas about how the nature of knowledge might evolve in a future where evolution, both biological and cultural, proceeds increasingly through intelligent design and selection rather than purely by chance variation and natural selection.
This is the world of artificial intelligence.

The synthesis I propose is underpinned by my own perspective on the evolution of knowledge and epistemology over the history of planet Earth, and is a speculative projection on my part of where that evolutionary trajectory will lead us in the future.

It is my aim in this introduction to give as concise a first sketch of the broad picture which constitutes my thesis in as concise and intelligible form as can be hoped for.

That broad picture is evolutionary.
What evolves in that picture in a coupled way are two phenomena, closely related to but more broadly conceived than, \emph{knowledge} and \emph{language}.
The broader idea, within which various kinds of knowledge are encompassed, is that of encodings or models.
These fulfil a role similar to that of sentences in a language in which knowledge may be expressed, but appear in the evolution of life very much earlier than language, which linguists recognise only in \emph{homo sapiens}.
Examples of such encodings appear at the very beginning of life on earth, since DNA provides an encoding of the structure of proteins essential for life, and the genome of an organism may be thought of as a repository of knowledge (or at least an encoding of information) about how that organism is constructed and operated.

The evolution of life on earth takes us at the same time through an evolution of the forms of such encodings of information, and at the same time constitutes a continuous aggregation of data about how the various products of evolution are structured to thrive in the environments provided in a changing ecosystem.

In the early stages of this process it is evolution which is aggregating these kinds of proto-knowledge, and in the process engaging in the design of organisms.
Even the earliest organisms are more likely to flourish if they can respond to the specific opportunities and threats in their immediate environment, and to do that evolve sense organs which collect knowledge about that environment and nervous systems which transform that knowledge into appropriate actions to guard against perils and seize opportunities.
In this way genetically hard wired behaviours become more flexible and the survival knowledge aggregated by evolution becomes supplemented by information gleaned by the organism.
The transient encodings which mediate in this kind of stimulus/reponse behaviour then evolve toward patterns of behaviour supported by persistent memory of aspects of the environment, which demands new methods of encoding information, and new kinds of proto-knowledge.


\chapter{Introduction}

Epistemology is sometimes `analytic' in which case it may be primarily concerned with conceptual analysis, or it may be `natural' in which case it investigates knowledge as an empirical phenomenon by methods similar to those of empirical science.

In speaking of an epistemological \emph{synthesis}, I am offering to contribute to the ongoing evolution of epistemology, in part through departures from what empirical observation or conceptual analysis may reveal of its past or present.

To that end I begin with the suggestion that knowledge consists in representations or models, of some other thing or system, which satisfy relevant criterion of adequacy or accuracy appropriate to their role or purpose.
The representations might be considered linguistic, but linguists have sufficiently rigorous conceptions of what a language is, that a long history of phenomena which are intended to fall under my definition will not involve language as it is construed by linguists.
One example of this is the consideration of a molecule of DNA as a representation of the chemical structure of a set of proteins which are needed for sustaining life in some particular biological species.

Some other terminology for those non-linguistic representations might be appropriate, perhaps in general one might talk of them as `codings', hoping that knowledge represented using \emph{bona-fide} languages would fall under that more copious umbrella.

Just as the linguists conception of language does not suit my purpose, my synthesis does involve stretching the idea of \emph{knowledge} beyond what is likely to be generally acceptable, and for that and other reasons which will become clearer in due course, the epistemological synthesis will to a large extent sideline the perhaps hitherto crucial questions about what exactly knowledge is and how it differs from related ideas such as true belief.
Under the propose conception, knowledge need not be a belief at all, and the binary distinction between knowledge and true belief will go the same way as the distinction between `hot' and `cold', to be replaced by other more exact measures of relevant characteristics.
Confirmation theory is one way in which philosophers have attempted to place metrics on confidence in a scientific conjecture, though this will not feature in my synthesis.







\chapter{Introduction}

In the main current of Western philosophical thought, which is often spoken of as beginning in Ionia with Thales around 600 BC, we find an idealisation of declarative language which is remarked upon by Isaiah Berlin in his writings on \emph{the roots of romanticism} \cite{berlinRR} as extant before The Enlightenment, qualified by thay `age of reason' and then challenged by romanticism and its pre-cursors.

Two principle elements of this were, (among three mentioned by Berlin), that all genuine questions have answers (which can be discovered), and that all true answers to those genuine questions are logically compatible.
I will later talk about a space where these conditions are realised, but this is a tiny part of that very large domain in which 



\chapter{Introduction}

\emph{Epistemology} is the philosophical discipline concerned with the theory of \emph{knowledge}.

In this monograph I will take a broad view of what knowledge and epistemology are.
This conception of knowledge embraces any way in which one thing is represented or modelled by some other thing (usually in an incomplete and/or approximate way).

In order for one thing to represent or model another, there must a correlation of some kind between the states of these distinct entities, perhaps but not necessarily mediated by a causal connection.
So we are not here talking of a single fixed structure, for there to be a correlation we must either be talking about a type of entities correlated with some other type of entities, or at least of a single entity whose state varies in a way which correlates with variations in some other entity or system.
It is this correlation that gives meaning to the representatives or models, and this connection with meanings suggests a broad conception of language and a way of thinking of knowledge in general as linguistic in character.

A broad conception of knowledge may in this way be associated with a similarly broad conception of language.
Doing so does some violence to the meanings hitherto associated with the terms `knowledge' and `language'.
Central concerns of epistemology for most of its history include the distinction between belief and knowledge, the question of how we can know, and indeed whether knowledge is possible.
Though the expansion of academia through the 20th Century resulted in much greater variety in epistemological research, the question whether knowledge could be characterised as justified true belief could still generate controversy.
Similarly, in modern times, many if not most linguists do not regard the ways in which organisms prior to homo sapiens might communicate or constructively collaborate to constitute languages.

It might therefore be appropriate for the broad conceptions of knowldge and of language which I will be addressing to adopt a more cautious nomenclature such as proto-language or proto-knowledge.
I believe that would be cumbersome and ineffective in disarming terminological critique, and will therefore rest exclusively on being as clear as I can about the language I am using, without proliferating too many neologisms in the process.

Some terms I will deliberately use in a broad and imprecise way, in a manner similar to that in which a scientist might use the terms `hot' or `cold' alongside more precise observations or prescriptions of temperature.

Knowledge, language and epistemology are among those terms which I will stretch convention in imprecise ways, but some related vocabulary will in particular contexts be given more precise meaning.

\chapter{Introduction}

\emph{Epistemology} is the philosophical discipline concerned with the theory of knowledge.

For most of its history, epistemology has had an anthropocentric focus, concerned primarily with the kinds of knowledge possessed by human beings with well founded true beliefs, and with the question whether these ever do amount to knowledge.
With the expansion of academia over the last century, epistemological research has broadened considerably, and the epistemological synthesis here offered is another perspective on how that might be done.

From this perspective knowledge is concerned with representation or modelling.
In this it is closely connected with a very broad conception of the idea of \emph{declarative language}.
The broad conception of knowledge which we seek encompasses a similarly broad conception of language sufficient for us to interpret all knowledge as captured by language.

Sometimes a structure (physical or abstract) conveys information about some other structure which, to a greater or lesser extent, is accurate or useful.
In order for this to happen there must be some correlation between the state of the representation and that of the system about which it is representative.
One way in which that correlation might happen is through a causal relationship between the structure and the representation.
Perception is such a causal relationship.
One way in which it might be useful is through a causal relationship between the representation and some action upon the thing represented.
An organism reaches out to capture and consume some nourishment, an action possible only because of the knowledge of its environment mediated by its sense organs, and transiently represented by intermediate states of its nervous system.

The distinction between abstract and concrete entities, a metaphysical distinction, turns out to be important to this enterprise.
When we talk about knowledge, we can talk of abstract representation systems, which can also be understood as representing other abstract structures.
But to possess knowledge, we must have it in concrete form.
The abstract structures might be infinite in size, but a concrete representation will be finite (though not \emph{necessarily} so!).


  








It is not certain that a theory of knowledge can be entirely objective in character, insofar as the attribution of knowledge is a mark of social approval not only intended to signal veracity, but often also a substantial level of justified confidence in that veracity.
As well as this question of veracity, there may be a broader concern with utility, which may attract social approbation even where strict veracity is known to fail.
An example here is the very useful knowledge conferred by Newton's laws of motion and of gravitation despite the present consensus that they are false.

This monograph is intended to offer ways of thinking and talking about knowledge (and of gathering, storing, sharing, and exploiting knowledge)  which may have advantages, and which therefore I hope deserve consideration for the future, in those contexts where the veracity and utility of knowledge are of particular importance.

There are many phenomena which may be considered exemplars of the concept of knowledge in its broadest sense.
The nature of these exemplars has evolved over the last four million years in the context of planet Earth, and most of the knowledge we now benefit from only exists in consequence of the evolution of intelligent life on earth, and of the artefacts made possible by the accumulation of progressively more sophisticated forms of knowledge in the cultures of homo sapiens.

\section*{About}

This book attempts an epistemological synthesis.

I think of this as the kernel for a complete philosophical system which is perhaps easiest understood as a 21st century successor to Logical Positivism, particularly as exemplified by the philosophy of Rudolf Carnap, whose central project was to do for science what he supposed Russell and Whitehead had done for mathematics in their \emph{Principia Mathematica} \cite{russell1913} and to progress a conception of scientific philosophy articulated by Russell in his book ``\emph{Our Knowledge of the External World, as a Field For Scientific Method in Philosophy}.'' \cite{russell21}
That achievement was to realise Frege’s conception of mathematics as the result of deductive reasoning from the definitions of the relevant mathematical concepts.
I will gloss over for the moment the controversy about whether this really was achieved, and the question how any such achievement could be extended to empirical sciences.

Carnap’s conception of philosophy was exclusively as ``logical analysis'' which he construed in terms of the new logical systems introduced by Frege and Russell, and he consequently regarded broad swathes of philosophy as it was and had been practiced as meaningless, particularly the whole of that most fundamental of philosophical disciplines, metaphysics.
There are many differences in my own philosophical perspective, and I am disinclined to police the boundaries of the discipline, but in important ways I share Carnap’s standpoint, and one of those concerns the importance of logical rigour, which depends upon clarity of language and demands strict conditions on the formulation of problems before the questions they raise can be considered meaningful.

Carnap’s concern for precision and clarity place language at the centre of his philosophical system, whereas I have chosen to place epistemology in that place, while addressing those kinds of knowledge which are dependent upon language.
I acknowledge the foundational interplay between all four most theoretical branches of philosophy, metaphysics, philosophy of language, logic and epistemology.
Despite my own sense of proximity to Carnap and the vehement rejection of metaphysics which he maintained throughout his life, I am relaxed about metaphysics and see in Carnap’s philosophy the roots of my own more accommodating stance.

Epistemology is that branch of philosophy which is concerned with the theory of knowledge.
In the last century there has been controversy about whether epistemology should be a part of some philosophical prelude to science, possibly called “first philosophy” (following Aristotle), and therefore conducted \emph{a priori}, or be a part of empirical science, “epistemology naturalised”.
Two developments seem to me to have shaken that dichotomy.
One is Postmodern philosophy, which denied the possibility of objective knowledge (except where politically convenient), and construed “episteme” as a cynical tool for exploitation (and hence as a matter of choice).
Another is the advance of information technology towards the engineering of intelligent artefacts.  The latter development made the representation of knowledge, and hence potentially choices about what knowledge is into an aspect in the design of synthetic cognitive systems (though most contemporary AI research does not do this).

Notwithstanding debate about how AI should be achieved, the aspiration to a systematic coherent repository of scientific knowledge, which is centuries old, is likely to be advanced with the aid of intelligent machinery, and questions about the best way to organise such a repository will, at the most abstract levels, be matters into which philosophical considerations enter.

Though the central thrust of the proposed work is an epistemological synthesis, this is a projection into the future of a process of epistemological evolution without which the proposed synthesis would be unthinkable.  An understanding of that history is an exercise in natural epistemology and a very selective account bringing out the key features which are important to making, understanding and reasoning about the proposed synthesis will be the subject matter of the first part of the book.


\chapter{Introduction}


\section{A}

At the heart of this monograph is the idea of \emph{deductive inference} and the closely related ideas of \emph{entailment} and \emph{logical truth}.

Deductive inference is best defined as a feature of propositional language.
A proposition is a claim about some subject matter which may be true or false, depending on the characteristics of that subject matter which are expressed by the proposition.
In this way a proposition divides the possibilities relating to that subject matter into two groups, those in which the proposition is true and those in which it is false.

Propositional language is that kind of language in which propositions can be expressed, usually by structures called \emph{sentences}.
In such languages it is sometimes possible to make deductive inferences, which allow us to infer from the truth of one proposition the truth of some other semantically related proposition.
Semantics here is that aspect of the meaning of sentences which concerns the truth conditions of the propositions expressed by the sentences.

The details of how a sentence expresses a proposition may be quite complex, the meaning of the language and various aspects of the context in which the sentence is asserted are likely to influence exactly what proposition is expressed and the resulting truth conditions.

Entailment is the relation between two propositions A and B which obtains when the truth conditions of B subsume those of A, i.e. when B is true for all those possibilities in which the A is true.
In this case we say that A \emph{entails} B and we may deduce (deductively infer) B from A.
This scheme can be extended to inferences made from multiple premises.
If the truth conditions of some proposition P subsume those of collection or set of propositions SP, i.e. if P is true in every possibility for which every member of SP is true, then we say that the set of propositions SP entails the proposition P, and the inference from SP to P is deductively sound.

In some cases a proposition may be entailed by the empty collection of propositions.
That would be the case in which the proposition is true in every possibility (for every possibility satisfies the empty set of propositions).

\section{B}

The purpose of this monograph is to present a case supporting the use of a particular \emph{logical foundation system} as a framework within which to organise a comprehensive body of declarative or propositional knowledge.

This is presented as a natural continuation of the historical development of our knowledge about:
\begin{itemize}
\item the foundations of logic, and
\item the technology for information processing and its applications to problems which require human levels of intelligence and super-human capabilitites in brute processing power, algorithmic reliability and accurate total recall.
\end{itemize}

It is not intended that this monograph progress the technical issues involved.
Established knowledge of these matters suffices for my purposes.
My aim will be to limit the detail presented to such as may be needed to make the presentation intelligible, with references to fuller details for those who seek them.
Consequently, this tract is substantially philosophical in character.
The philosophy is oriented towards explaining and supporting what is in effect an architectural proposal for a  logically structured intragalactic shared distributed body of declarative knowledge (though few of the ideas depend upon the suggestion of scale given here).

\section{Prospectus}

At the heart of this monograph is a specific system which I describe as a \emph{quasi universal logical foundation} around which an epistemological architecture is constructed.

The presentation of this foundational system is given in the first instance through an historical sketch in which aims to introduce key features of the system as they evolved.
In Chapter \ref{ChapDeduction} the discussion takes us from the beginnings of language and informal deductive reasoning through the first systematic applications of deduction to the development of formal deductive systems.
In Chapter \ref{ChapLFS} the idea of a \emph{logical foundation system} is introduced, and single line of development is traced through to the present day.
The idea of semantic reduction is introduced, whereby one language may be interpreted in another, leading to the concept of \emph{quasi universal logical foundation system}, together with some reasons to suppose that the chosen system is quasi universal.
In Chapter \ref{ChapFRS}
In Chapter \ref{ChapKTDL}

\chapter{Introduction}

The prosperity and well-being of humanity is largely attributable to the large brain which evolved over 3 Billion years of life on earth.
It took a while to pay off.
For most of the 300 thousand years since modern humans evolved the main benefit of that large brain was to enable humans to migrate across the globe and survive in almost every available habitat, however far removed from the conditions in Africa where we first appeared.
The brain gave us adaptability, made it possible for us to find ways to survive.
Cultural evolution, which was facilitated by those large brains, and by the linguistic capabilities which co-evolved with them, made it possible for humans to find, refine and preserve the tools and other resources needed to migrate around and settle throughout planet earth.

It was not until the advent of modern science, and the industrial revolution which followed, that the average human was lifted much above subsistance living, just a few hundred years ago.
The modern science upon which the industrial revolution depended was numerical, and therefore required a well developed discipline of mathematics.
Mathematics had been established as a deductive science some two millenia earlier in ancient Greece, the rigorous deductive methods codified by Euclid in his compilation of the Elements \cite{euclidEL1}.
At this time, the theoretical discipline of mathematics was distinct from the useful work which we might now consider applications of mathematics, which were known by the Greeks as \emph{logistikē}.
The theoretical discipline, along with the whole of what Aristotle referred to as theoretical science, was regarded by Aristotle as superior to logistikē because it was undertaken purely for the love of knowledge (which was the meaning of the term \emph{philosophy}).

was later described as using \emph{hypothetico-deductive}\footnote{In which a hypothesis is adopted and its consequences for some experimental or observational context are inferred and checked against what is observed to see whether the hypothesis conforms to reality.} and/or \emph{nomologico-deductive}\footnote{In which an established law of nature is applied to a problem by deducing from it, together with the particulars of the problem, a forecast for the behaviour to be expected.} methods, in which general scientific laws (of which the most influential and fertile were Newton's laws of motion and of gravitation) are tested or applied (respectively) by \emph{deductive} inference.

The industrial revolution was a child of The Enlightenment, which was a high point in the status of reason.
What we would now describe as a peak in the \emph{hype-cycle}, when the expectations of what could be achieved by modern science were at their most unrealistic, neither tempered by any sense of limits on what phenomena would prove predictable nor by much appreciation of conflict between scientistic social engineering and democratic governance.

Notwithstanding the hubris of the age, it was on the cusp of major improvements to the human condition, of an acceleration in the rate of beneficial change the pace of which continues to advance.
Though the conception of \emph{reason} on which this was predicated was not purely deductive, embracing the whole of the then new modern empirical scientific method, deduction plays a fundamental role in both the verification of scientific hypotheses and in the applications to engineering design which drove forward the industrial revolution and its sequel to the modern day.

Despite its fundamental role, deduction was not then well understood, and major developments in understanding the nature and scope of deduction which are crucial to its further exploitation were yet to come.
It is the purpose of this monograph to contribute to an understanding of how deduction can best be exploited in a future transformed by intelligent machinery, and to that end a sketch of the development of deductive reason and its applications will help us understand the opportunities which are now open to us.

The intellectual importance of language to humanity, and the vast gulf which separates human competence with language from our evolutionary predecessors make it unlikely that language could have arisen subsequent to the evolution of modern humans.
The overwhelming probability is that linguistic capabilites were an important if not predominant part of the advantages which drove the rapid expansion in brain size during the evolution of hominins.

\chapter{Introduction}

Though enthusiasts for Artificial Intelligence anticipate that intelligence \emph{in silico} will surpass human intelligence, the emphasis has seemed to be substantially upon \emph{replicating} human intelligence, predominantly by methods which mimic the anatomy of human intelligence, notably ``neural nets''.

Typically it does result in overshoot.
AI is often presented as superior in those areas which it has mastered, beating grand masters at chess and world champion go players.
But I hear few aspirations for doing things in novel and potentially superior ways enabled by Artificial Intelligence and building artificial to 

\chapter{Introduction}

I had at first intended that this monograph, which is in essence a perspective on the logical structure of knowledge, would fall into two parts.
The central message is futuristic.
To understand the ideas and their supporting rationale it is necessary to understand the history leading to them.
Though knowledge may be thought of as a peculiarly human phenomenon, the history of knowledge and its predecessors stretches back a long way, perhaps even beyond the origins of life on Earth, billions of years ago.
That history is best understood through an evolutionary lense.
One thread in the story, for example, is the history of memory.\footnote{Examined through an evolutionary lense by Murray et. al. \cite{murray2017evolution}.}
Memory of course, must be memory \emph{of something}, however imperfect, and to the extent that it is accurate it might reasonably be construed as \emph{knowledge}, no matter how primitive the animal whose nervous system captured and preserved it.

My first intention had been to separate my presentation into one part covering the relevant history, and a second which looking to the future from that base.
This would leave the moment of truth far too late, and I have concluded that a completely different approach is essential.

This introductory chapter is the first fruit of my change of heart, in which I begin with the simplest presentation of the most threadbare history from which an elementary account of my synthesis can be based.
For some readers that will be more than enough, but at least before we part ways they will perhaps have had some inkling of what I am about, rather than tasting only preliminary histories which contain nothing of the ideas which they are intended to support.

\section{Hume's Two Forks}

In the eighteenth century, during ``The Enlightennent'', David Hume, philosopher and historian, made two important epistemological distinctions.

The first, took centre place in his ``An Enquiry Concerning Human Understanding'' \cite{hume48}, which was his condensation of what he thought most important in his prior \emph{Treatise} \cite{hume39}.
In his words, the distinction he drew was between propositions expressing ``relations between ideas'' and those expressing ``matters of fact''.
The first of these is that same category which was later to be embraced in the philosophy of Rudolf Carnap by the word 'analytic', and phrase 'logical truth', a category which had a fundamental importance in his philosophy and will be equally if not more important here.
Hume's ``matters of fact'' correspond to Carnap's ``synthetic'' 

\chapter{Introduction}

We are in interesting times, in many different respects.

The phenomenon of greatest interest in provoking the ideas presented here is the imminence of intelligent machines, and the possibility that these machines will enable a transformation in the ways in which knowledge is gathered and applied.
The interest to me is not so much in how intelligent machines might be devised which replicate those human capabilities which we consider to demand or exhibit intelligence, but in the prospect that information processing technology might facilitate the adoption of better ways of gathering and exploiting knowledge.
The formal basis for the proposed methods is known but is too cumbersome to be widely applicable without considerable advances in the available automation.

I was born into a world which anticipated machine intelligence, but had a limited understanding of how that might be achieved, and of machines which fell short of the brute compute power which would be needed. 
My own interest in the problem dates back to my first days as an undergraduate (though it was not a subject which undergraduates could then study).
It was then that I first learned a little of the work of Alan Turing, a famous logicism who subsequently exerted subtantial influence over the field of artificial intelligence (in ways which he probably did not intend).

That first spell as an undergraduate student of engineering proved short lived, and years later I returned to academic studies to obtain a BA in mathematics and philosophy.
My taking up of philosophy was a surprise to me.
Philosophy struck a deep chord, and gave me a sense of vocation not diminished by my perception that I was completely unsuited to progressing an academic career in philosophy.
This was the beginning of a lifetime of philosophical rumination, and a more or less continuous search for a way to turn those ruminations into substance.

My interests in formal logic, computer languages, artificial intelligence and under it all, philosophy, all date from that time.
They coloured my thinking many years later when I had the opportunity to work with the Cambridge HOL system, then recently developed by Mike Gordon and his embryonic Hardware Verification Group at Cambridge University.

The epistemological synthesis which I present below was seeded by the perception that the Cambridge HOL system possessed many of the desirable characteristics of a representation system for knowledge suitable for use in the kind Leibnizian project for the mechanisation of scientific knowledge which the advent of machine intelligence demanded and would facilitate.

The synthesis is \emph{foundational} in multiple senses, and appropriately for an epistemological enterprise, it digs deep in its response to the kind of academic scepticism to which it is inevitably subject.
Though not so intricate as G.E.Moore's defence of common sense \cite{moore1925,moore1993}, it is in its way similarly grounded, not in the good sense of the common man, but in that of the practical engineer.
Because of its inspiration in (rather exotic) engineering practices, my account begins there, before before starting afresh from a philosophical ground zero.

\section{Purpose and Structure}

\section{Varieties of Epistemology}

This monograph comes in two parts, the first of which is presented as \emph{natural} epistemology and the second as an epistemological \emph{synthesis}.
These are two distinct kinds of epistemology of which the former provides background intended to make the latter intelligible, and identifies particular epistemologically relevant developments upon which the following synthesis is built.

Epistemology is viewed through an evolutionary lens, the synthesis constituting a projection of the evolution of knowledge and epistemology into the future.
The presentations of epistemology and of evolution are reciprocal,
evolution is seen through an epistemological lens, as a progressive accumulation of various kinds of knowledge.
It is not a mere aggregation, the kinds of knowledge gathered progress with the evolution of the cognitive systems involved.

 We are at a point of accelerating advancement (an inflexion point) in the evolution of evolution itself.
Genetic engineering, synthetic biology, artificial intelligence and the prospect of self reproducing systems of artefacts, or even of artificial life, all promise to transform and accelerate evolution.
It is natural if we see evolution, knowledge and epistemology as co-evolving phenomena, to expect similarly radical transformations in our conception of knowledge and our understanding of how it can be gathered, assessed, managed, shared and applied.

Epistemology has hitherto been largely anthropocentric.
It is defined by what philosophers take to be the meaning of the concept of knowledge and related concepts in various of the natural languages spoken by \emph{homo sapiens} (mostly in English).
Idealised conceptions of knowledge arising in that way have centered around the idea that knowledge is \emph{justified true belief}, if only by distancing themselves from that idea.
We have concepts arising from the evolution of life on earth which are psychological or mental in character, essentially linked to human nature.

The evolutionary perspective I adopt here takes us back beyond the beginnings of life on earth, and forward to a time at which most knowledge will be gathered and applied by evolutionary successors to homo sapiens or by inorganic or hybrid systems.
Conceptions of knowledge and hence of epistemology which are decoupled both from human nature and human language are necessary to support a credible projection beyond the geographic and temporal scope of humanity.

\subsection{Knowledge, Episteme and Epistemology}

Once significant bodies of knowledge became a part of shared cultures, the evolution of culture intertwined with that of knowledge and epistemology.
From the earliest days of philosophical reflection on the nature of knowledge, cultural relativism, the observation that distinct cultures disagree about what is and is not true, has been an important source of scepticism about the objectivity of claims to knowledge.
A first exemplar of cultural relativism is found in the sophists who taught philosophy in Athens at the time of Socrates.

A modern philosophy involving such cultural influence in science is found in the idea of a \emph{scientific paradigm} within which `normal science' is conducted, and the idea that such paradigms are overthrown only by infrequent scientific revolutions.\footnote{These ideas are most closely associated with Thomas Kuhn \cite{kuhn2012structure,kuhn2000structure}.}
Because of its specificity to science the scientific paradigm may possibly be a way to think of the epistemological synthesis presented here, which is rooted in logic and mathematics.

Another kind of cultural relativism became influential through the writings of the French postmodern philosopher Michel Foucault \cite{foucault1966order}.
Though he rejected the attribution of cultural relativism, Foucault gave the name \emph{épistémè}, a French derivative of the ancient Greek word for \emph{knowledge}, to those aspects of a culture or subculture which determine what is accepted as knowledge.
In Foucault's hands hegemonic cultures establish and entrench cultural norms cynically devised by dominant groups for the purpose of maintaining an oppressive dominance over less privileged social groups.

In the following I will adopt Foucault's term for the subject matter (if not the entire content) of my epistemological synthesis, dropping the accentuation in consideration of its tenuous correspondence with Foucault's ideas (against which it may be considered a counter).

An episteme is not itself knowledge, but is \emph{about} knowledge, and hence a step towards a metatheory of knowledge, without constituting a full blooded theory of knowledge.
The epistemological synthesis which follows is intended to constitute a proposal for a kind of episteme, supported by sufficient philosophical metatheory to reasonably be offered as epistemology.

\subsection{Precursors of Knowledge}

The progress of science depends upon the refinement of language as necessary to provide the concepts upon which the statement of natural laws depend.
Many concepts belonging to  mathematics are among those necessary for empirical science, but other necessary innovations include the names of structures too small to be perceived by the naked eye, and hence not spoken of in pre-scientific language, and scientific refinement of concepts present in ordinary discourse but spoken of with insufficient precision.
A simple example is the language around heat, which in pre-scientific language may be hot, warm, tepid, cool or cold, concepts inadequate for expressing the kinds of laws which govern the behaviour of gasses or the performance of thermodynamic systems.
For these purposes numeric temperature scales are essential, for which the scientist has to create new terminology not derived from everyday language.

In the study of that natural phenomenon which we speak of as knowledge, philosophers may be reluctant to look beyond the normal scope of knowledge attributions, but science may find that the most enlightening theories may address phenomena of broader extent and demand new terminology.

One motivator for broadening the ordinary concept is the evolution of memory in our ancestors.
A memory one naturally expects, is a memory \emph{of something}, perhaps something observed, and serves to bring back into consciousness something similar to the experience of which it is a memory.


In extending the idea of knowledge beyond the bounds of human cognition, a first place to consider is the nature of memory and its role in those parts of the animal kingdom which benefit from a central nervous system.

\ignore{
The following concepts are useful in characterising the kinds of knowledge and its pre-cursors which become a significant part of the evolutionary history beyond its particular manifestations in homo sapiens.

\begin{itemize}
\item information
\item representation, model
\item declarative knowledge
\item episteme
\item epistemology
\end{itemize}

}%ignore

\subsection{Epistemologies Compared}

Some brief comparative observations may help to place the kinds of epistemology which is intended.

\paragraph{The Western Tradition}

The ``Western'' philosophical tradition is normally held to have begun in ancient Greece at the same time as the initial development of mathematics as a theoretical discipline, which over a period of about 300 years from 600 BC established the axiomatic method and applied it most notably to Euclidean Geometry but also to other parts of mathematics which were gathered and perfected by Euclid in his ``Elements'' \cite{euclidL1}.
A distinctive feature of the mathematical developments was their employment of deductive reason in axiomatic theories, which was part of a broader investigation of nature and the cosmos by reason.

The use of reason beyond mathematics was less successful, failing to secure consensus or to yield results which could stand the test of time, and readily capable of obtaining contradictory results which formed a part of the resulting controversy.

Despite the failures of reason beyond mathematics, philosophers (a designation at the time which encompassed the science of the day) sought the imprimateur of deductive proof for their theories, and Plato and Aristotle both attempted syntheses which made a place for deductive reason beyond mathematics.

Plato's system was 

\paragraph{Evolutionary Epistemology}

\paragraph{Scientific Paradigms}

\paragraph{Foucauldian Episteme's}

\paragraph{Epistemology Naturalised}

\subsection{Epistemology as Cognitive Engineering}

\section{The Application of Science}

It is my aim here to present an \emph{episteme}, which is to say, a conception of a certain kind of knowledge, of how to maintain and apply that kind of knowledge, and to offer various epistemological theses about that episteme which speak to its suitability for the development and application of science and the conduct of commerce and a broad range of other aspects of intelligent life in the 21st Century and beyond.

The presentation will include a number of historical sketches showing the features of the  episteme as natural continuations of well established progressions, of which this is the first lightweight sketch.

The emphasis is here on the application of science, which of course depends upon the nature of that science and evolves with it.
Three stages are considered in which substantial advances in science and in scientific methods were realised which are relevant to the further developments which are embraced by the proposed synthesis.

Those periods are:


\begin{itemize}

\item[600-300 BC]

  The Ionian philosopher Thales (624–546 BC) is generally credited as the first to 

  This is the period from the beginnings of Greek Mathematics and Philosophy in the work of Thales through to the philosophy of Aristotle with his syllogistic conception of demonstrative science and the consolidation and axiomatic systematisation of Greek mathematics in the Elements of Euclid.

\item[1473-1779 AD] The beginnings of modern empirical science beginning with Copernicus through the revolutionary quantitative laws of motion of Newton and the infinitesimal calculus of Newton and Leibniz (which paved the way for the industrial revolution) and into the Enlightenment and the French Revolution.

\item[1800-2100 AD] The period of modern logic, information technology and artificial intelligence which make possible the transition to a conception of logic

\end{itemize}



\subsection{Pre-theoretical Mathematics}

It is generally recognised that before the initiation of axiomatic geometry by the ancient Greeks in the sixth century BC, mathematics primarily consisted in a body of arithmetic and geometrical technique developed by the Babylonian and Egyptian civilisations.

It was thus primarily a body of technique for solving a variety of practical problems, knowledge of \emph{how} to do things rather than knowledge of a body of mathematical truths.

\subsection{Mathematics as a Theoretical Discipline}

It is with the earliest Greek geometers, beginning with Thales and Pythagoras, that a different kind of mathematics was created.
Mathematics became a theoretical discipline building a body of propositional knowledge by the systematic deductive method eventually to be documented in Euclid's \emph{Elements} \cite{euclidEL1}.

The axiomatic method articulated by Euclid was the product of his own systematisation of perhaps 250 years of progress by earlier Greek mathematicians.
As such, it continued a progression in the composition of comprehensive texts was found in works by Hippocrates of Chios, Leon, and Theudius of Magnesia, the latter of which, compiled in Plato's Academy, was probably the standard text for Aristotle's peripatetic school.
Aristotle give many mathematical examples in his writing which are now taken to be from the elements of Theodius and provide our best source of knowledge about the innovations found in Euclid's elements.

From this history we may speculate that though the earliest work of Thales was deductive, the necessary principles for properly organising and conducting were but gradually refined and articulated over that 250 year period, and though the 



This new theoretical approach was to dominate the development of mathematics thenceforth, with varying standards of rigour which we will further discuss.

In the earliest days, the strict features of the axiomatic method were not yet clearly delineated, but 

\subsection{Plato and Aristotle}

\subsection{Empirical Quantitative Science}

During the Renaissance, after a period of revival of interest in the works of classucal antiquity modern science gradually moved ahead of that tradition.
It was during this period that science became unambiguously empirical.
Twentieth century philosophers of science have used the terms ``hypothetico-deductive'' and ``nomologico-deductive'' to describe the emerging model of science.
The former term concerns an approach to testing scientific hypotheses, which consists in deducing particular consequences of a proposed general hypothesis and examining whether those consequences correspond to observed phenonena.
The latter speaks to the application of established scientific laws which proceeds in a similar manner, enabling the prediction of phenomena by deduction from the laws and the particular circumstances of interest.

In this period we see a progression from geometrical to numerical cosmological theories takes place, as we move from geocentric models of the cosmos based on the conception of orbital trajectories compounded from perfect circles with so called ``epicycles', through the similar heliocentric conception of Copernicus, Kepler's more quantitative models involving elliptical orbits (all of which are descriptive rather than explanatory) and ultimately to Newton's theories of motion and gravitation which enabled the motions of planets to be explained in terms of the underlying forces which determine those motions.

The explanation of planetary motions (and many other natural phenomena) provided by Newton's theories fall fully within the nomologico-deductive analysis of later philosophers, and differs from the earlier Aristotelian conception of \emph{demonstrative science} (though the establishment of the natural laws is more systematically empirical than Aristotle had envisaged).



\subsection{The Impact and Application of Information Technology}

Already when the theories of Newton were formulated.



\part{Natural Epistemology}



\chapter{Culture}
\subsection{Epistemology and Rationality}

The philosophical `historian of ideas' Isaiah Berlin, in talking of the roots of romanticism \cite{berlinRR}, gives the following characterisation of 'the Western Tradition' as it stood before the enlightenment of the late 17th/18th Century.

\begin{quotation}

First, ``three legs upon which the whole Western tradition rested'':
\begin{enumerate}
\item All genuine questions have answers.
\item The answers are knowable.  By someone.
\item All the answers are compatible.
\end{enumerate}
\end{quotation}

The extra twist added by the enlightenment as the epistemological dominance of the Catholic Church gradually waned, was that reason rather than revelation, scripture or any other authority, is the method by which these answers should be sought.

This is a remarkable distillation of essence in a period of considerable intellectual diversity,\footnote{For an equally controversial perspective drawing out that diversity, the work of Jonathan Israel is valuable, e.g. \cite{israel2002radical, israel2013democratic}}%
 but is nevertheless a valuable perspective, if only as an aunt sally from which we can learn by trying to understand both its merits and its limitations.

 For the story I have to tell here, it is the first and last of those `legs' which are important, it does no damage to the ideas presented here if some questions cannot be answered.
 The restriction to `genuine' questions is clearly essential, nonsense questions are easy to find, but there is here no hint of how one is to distinguish those genuine questions which have answers from the others.
That true answers to `genuine' questions are logically compatible is closely related to the Law of Contradiction usually attributed to Aristotle, and is essential to the application of deductive reasoning.

 
 This idealisation can be seen to have originated as far back as the ancient Greeks, where its weaknesses had already been exposed, and partially addressed.
It is a picture which holds good in the narrow domains of logic and mathematics, in which not only the theory but also the practice have been substantially realised, and in which the need for logical coherence, and hence the compatibility of all conclusions, is essential. 
In consequence of the achievement in those special domains,  philosophers have sought, unsuccessfully, to apply the deductive (axiomatic) method to other domains, with equivocal results.

In about 600 BC the development of mathematics as a theoretical discipline, and the broader speculations of the pre-socratic philosophers began.
The successes of what we now call \emph{deductive} reason in mathematics, ultimately resulting in a comprehensive articulation of the axiomatic method and the compilation of hundreds of years of solid mathematical progress in Euclid's Elements \cite{euclidEL1}, substantially complied with the ideal which Berlin spoke of above.
But the application of reason in other domains failed, most conspicuously in yielding a consensus on any coherent body of doctrine, even involving the exploitation of contradictory conclusions in demonstrations of absurdities such as the impossibility of motion.

The period of participatory democracy in Athens placed a premium on success in public debating which spawned a class of professional philosophers who earning a living by teaching the oratorical and logical skills.
These were known as `sophists' a term which continues to have derogatory connotations, and among their number were prominent figures whose experience of different cultures made them sceptical of objective truth and adherents of forms of relativism, most conspicuously Protagoras whose most famous dictum is probably ``man is the measure of all things''.

The great classical philosophers, Socrates, Plato and Aristotle, contructed progressively sophisticated systems in response to these precedents.

\part{Synthetic Epistemology}

\chapter{Setting the Scene}

My aim in this part of my monograph is to project the evolution of kneowledge, episteme and epistemology into the future by the description of an episteme, together with philosophical arguments presenting a supporting epistemological synthesis.

To an extent the synthesis stands on its merits in the context of the present, but its fullest support includes elements resting on projections as to the nature of the universe as it unfolds into the future.
In this chapter I present those projections, not of episteme or epistemology, but of the relevant aspects of the future they are intended to serve.
Later chapters will present the episteme and epistemology.

\section{The Future of Evolution}

Technologies are already in place which enable fundamental changes to the way in which biological evolution takes place.
The core mantras of Darwin's account of the evolution of species, random variation and natural selection, need no longer obtain.
Genetic engineering and synthetic biology give us evolution by design.

As well as the accelerated evolution of the intelligent life on earth by these new means, we are now entering the era of intelligent artefacts and likely to see increasing integration of biology and technology.
Our intelligent interventions will not be confined to the transformation of existing organisms, or creation \emph{de novo}.

The direction and speed of future evolution, both of natural and artificial life and intelligence is likely to be further stimulated by the migration of human progeny (both natural and technological) across the solar system and through the galaxy, leading to environmental pressures very different to anything found on earth.

\section{General Artificial Intelligence}

The current bleeding edge of AI is generative Large Language Models, which have shown unanticipated ``emergent'' capabilities which before close scrutiny might lead you to think that the general intelligence sought is almost upon us.
Despite impressive capabilities, it is easy to discover how shallow their understanding of the materials they have mastered is.
Some imagine that by pressing forward this line of technological development genuine intelligence will be realised, but at the same time a great diversity of effort is being undertaken to improve the capabilities in a variety of ways while staying broadly within the paradigm of training neural nets with very large volumes of data.

Google Deep Mind, initially one of the staunchest advocates of neural learning by watching (particularly in games) has made its most impressive recent advances by breaking from that paradigm, in a series of programs sporting the ``alpha'' prefix.
The capabilities of interest here are ones which involve problem solving in simple but combinatorially explosive solution spaces.

\chapter{A Priori Truth}

This part is devoted to defining certain concepts culminating in the idea of a \emph{Universal Foundation for Logical Truth} and to two such foundation systems, first order set theory and the Cambridge HOL logic, which are shown to be equivalent as universal foundations.

As normally presented these logical systems are not equivalent.
There are more logical truths (in the sense defined below) expressible in the standard interpretation of the ZFC axioms for set theory than in the Cambridge HOL logic.
But the notion of Universal Foundation we expound consists in an unbounded sequence of increasingly expressive systems distinguished primarily by the cardinality of their minimal ontologies, i.e. in how many things they presume there to be.
The claim to equivalence is the claim that every logical truth which is expressible in some member of one of the the family is also expressible in a member of the other family.

It is a hypothesis that there are no more expressive universal foundation systems, which is treated in a manner similar to Church's thesis.
Rather than being proven, it survives until some convincing counterexample may be found.
There is of course a difficulty in any logical system which abjures semantics, as to whether an interpretation in set theory can be correct.
Formally there is little doubt that in any system formally consistent, if only in the sense of Post, can be given an interpretation in these universal foundations which is based on identifying truth with provability, but whether this would be philosophically acceptable to those who prefer constructive foundations is moot.

In defining these ideas an important element is the choice of reduction, particularly in relation to semantics and hence logical truth.
Two formal languages are considered equivalent if they a mutually interpretable by functions which preserve meaning, so a relevant notion of meaning must be articulated.

Once the definitions are complete there are two directions of further discussion, which flow from the distinct roles envisaged for the two Universal Foundations which have been chosen to exemplify the concept.

The Cambridge HOL logic is chosen because there is a natural way in which we can define the abstract semantics of some arbitrary language and then in practice be able to reason with that language in the resulting context in HOL.
This is a consequence of the abstract syntax of the simply typed lambda calculus being effectively a universal abstract syntax (aided and abetted by the ability to define new type constructors which correspond to syntactic categories in the required abstract syntax).

First order set theory on the other hand, is more convenient for establishing the semantics and proof rules of these universal foundations.
This helps in addressing sceptical arguments from regress in relation to both meaning and proof in the foundations.
In addressing the regress in meaning and justification, careful but ultimately circular arguments prevail.

\chapter{First Philosophy and Foundationalisms}

I am here engaged in something which may reasonably be considered ``First Philosophy'' and which shares something of the motivations and purposes which earlier philosophers have considered under that heading.

That kind of philosophy which us thought of above all else as a rational enterprise must surely first of all put in place the necessary pre-conditions for those kinds of knowledge which are amenable to deductive reason.
I am therefore concerned with \emph{knowing that} rather than \emph{knowing how}.

I think of \emph{knowing that} generally as consisting in the possession of some kind of model of aspects of its subject matter.
Such models or representations may be of particulars states or of patterns of change  in the form of scientific laws. 
The role of deduction is in the application of such models, which may consist in permitting future states to be inferred from present states given knowledge of the laws, or may establish the suitability of some design by demonstration of its behaviour in its intended use.

This kind of application of deduction is often mediated by mathematics, the role of which is to enable the construction of abstract models which can be applied in the above ways.

For these purposes we need knowledge represented by sentences in a declarative knowledge which has a definite semantics, i.e. in which there is a clear interpretation of the sentences of the language as to what they say about the intended subject matter.
Languages which are suitable for expressing this kind of knowledge may also be equipped with detailed syntactic rules which govern sound deductive reasoning with sentences of the language.

\section{On The Nature of First Philosophy}

To give a sense for what I here offer as ``first philosophy'', or of why I find that term appropriate I will say a few words about how ``first philosophy'' has been construed by some earlier philosophers, namely Aristotle, Descartes and Carnap.

\subsection{Aristotle's Conception of First Philosophy}

``First Philosophy'' is the phrase used by Aristotle to describe the subject of his writings which were to be labelled ``Metaphysics'' by later editors.

Aristotle's con

In Aristotle's time, the scope of ``philosophy'', the love of knowledge, embraced the science of his day, and most of Aristotle's work was concerned with the various sciences, which he classified as theoretical if

The term \emph{First Philosophy} was first used by Aristotle, who used it to describe the matter of the volume which later editors named ``Metaphysics''\cite{aristotleMetap}.

Aristotle had a hierarchic conception of knowledge, lower levels concerned with brute facts close to our practical experience of the world, and higher levels involving gradually increased abstraction.
At the highest level, completely divorced from practical applications,and  exhibiting the highest level of abstraction and degree of true wisdom, was \emph{First Philosophy}.
He saw first philosophy as concerned with `being qua being' or being \emph{in  itself}, rather than being \emph{as matter} or being \emph{as mind} or any other kind of being, and thus as a study prior to any of the sciences concerned with their special subject matters.

Descartes, much later, had a distinct emphasis and character to what he offered as first philosophy \cite{descartes2013meditations}, which connects it more directly with the foundational approach to an epistemological synthesis attempted here.
DesCartes wanted to discard all that had preceded him and start a rational reconstruction from scratch, for which first philosophy provided a starting point.

I use it here loosely, rather than in close correspondence with Aristotle's Metaphysics, because it is suggestive that philosophy must begin somewhere, and hopefully progress onward, rather than being a amorphous mass incapable of orderly presentation.

As well as being appropriately presented in a progressive way, my conception of ``First Philosophy'' includes the idea that some aspects of philosophy are also logically prior to others, and indeed, that a presentation which constitutes a logical progression is desirable (in a primordial sense of ``logical'' which is mere suggestion prior to any conception of what logic might be).

It is furthermore, here coupled with notions of foundation, and thus with something like the kind of doctrine which has been called ``foundationalism'', but which lacks some of the absolutism which is commonly associated with that term, and which makes such foundations susceptible to radical scepticism.

I do not seek an epistemology which looks for absolute certainty, but rather for one which may be an effective basis for the future prosperity and well being of humanity.

If I were to launch immediately into the most fundamental elements of this supposed logical progression, the reader might be left in a state of suspense as to how the various sceptical arguments which might be marshalled against it could be answered.
This preliminary discussion is intended to mitigate the suspense.

One modern response to various scepticisms may be found in G.E.Moore's ``A Defence of Common Sense'' \cite{moore1925,moore1993}.

\subsection{Some History of Foundations}

In contrast with Philosophy, Mathematics is that domain of intellectual endeavour which has the very highest reputation for clarity and certainty.
Since the transformation of Mathematics into a theoretical discipline (rather than a collection of practical methods), realised by deductive reasoning, its reputation has been the envy of many philosophers and has inspired a number of attempts to usurp its deductive imprimatur for the sake of methods and doctrines which fall short of deductive rigour.

When we look under the hood we find that reputation has been achieved by ideas which are broadly (but sometimes explicitly) \emph{foundational}, that there have been wide variations in the rigour of the proceedings and that, arguably, the more advanced mathematics has become, the more it has depended upon clearly articulated and elaborately constructed foundations.

The point of this section is to suggest that the interest of mathematicians and some branches of empirical science and engineering in foundational issues is distinct from that of philosophers.
Philosophers are more likely to take foundational studies as intent on achieving absolute precision of meaning and certainty of proof, making of ``foundationalism'' a strawman readily refuted.
A defence against such foundational scepticism might then be analogous to Moore's rejection of sceptical arguments on the basis of the certitudes of common sense.
It might serve such a defence to examine the practice of mathematicians and engineers, for whom foundational advances are a vital part of how their discipline may be progressed.

Sir Thomas Heath observes, in the preface to his history of Greek Mathematics \cite{heath1921}, that the foundations of mathematics are Greek, and consisted in first principles, methods and terminology.
Our knowledge of early Greek mathematics and of the rigour of its demonstrations comes to us primarily in the compilation by Euclid in his elements, which is the culmination of 300 years of Greek mathematics, and probably not representative of standards uniformly adopted \emph{ab initio}, but rather the result of gathering together 2-300 years of the results of work if varied standards of rigour into a single body of work presented to a uniformly high standard of rigour based on the same fundamental principles and methods.

What we see there, and what we will describe in greater detail concerning more modern foundational work, was not a starting point from which all the research was undertaken, but the result of a period of evolution of methods which ultimately could be codified to provide the body of work with a highly rigorous derivation from a single  foundation.

Axiomatic Euclidean geometery was a high water mark in the rigour of mathematics which was not to be surpassed for over two thousand years, probably because the need was not then felt, despite considerable further development of mathematics since the time of Euclid.

Eventually developments transpired which propelled mathematics at the same time, into its greatest practical significance and beyond the confines of rigour.
This flowed largely from the work of Newton and Leibniz.
Newtonian mechanics necessitated the kinds of operations on mathematical functions which were to be supplied independently by both Newton and Leibniz.
The procedures of differentiation and integration, which operate on functions to give on the one hand, the slope of the graph of the function and on the other the area under its curve.

The definitions of these concept made special demands upon the number system, particularly in the use by Leibniz of infinitesimal quantities, but also in Newton's account of fluxions as `last ratios'.

Doubts about the rigour of the procedure were voiced by Berkeley
who challenged the coherence of the ideas of fluxion and infinitesimal quantities\cite{berkeley2018analyst} in 1734 but it was not for another century before mathematicians became seriously embarrassed:

\begin{quotation}
  \emph{There are are very few theorems in advanced analysis which have been demonstrated in a logically tenable manner. Everywhere one finds this miserable way of concluding from the special to the general, and it is extremely peculiar that such a procedure has lead to so few of the so-called paradoxes.}

Abel, 1826
\end{quotation}

There ensued the most fertile period of foundational innovation in the history of mathematics.
This was not a search for absolute certainty, but for conceptual clarity and deductive rigour.
It occurred in several stages, most of which were instigated by Mathematicians and represented real advances in mathematics.

The first stage in this process was to show that the central notions of the calculus could be made precise without resort to infinitesimals or fluxions.
This aspect of the work had already been undertaken by Cauchy who's 1821 book ``Cours d'analyse''\cite{bradley2010cauchy}, rigorously defined the concept of convergence for sequences and series using what are now called Cauchy sequences.
Cauchy used these sequences to provide a precise epsilon-delta definition of limit and continuity for real functions which provided the means to  define differentials and integrals as limits \cite{cates2019cauchy}, giving precision and clarity to procedures which had evolved less formally over thousands of years going back to the work of Archimedes.

Having made these important advances in the rigour of analysis, there remained significant areas of uncertainty.
There was still no clear conception of the number system on which this work depended, though the position had improved with the elimination of infinitesimals from the theory.
The development of analysis had made it necessary for mathematical functions to be regarded as abstract entities rather than syntactic expressions, but the coherence of that position depended on clarification of the exactly what these abstract functions are.

From a modern perspective the clarification of these two points rests on the theory of sets which was begun by Cantor, but the reduction to set theory was a later development.

\section{Some Preconditions of Deductive Rigour}

The centrepiece of this synthesis is a formal logical system, consisting of a declarative language with an abstract denotational semantics and effectively decidable deductive rules.

The presentation here is informal, but describes formal treatments many of which are already in place in the literature, and all of which would generally be accepted as entirely feasible.
This preliminary discussion concerns the key requirements that will have to be met, with a first indication of how those requirements will be met.

Deduction can only take place in a declarative language which has a definite truth conditional semantics.
It is possible to formally reason in languages which lack such a semantics, and there are technical conceptions of consistency (e.g. consistency in the sense of Post) which give minimal criteria of correctness for such a deductive system, but they nevertheless fail to qualify as deductive if it cannot be shown to preserve truth.

\chapter{Logical Truth}

In due course I will address some of the sceptical challenges to the idea of first philosophy, and to many aspects of the synthesis which is proposed, but at this stage I am concerned only to precisely describe the first stages in the synthesis by offering some informal definitions.

The single most important concept around which all else is constructed is that of \emph{logical truth}.
Many will be inclined to contest my use of that term for the concept which I will now describe.
It is the concept which is important, not the name I use for it.

Ultimately, in the synthesis which I provide, there is a place for addressing all the forms which knowledge may take, but insofar as epistemology is concerned with subject matters belonging to the natural sciences, it will receive no special attention beyond the considerations later advanced for the empirical sciences.

Placing the epistemology of logical truths (logically) prior to that of empirical science reflects the important role which mathematics plays in the natural sciences, which we consider in its broadest sense as the interpretation of science as constructing abstract models of physical systems and phenomena.

The concept of logical truth will be defined in the following stages.
\begin{itemize}
  \item
    In the first instance it is defined as a characteristic of sentences in languages of a particular kind (declarative languages with fixed vocabulary and a well defined truth conditional semantics).
  \item
Then the definition will be augmented to admit languages with an extendable vocabulary, whose truth conditional semantics is appropriately augmented by the process of extension.
In these latter we may say that the truth conditions for sentences in the language are sensitive to a context which reflects the extensions to the vocabulary and the constraints incorporated by the extensions.
\item
At this point we may speak of relationships between these 
\item

\end{itemize}


\section{Ontology}

I divide those things which may exist into three kinds as follows:

\begin{itemize}

\item concrete objects

  Those things which have spatio-temporal location and may be causally related to other concrete objects.

\item Purely abstract objects

  These are entities which do not have spatio-temporal locations and are not causally related to other objects.

\item Hybrid objects

  These are complex entities which are built from or built into both concrete and abstract objects, for example, a set of concrete objects will be such a hybrid.
  Thus, for example, if in the Simple Theory of Types, the individuals are held to be concrete objects, and the propositions are abstract, all other types will consist of hybrid entities.
  If on the other hand, the individuals are purely abstract, then the entire ontology will be purely abstract.
\end{itemize}

For the purposes of defining the notion of ``logical truth'' abstract ontology will suffice.

Some philosophers will think it reasonable to ask whether any abstract objects exist, and how we can possibly establish that they do.
By engaging in ``first philosophy'' I arrogate to myself the right to articulate without restraint whatever concepts will provide the basis for the epistemological synthesis which will follow.
This includes the concept of existence.
It is open to us to decide what that concept means and how it will be used in our system.

The position I adopt on the matter of existence is sensitive to the kind of entity concerned, and I need only at this point speak to the purely abstract entities at stake, in relation to which I proposed a wholly conventional stance.
This may be considered analogous to a fictionalist stance, differing from it in crucial ways.

The fictionalist is going to tell a story which he acknowledges is not factual, but which he considers instructive or entertaining in some other way.
The statements he makes in his fictional narrative will sometimes be false, and may not even be mutually consistent.

The conventionalist stance I take differs from fictionalism in these two principle respects:

\begin{itemize}
\item Within the scope of the relevant convention, the consequences of the convention are true, not false.
\item It is very important in the adoption of conventions, that the principles adopted be logically consistent, even when these conventions are adopted en route to the establishment of the concept of logical consistency.
\end{itemize}

In what follows hybrid objects will have no role, and it is very likely that I will tire of prefixing ``abstract'' with ``purely'', which should therefore be understood as implicit.

Normally in what follows, the adoption of some such ontological convention is a part of the definition of a formal language, and its scope is primarily in determing the meaning and truth value of expressions in that language.
More broadly in the less formal conduct of mathematics and perhaps other sciences, a convention may be simply contextual rather than being a part of the adoption of a language.
For example, there are multiple different formal axiomatisations of set theory, which disagree among themselves about the domain of which they speak, about which sets must, may or cannot exist.
The ``standard'' conception is of a domain exclusively of well-founded sets, but no axiomatic system guarantees that this is the case.
On the other hand, the axiom of separation, which guarantees the existence of all subsets of a set obtained selecting the members satisfying a given formula, makes it possible to prove that there is no universal set which contains all other sets.
Notwithstanding that difficulty with the universal set, many consistent set theories are available in which the universal set does exist and the principle of separation is not valid.

I may as well note in explaining my adoption of this conventionalist convention, that if I were to prove mistaken in disbelieving that there is any objective truth about what purely abstract entities exist (including the possibility that there are none), that the value that I associate with this position would be unabated.
Since abstract entities are causally unconnected with us, there existenve or otherwise in any absolute sense is immaterial to the utility which attaches to adopting the convention that some appropriate collection of abstract entities does exist.

Among the very many connections between the synthesis I present here and the philosophy of Rudolf Carnap, there is a direct connection here between my ontological stance and that which Rudolf Carnap attempted unsuccessfully to explain by distinguishing ``internal'' from ``external'' questions \cite{carnap50}.
``Internal'' questions are those posed in the context of some well defined language, and which are to be answered on the basis of the principles explicit or implicit in the definition of the language, ``exernal'' questions are those ontological questions which might arise in the process of defining the semantics of such a language, and were regarded by Carnap as meaningless pseudo questions.
Of course, we can ask what is the meta-language in question (which would normally be an idiom of a natural language such as English aumented by the vocabulary of some special discipline, perhaps mathematical or philosophical logic.
This is unlikely to provide any definitive ruling on the kinds of ontology which Carnap was considering,

The ontological conventionalism with respect to abstract entities which I am adopting here is not sensitive to the ontology of the metalanguage (which is indeed, in the first instance, English).
It is possible to describe an abstract domain whether or not the existence of that domain is demonstrable in that context, in just the same way that one can write a novel about an entirely fictional population.
If we then define the meaning of existence in the object language as membership in that possibly fictional domain, then ontological questions posed in that object language will be answered according to the structure of the chosen domain rather than by reference to any ontological principles which belong to the meta-language.

Though this discussion has addressed only abstract ontology, conventionalism in respect of concrete ontology is at least implicit in what will be said later about the representation of scientific and engineering knowledge.
It should also be noted that modern logic is generally agnostic about what things are, insofar as all properties and relations are external, and so long as those externalia are preserved, the substitution of a completely different collection of individuals in the domain of discourse has no effect on the truth values of any sentence.
This apparent indifference to identity is reinforced in the logical foundations which are proposed by ontological ambiguity in type constructors, the effect of which is that though various complex types of abstract entities are constructed in way similar to the constructions which would be used for mathematical objects in a pure set theory, there is no basis for identifying these newly constructed entities with the constructions themselves.
It is a foundational scheme in which numbers are not \emph{identifieed with} sets, or any other type of universal foundational ontology.

Though fully in the spirit of positivism, this epistemology is not nominalistic, a departure from the mainstream first appearing in Rudolf Carnap.

\section{Language}

It is not impossible to reason about the world without making use of language, but as soon as we talk about truth, language is presumed.
A generic definition of logical truth therefore depends upon an explicit and general account of some class of languages of which logical truth can meaningfully be predicated.

Though linguists and mathematical logicians may have conceptions of language which are purely syntactic (e.g. ``a set of sentences'') for the purpose of defining the concept of logical truth (as it is here construed), some semantics is essential.
The kind of semantics which is necessary is \emph{truth conditions}.

The essence of declarative language is the use of sentences to make claims which discriminate between various possibilities which constitute the subject matter of the language.
For the barest account of logical truth, we therefore require of the truth conditional semantics:

\begin{enumerate}

\item that a range of ``possibilities'' be identified, which are those for which the truth conditions settle a truth value.

\item that for each sentence in the language, a subset of possibilities is given, which are the cases for which the conditions expressed by the sentence under the semantics hold, in which case the sentence is then deemed true.
\end{enumerate}
    
\section{Logical Truth as Tautology}

For the broad swathe of languages which satisfy the conditions given above, the logical truths are those sentences which are true in every possibility, hence necessary.

It may be noted that there is a special case arising when there is only one possibility.
This is exemplified by the language of first order arithmetic when given with the standard semantics which admits of just one possibility (up to isomorphism).
In that case, the idea that a sentence conveys information when asserted by eliminating possibilities is not as instructive as it might otherwise be.
That we have a clear intuitive understanding of the structure of the natural numbers clearly does not entail that we know what arbitrary sentences of arithmetic are true, and it is therefore informative to know that a sentence is true even if it has not ruled out any possible structure of the natural numbers.

The sceptically minded will further note that this conception of logical truth is just the identification of logical truth with analyticity, and that the concept is thus rendered wholly conventional and lacks the absolute character which some would wish to see in the notion of logical truth.
These consequence arise however, not from any defect in the notion of logical truth thus articulated, but rather from the fact that the syntax and semantics of languages are completely conventional, and hence that, however absolute a conception of logical truth you might have, the question of which sentences express logical truths must be determined by those conventions (though I do not deny that the relevant ``conventions'' may in the case of natural languages have been fixed, if indeed they are fixed, by biological and cultural evolution rather than by the whim of language designers).

\section{Defining Foundational Universalism}

A ``Universal Foundation'' for logical truth would be a language to which logical truth in any other language is reducible.
The idea is therefore dependent upon what concept of reducibility is intended.

My motivation in addressing this topic comes from the desire to support a linguistic pluralism which includes the ability to reason about systems whose subsystems or components may be given in distinct languages.
In that case one needs a logical environment in which claims about subsystems in different languages can all be comprehended coherently.

A natural way to consider this kind of universality is by translation from each language into the universal language  which reduces truth in the source language to that in the target language.
The weakest constraint one could put on such a translation would be that it is an ``effective'' mapping which preserves logical truth.
This introduces a constraint on languages which it was not necessary to include in the definition of logical truth above, that the syntax, i.e. the set of sentences, be countable.
The kinds of reduction which arise in this way are those which are studied in \emph{recursion theory} and the the structure created by these reductions are various hierarchies of degrees of recursive unsolvability \cite{rogers1967theory}.

Regrettably, this extensive;y researched branch of mathematical logic will not give a convincing account of how logical truth can be encompassed in one universal family of languages unless we were prepared to regard all logically true sentences as expressing the same logical truth.
In that case we should be prepared to find, when attempting to prove a logical truth which had some material bearing on the problem we are addressing, that we were actually reasoning about some other proposition which happened to have the same logical status.

To progress this agenda we need much stronger constraints on reductions so that they can reasonably (and practically) be understood to preserve the meaning of the original.
For this purpose we need a more refined semantics than sentential truth conditions.
To accomplish this we need translations which not only preserve truth value, but preserve subject matter.
To that end we need more detailed structure in the kind of semantics which is to preserved by the reductions.

Not only should the mapping preserve the truth values of sentences, but it should preserve the values of the non-sentential expressions  which are the constituents of the sentences in the language.
We therefore appeal to the idea that the semantics should be presented as a homomorphism defined over the abstract syntax considered as a many sorted algebra, in which the sorts correspond to the various syntactic categories or phrase types in the phrase structure grammar of the language.

This more elaborate conception of semantics defines a new class of languages relative to which a claim to universality of some family of languages will be judged.

The notion of ``family of languages'' is quite restrictive, it is not some arbitrary collection of languages, but a series of very closely related languages, which have the same syntax but a progressively refined semantics.

\section{Two Universal Foundations}

Two putative universal foundations serve the purpose providing intelligible foundational underpinning for practical foundation system.
There is no suggestion that either of these is unique, but the two choices are made to be appropriate in distinct ways.

The first is chosen for the kinds of simplicity which contribute to underpinning the semantics and the proof rules through the deepest penetration of formality supported by intelligible but less formal semantic refinement.
The second is chosen for the convenience with which it can provide an underpinning for the variety of formal notations which are used in practice in mathematics, science and a full variety of practical applications in all domains.

The first language is the language of first order set theory, the second is the variant of Church's \emph{Simple Theory of Types} used by the Cambridge HOL system.
In both cases the families are generated by progressive refinements of the semantics which eliminate smaller interpretations.
In the proof theory these are effected by large cardinal axioms.
Given the standard semantics which consists primarily in demanding full power sets (in the set theory) or full function spaces (in the type theory) these axioms suffice to refine the semantics appropriately.

\chapter{Digging Deeper}

In this chapter I consider the credentials of the simpler of the two foundational systems, with the intention to examine how well defined is its denotational semantics and proof rules are, how solid are our reasons for believing that the proof rules are sound with respect to the semantics (that it proves only logical truths), and in what ways possible ambiguities might be reduced and certainty of soundness improved.

First order set theory is a first order language, a concept which has been central to much of the work in Mathematical Logic in the last century.
This means that the semantics and proof theory is well understood and many result have been established to the highest standards of rigour found generally in modern mathematics.
For the purpose of this synthesis there is very little added to the standard account.

The theoretical study of logical systems often treats semantics in a strictly analytic rather than a prescriptive manner.
By this I mean, that a formal language and its deductive system is addressed by determining the full range of \emph{models}, where a model is any interpretation of the language for which the logical system is sound, rather than expecting that the definition of the language should stipulate the intended subject matter (possibly by describing one or more intended interpretations).

If the semantics is analytic rather than prescriptive, then it renders any consistent logical system sound and complete.
The point of semantics in this case may well be to establish consistency, which is achieved by furnishing any model.

If the semantics is prescriptive then it is not only possible, but likely, that the deductive system will not be complete, since all but the simplest mathematical subject matter do not have decidable truth conditions, and cannot therefore have a complete axiomatisation in relation to which proofhood is decidable.

It is not usual to talk of first order languages in terms of denotational semantics, the study of semantics in this case is done using the language of \emph{model theory}.
Languages are distinguished by their vocabulary, and meaning is assigned to the names in the vocabulary by means of a structure called an interpretation, which consists of a domain of discourse (a set of values which constitute the range of the first order quantifiers) and an assignment to each name of a value defined over that domain of discourse.
The names in question are called constants, and represent either predicates which may take as values any subset of the domain of discourse which is the set of elements which are deemed to satisfy the predicate, n-ary relation symbols which denote sets of tuples of values from the domain of discourse, or functions which represent mappings from n-tuples to values drawn from the domain.

The standard account of the semantics of a first order language determines how the value of an expression in the language can be obtained for each interpretation of the non-logical constants in the language.
Since the expression may contain free variables, it is necessary to have a value for these variables before a value for the expression can be obtained.

A denotational semantics can be determined for a first order language by stipulating a particular set of interpretations as intended interpretations.

In first order logic with equality, the relation of equality is fixed and represents equality of pairs in the domain of discourse, and therefore does not appear in the structure defining an interpretation.
In first order set theory there is just one other relation symbol, which is the binary relation of membership.
The 



\chapter{Connecting the Families}

In this chapter the equivalence of the two families is argued.
In the first case this is done by reference to the various treatments of the semantics of CHOL in FOST.
This is supplemented by a discussion of the interpretation of OFST in CHOL using a strong axiom of infinity which in effect asserts that the cardinality of the 'individuals' is inaccessible.

Both families consist of a linearly ordered set of languages indexed by cardinal numbers which represent the minimum cardinality of the ontology.
For lower cardinalities, CHOL and FOST may not be equivalent (this depends on the detail of how these ideas are defined), but once we reach a sufficiently large cardinality they will be equivalent (i.e. bi-interpretable in the relevant sense).

For practical purposes, i.e. for the purposes of science (excepting some parts of set theory), engineering and commerce, large cardinals are not needed, the relevant mathematical structures being modest in size, and the heirarchy of languages becomes irrelevant.
This is evidenced in practice by the sufficiency of CHOL with the standard axiom of infinity (ensuring only a countable infinity of individuals) for engineering purposes.

\chapter{Building Higher}

In this chapter the way in which CHOL provides universal support for the class of languages with a well defined denotational semantics is discussed.

The idea here is that no injection between the language for the application and the CHOL language is necessary, because the use of the standard mechanisms for conservative enables the denotational semantics to be renedered in CHOL in a manner which results in the abstract syntax for CHOL containing the abstract syntax of the application language, and the semantics of CHOL with those extensions which define the denotational semantics of the target language will correctly render the meaning of expressions in that abstract syntax.

\chapter{Beyond the Limits}

The universalist conjectures which I have presented have their limits and in this chapter I mention some languages and deductive systems which either are technically not within the scope of these universal foundational families or which are based on philosophical ideas which are incompatible with our perspective.

When adequate formal systems for mathematics were first devised the philosophical attitudes toward these and other foundational matters fell into three main camps known as Logicism, Formalism and Intuitionism.
The hard core logicists were Gottlob Frege and Bertrand Russell, both of whom were also \emph{universalists}, holding that there should be just one universal conception of logic which is sufficiently rich to define all the concepts of mathematics and to formally derive mathematical theorems.
It is with Russell that type theories like Cambridge HOL originate, though Frege's logical system is generally regarded as constituting a second order logic, which prefigures the more elaborate type system contructed by Russell (and later simplified en-route to HOL).
The formalists lead by David Hilbert were linguistically pluralistic.

The story I have given above attempts a reconciliation of these two approaches to foundations, following the precedent of Rudolf Carnap in whose spirit this synthesis has been constructed.
It represents a compromise, proving a reconciliation of the pluralistic and universalistic foundational perspectives.

Intuitionism is more difficult to reconcile, because of its outright rejection of much of the mathematics of its time, most particularly of Cantorian set theory.
If intuitionism were merely a weaker logical system adequate only for a part of mathematics it might still reasonably be supported in the more full blooded system which I have proposed, but it is unlikely that its advocates would be content with any such accomodation.
Philosophical reservations (to say the least) would remain.

One reason for the philosophical incompatibility of intuitionism is its rejection of formal systems of any kind, which however was not replicated across the many logical systems which shared some of its concerns.
When formalisation was admitted (as later became normal), semantics remained prohibited, for truth was then, in many systems, identified with the existence of a constructive proof.
So the idea of rooting logical foundations in the concept of logical truth defined in terms of semantics remains unacceptable.

A technical reduction might nevertheless be effected through the identification of truth and proof, but its not clear that repairs the philosophical breach.

Intuitionism, spawned a broad range of logical systems which radically departed from the classical models, often with proponents without a uniform philosophical perspective, but with technical reasons for preferring the novel logical systems which emerged.
An important innovation in these constructive logical systems appeared in the work of Martin L\"of in the shape of a dependent type theory inspired by a correspondence between the types of terms in a type theory and the structure of propositions.

\chapter{Empirical Knowledge}

Concerning the representation and application of empirical knowledge.

\chapter{The limits of Language}

Every bit of information stored in an information processing system is represented by a material structure which involves very many submicroscopic parts, the nature of which is the subject of complex ongoing research in fundamental physics.
It is therefore inconceivable that we could ever have a complete description of the material universe.

It is not impossible that there might be a finite description of what kind of state the universe might be in, and of the ways in which the state of the universe progresses from one moment to the next, but contemporary physics must surely cast doubt upon that possibility too, notwithstanding the insistence of some physicists that a ``theory of everything'' is a real possibility.

This suggests that our knowledge of the physical universe of which we are a part, or even of any part of it, must be incomplete.

It is nevertheless possible to construct theories about, or models of the universe which prove of considerable value in progressing the prosperity and well-being of ourselves and those we care about.

Isaiah Berlin has characterised ``Enlightenment Thought'' as involving the belief that all truths are logically compatible, but the reality is that we routinely make us of ideas about the way the world works which are logically incompatible.
A clear example of that is the continued use of Euclidean space time and Newtonian physics after the discovery or invention of Einstein's theories of relativity.

\chapter{Abstract Models of Physical Reality}

\appendix

\chapter{Varieties of Epistemology}


My purpose here is not to survey the many varieties of epistemology which philosophers and others have engaged in, but to describe the two particular variants which I address in this monograph.

The work falls into two parts which I describe as \emph{natural} epistemology and \emph{synthetic} epistemology, terms I have adopted for the purpose of this exposition rather than reflecting any established nomenclature.
Of those two, the latter provides the substance of the work, the former plays a supporting role, intended to give background essential to an understanding of the synthesis which follows.

The main purposes of the preliminary natural epistemology are:
\begin{itemize}
\item to present the following synthesis as a natural future development in a long standing evolutionary history
\item to identify those elements in that evolutionary progression which are most important for an understanding of the synthesis
\end{itemize}

In taking knowledge and epistemology as having evolved over the entire history of life of earth and as now entering into a period of more rapid and radical change than ever before, I am stretching the terms beyond their previous application.
One aspect of this is the abandonment of the predominant anthropocentrism of traditional epistemology.
Contemporary epistemology has moved away from anthropocentrism by considering knowledge in animals, artificial intelligence, and the implications of posthumanism.

In my first part, the evolution of knowledge is thought of as beginning with the prebiotic evolution which culminates in abiogenesis, and as progressing throughout the evolutionary history of homo sapiens ultimately morphing into the cultural evolution which in modern times lead to information technology, the prospect of artificial intelligence, and the progression of biological evolution to intelligent design and selection.

\subsection{Knowledge and Mind}

Epistemology has traditionally been anthropocentric, regarding knowledge as as essentially human enterprise.
This perspective on knowledge is exemplified by its conception as justified true belief, and hence as in part a kind of psychological or mental phenomenon.

\section{Bard}

Contemporary epistemology often emphasizes human cognition and understanding. However, it also ventures into broader realms that explore knowledge beyond human boundaries. While historically centered on human cognition, contemporary epistemology is increasingly acknowledging non-human forms of cognition and knowledge representation.

Anthropocentrism in epistemology arises from its traditional focus on human perception, justification, and belief formation. But contemporary thought expands beyond this by considering:

1. **Animal Cognition:** Studies in animal cognition challenge the human-centric view by exploring how animals acquire, process, and apply knowledge. Research indicates sophisticated cognitive abilities in various species, altering traditional conceptions of knowledge acquisition.

2. **Artificial Intelligence:** Discussions on AI and knowledge representation highlight the development of systems that can store, process, and utilize information in ways akin to human cognition. This challenges the exclusivity of human knowledge processes.

3. **Extended Cognition:** The concept of extended cognition posits that cognition isn't confined to the brain but can extend to objects, tools, or technologies. This extension broadens the scope of what constitutes knowledge and cognition.

4. **Posthumanism:** Some branches of thought within epistemology, influenced by posthumanism, explore the possibility of knowledge existing and evolving beyond human existence. This perspective raises questions about the nature of intelligence and knowledge in synthetic or non-human forms.

While the historical foundation of epistemology might be anthropocentric, contemporary discourse increasingly acknowledges and explores knowledge frameworks that extend beyond human cognition. References to contemporary philosophical works like "Posthuman Knowledge" by Rosi Braidotti or "Animal Minds" by Donald R. Griffin can offer detailed insights into these evolving perspectives within epistemology.

Extended cognition challenges the traditional view that cognition is solely a process occurring within the boundaries of an individual's skull. It posits that cognitive processes can extend beyond the brain to include external objects, tools, or even other individuals. This concept was notably developed by philosophers Andy Clark and David Chalmers in the late 1990s.

Key aspects of extended cognition:

1. **Cognitive Extension:** External objects or tools are integrated into the cognitive process, effectively becoming a part of the thinking system. For instance, a notebook or a smartphone can serve as external memory storage, aiding in information retention and retrieval.

2. **Functional Integration:** The external objects or tools are not just accessories but are functionally integrated into the cognitive process. They are considered as parts of the thinking system and contribute significantly to problem-solving or decision-making.

3. **Cognitive Offloading:** Individuals use external resources to offload cognitive tasks. This offloading can range from simple memory aids (like notes or calendars) to complex computational tools that augment problem-solving abilities.

4. **Mutual Dependence:** The relationship between the mind and external tools is reciprocal. Just as the tools extend cognitive abilities, the cognitive processes of the individual shape how these tools are used or adapted.

Examples of extended cognition can be seen in various daily activities: using a calculator to perform complex mathematical calculations, relying on a GPS for navigation, or even utilizing language as an external system for thought organization.

This concept challenges the traditional internalist view of cognition, emphasizing that cognitive processes aren't confined to the brain but can incorporate external elements. References like Andy Clark's "Supersizing the Mind: Embodiment, Action, and Cognitive Extension" or David Chalmers' "The Extended Mind" delve into this concept, offering detailed arguments and case studies supporting the idea of extended cognition.

The concept of extended cognition, as traditionally understood, often focuses on human interactions with the environment rather than exploring non-human forms of cognition. It primarily addresses how humans use external tools to augment their cognitive processes.

However, recent philosophical discussions have expanded the scope of extended cognition to encompass a broader understanding beyond human-centric perspectives. 

One direction involves considering how non-human entities or systems, such as certain animals or even artificial intelligence, might engage in forms of extended cognition. For example:

1. **Animal Cognition:** Some studies suggest that certain animals use tools and incorporate external elements into their cognitive processes. This challenges the strict anthropocentric view by acknowledging cognitive extensions in other species.

2. **Artificial Intelligence:** Discussions on AI often touch on extended cognitive processes, exploring how these systems interact with their environment and utilize external databases or networks to enhance problem-solving.

Expanding the concept beyond human-centricity involves recognizing cognitive processes in various forms of life or intelligence, not solely limited to human beings. This extension challenges the exclusive focus on human cognition and opens avenues for exploring cognition across different life forms and artificial systems.

References such as Peter Godfrey-Smith's "Other Minds: The Octopus, the Sea, and the Deep Origins of Consciousness" or work by Philipp Kellmeyer on AI and extended cognition could offer insights into these broader perspectives on extended cognition beyond human-centric views.


\chapter{Varieties of Epistemology}


The purpose of this work is to construct, present and advocate for a \emph{synthetic epistemology}.
Notwithstanding such prior usage of that concept which there may have been, what I intend in using the term is that the ``epistemology'' which I present has been devised as suitable for future application in a world in which the corpus of systematic knowledge which includes both scientific knowledge of the principles which govern the world (including the logic and mathematics which enables those principles to be articulated) and also an ever increasing body of detail about the the world governed by those principles, is maintained and progressed in a collaboration between human beings and intelligent artefacts, the latter facilitating precise language and formal reasoning which might otherwise be too onerous for mere humans.

Though that central purpose is synthetic, the synthesis is not a fantasy constructed \emph{in vacuo}, but a stage in a historical process which has been underway for billions of years.
Some examination of that historical progression is therefore attempted for two reasons, and this I present as ``natural epistemology'', which you can imagine, may be something like but not entirely the same as ``epistemology naturalised''.

\subsection{}

The purpose of this work is to present some ideas for the representation of all those kinds of knowledge which are amenable to deductive reason.
Let us say \emph{declarative knowledge}, that kind of knowledge which is found in true declarative sentences.
My aim is to articulate certain ideas about how large bodies of knowledge can be organised in such a way as to maximise our confidence in the truth of the propositions and to facilitate deductive reasoning for the purposes of elaborating and applying the knowledge.

These ideas are \emph{foundational} in more than one respect, the most fundamental of those being very closely related to some of the important work done in relation to the logical foundations of mathematics over the last 150 years.
The focus on foundations is complemented by attention to applications.
A single logical system is identified as the pivot through which the foundational underpinnings support the full range of applications in almost every aspect of our lives.

That pivotal system is the logical system which has been adopted by Cambridge HOL and some other interactive theorem provers.
That system is itself a logical foundation system for mathematics with well established credentials, within which almost all known matheamatics can be derived by machine supported and checked deductive inference from formal definitions of the relevant mathematical concepts.

Most foundational research has historically been conducted by scientists (and sometimes engineers) seeking to improve the clarity and rigour of their work in the face of difficulties arising from perceived defects in those areas.
  It is a process of working backwards from the applications in need of support toward some more solid and stable basis on which they can be constructed, and there is usually a good practical sense of when enough has been done, though sometimes foundational advances expose deeper difficulties which are then in need of attention.
  This is the kind of process which was seen in mathematics througout the 19th Century and eventually provoked a crisis at the turn of the 20th as a result of Russell exhibiting a paradox which vitiated Frege's foundational enterprise.

  A quite different foundational enterprise occurs in philosophy when philosophers look to foundations for a response to radical scepticism.  The philosophy in this context may not be well connected with the practice of science, and the expectations of foundations may be less easily satisfied.

To be certain of the truth of a sentence, we must first establish a definite meaning for it.
The meaning of a sentence depends upon the language in which it is expressed, and on the semantics of that language.

The distinction between true belief and knowledge plays very little role in this project, despite a deep concern with the levels of assurance with which truth can be established.

**************************


I am primarily concerned with making meaning precise and enabling truth to be establish with the highest level of assurance.

Insofar as this discussion can be construed as 
The conception of epistemology as concerned specifically with the theory of \emph{knowledge} is therefore 

Either:
\begin{itemize}
  \item
Before going further I should confess that, though I will often talk about knowledge, I regard that term in much the same way that an Engineer might think of the concepts `hot' and `cold', as poor substitutes for a temperature.
In the 'epistemology' here proposed, the semantics of declarative sentences is factored into two parts.
The first of these is an abstract semantics interpreting the sentence in relation to some abstract model, and the second an indication of the concrete structure which that abstract model is intended to replicate.
Knowledge in this system consists of formal logical truths about the abstract model (usually established by machine checked formal deductive proof) and more elaborate and mixed information about how that abstract model is applicable to the concrete world, rooted in a bijection between those elements of the model which are intended to represent physical entities with the entities they represent.
\item
By contrast with much epistemology, the focus is not on the word \emph{knowledge}, which plays only a minor expository role.
Instead the emphasis is on \emph{truth} and the confidence with which truth can be established (primarily in relation to \emph{a priori} knowledge), and on utility or applicability in relation to theories about, or models of, parts or aspects of the material universe.
It is thus oriented toward scientific knowledge represented in abstract structures, rather than addressing the kinds of everyday knowledge which people acquire and exploit in their everyday lives.
\end{itemize}
- or some amalgam!


********************************

Notwithstanding this emphasis on science and engineering, the synthesis is applicable to any domain sufficiently definite that deductive reason is in principle possible, and it is expected that any practical difficulties in its use in domains less comfortable with formal notations will be mitigated by the intermediation of Large Language Models or Artificial General Intelligence.

An epistemological synthesis must begin somewhere.
A major consideration for this synthesis is the establishment of clear meanings, a necessary condition for truth to be definitively established, and the provision of contexts in which deductive reasoning can be rigorous and substantially automated.
In such matters sceptical arguments based on regress both of definitions (of concepts and languages) and of justification demand an answer for which we offer a foundational perspective.
For this reason there is a logical order in which the ideas are best presented, withstanding the inevitable circularity incurred in the most fundamental parts (for the risks in which, mitigations are offered).

There are two orderings of interest here.
There is the ordering of presentation so as to best communicate to readers the ideas.
There is also, in a foundational presentation, a logical ordering of the substance.
This is the kind of order which is required in definitions to avoid their vitiation by circularity.
As well as the ordering of definitions, we will be concerned with an ordering of languages, in which more complex languages are defined in or by reference to simpler languages.

The avoidance of circularity is desirable, since it contributes to ensuring that meaning is clear and well defined, the maintenance of coherence and the avoidance of contradictions.

\part{Natural Epistemology}

\chapter{Prebiotic Epistemology}

\chapter{Biological Knowledge and Knowing}

\chapter{Culture and the Advent of Epistemological Innovation}

\subsection{Some History of the Vision}

The ideas addressed by this epistemological synthesis seek to progress conceptions of knowledge which have developed over the last two and a half millenia, beginning perhaps with Aristotle's conception of ``demonstrative science'', a conception of empirical science modelled on the axiomatic method which had been successful in mathematics, particularly in what we now know as Euclidean Geometry.

There follows a brief sketch of some of the key development leading up to the present synthesis to provide a backdrop against which the account of purpose and the key requirements which flow from that purpose may be more clearly understood.

\subsection{Gottfried Wilhelm Leibniz}

Leibniz (1646-1716) was a polymath of the early Enlightenment, innovating in Philosophy, Logic, Mathematics, Science and Engineering. 

Leibniz conceived of and attempted to design a \emph{lingua characteristica} (a language in which all knowledge could be formally expressed) and a \emph{calculus ratiocinator} (calculus of reasoning) such that when philosophers disagreed over some problem they could say 'calculemus' (let us calculate) and agree to formulate the problem in the lingua characteristica and solve it using the calculus ratiocinator.

\paragraph{The Lingua Characteristica}
Leibniz engaged in four kinds work related to his proposed lingua characteristica, (or universal characteristic).
\begin{itemize}
\item Encyclopaedia - he sought the collaborative development of an encyclopaedia in which would be presented in non-symbolic form all that was so far known. This was to provide a basis for the lingua characteristica in which the knowledge could be formally expressed. This enterprise was not completed, but beneficial side effects were the foundation of new academies and of the journal Acta Eruditorum.
\item Universal Language - he promoted the development of a language universal in the sense of being spoken by all. There have been many such projects of which the best known today is Esperanto
\item The lingua characteristica - a formal language universal both in being understood by all, and in encompassing all knowledge.
\item The calculus ratiocinator - a method of computing the truth value of a proposition in the lingua characteristica
\end{itemize}

  The lingua characteristica was to be a language in which predicates were numerically encoded in such a way as to render the truth of subject predicate proposition (and Leibniz considered all propositions to have this form) could be obtained by arithmetical computation.

\paragraph{The Calculus Ratiocinator}
This is roughly how he proposed to do it. He believed that every predicate was either simple or complex and that complex predicates could be analysed into simple constituents. He proposed to assign prime numbers to each simple predicate and then represent a complex predicate by the two products, one of the primes representing its simple constituents and another similarly representing the simple constituents which occurred negated.
Complex predicates are therefore thought to be invariably conjunctions of finite numbers of simple predicates or their negations.
He also believed (following Aristotle) that every proposition had subject-predicate form, and that in a true proposition the predicate was contained in the subject, i.e. the set of simple predicates from which the predicate was composed was a subset of the set from which the subject was composed. This can be sorted out by numerical computation, you just check whether the first part of the predicate divides that of the subject without remainder, and that the second part (the negated constituents) of the predicate divides the second part of the subject.

His main difficulty in this was in discovering what the simple predicates are. Leibniz thought the complete analysis beyond mere mortals, but believed that a sufficient analysis (into predicates which are relatively primitive) for the purposes of the calculus ratiocinator would be realisable.

We now know much more about logic and its scope, and can see that this scheme could not work except perhaps for very simple fragments of scientific language.
This is primarily down to the limitations of Aristotle's Syllogistic logic within which Leibniz was working, but also because of limitations on what can be done even with modern logic requiring some qualification to any modern reconception of that vision, such as that in which I am here engaged.

Leibniz has nevertheless been an inspiration to many other philosophers, logicians and computer scientists and engineers who have progressed aspects of his ambition in the centuries which followed.

For my present enterprise the aspiration to completely general way of representing knowledge amenable to deductive reason and suitable for gathering together a comprehensive body of precise knowledge in a manner conducive to mechanised deductive reason is a primary aim of the proposed synthesis.

In order to tease out in simpler terms some aspects of the big picture here, its interesting to see the perspective of a modern philosophical historian of ideas on some features of Western thought up to and in the Enlightenment.

\subsection{Isaiah Berlin}

Isaiah Berlin's take on The Enlightenment\cite{berlinRR} comes in two parts.
First, ``three legs upon which the whole Western tradition rested'':
\begin{enumerate}
  \item All genuine questions can be answered.

    In principle, by someone.  Perhaps only God.
\item  The answers are knowable.
\item All the answers are compatible (with each other).
  It is a logical truth, Berlin says, that one true proposition cannot contradict another.
\end{enumerate}

and then, the extra twist added by the Enlightenment:
\begin{quotation}
That the knowledge is not to be obtained by revelation, tradition, dogma, introspection..., only by the correct use of reason, deductive or inductive as appropriate to the subject matter.

This extends not only to the mathematical and natural sciences, but to all other matters including ethics, aesthetics and politics.
\end{quotation}
and... that virtue is knowledge.

This is a simple description of an unattainable ideal, elements of which are important to this synthesis.
It will not be expected that all questions have an answer, for it is convenient sometimes to work with entities for which we have only incomplete descriptions, but it is an aspiration that any question definite enough to be amenable to deductive reasoning, either in its establishment or its application, can be accommodated within the synthesis.

There are now strong reasons to doubt that the answer to any properly formulated question can be discovered and established.
In many aspects of the proposed synthesis, absolutes are known to be unrealisable, and it is more important to be confident in the answers which do come than for such answers to be always forthcoming.

That all the answers be compatible is possibly the most crucial requirement in a system intended for large scale deductive elaboration, for in default of coherence, no result can be trusted.

\subsection{Hume and Kant}



\subsection{Frege}

\cite{frege1980}


\chapter{The Mechanisation of Logic}

\section{Meeting Cambridge HOL}

During the 1980s Mike Gordon and others at Cambridge University began research into the formal modelling and verification of digital hardware using software supporting the interactive development and checking of formal logical proofs.
This arose from prior work at Stanford and Edinburgh Universities on reasoning about computer software using a logic called ``Logic for Computable Functions'' (LCF) devised by the logician Dana Scott \cite{scott1993type}.
For the purpose of reasoning about hardware it was decided to adopt a Higher Order Logic instead of LCF.
After a number of adaptations, a logical system and a culture stabilised, based on a derivative of Church's ``Simple Theory of Types''\cite{churchSTT}.

This logical system is sufficiently powerful for the formal development of applicable mathematics, in a similar manner to that advocated by Frege and exemplified by Russell and Whitehead in their influential \emph{Principia Mathematica}\cite{russell10}.

The Fregean prescription (expressing his logicist thesis, contra Kant, that \emph{mathematics is logic}) was:

\begin{center}
  Mathematics = Logic + Definitions
\end{center}

To formally derive mathematics it suffices to begin with a formalisation of logic, add the definitions of the concepts of mathematics, and then derive in that logical system the theorems of mathematics.

Though Frege's prescription was specifically for mathematics, he considered his logical system to be of more general application.
Inspired by the ideas of Frege and Russell, Rudolf Carnap devoted his academic life to philosophically facilitating similar methods throughout science.
This was before the invention of the digital stored program computer, and the size and complexity of the formal proofs required would have been sufficient to make those ideas impractical for scientists.
The ideas, an important thread of the anti-metaphysical \emph{logical positivism} of the Vienna Circle, were even less palatable to philosophers.
In the hands of W.V.Quine a sceptical attack on Carnap's conception of logical truth, delivered in his ``Two Dogmas of Empiricism''\cite{quine53} (among other criticisms), served to de-throne positivism from the leading role in analytic philosophy which it might otherwise have occupied.

Despite the vicissitudes of Carnap's programme, when the Hardware Verification Group at Cambridge, with the advantage of computational support, looked to reason reliably about digital hardware, they advanced the programme pioneered by Frege, Russell, Carnap, Church and others.

Among those it is perhaps only Carnap who had attempted to address the problem of applying these logical methods in the empirical sciences.
He understood that Frege's prescription for the logical derivation of mathematics would not suffice for the empirical sciences.
Frege's prescription was specific to the \emph{a priori} sciences whose conclusions could be expected to be logically necessary.
These were understood to include mathematics, but not the empirical (\emph{a posteriori}) sciences.
For this reason Carnap had moved from the ``universalist'' stance of Frege and Russell, which had sought a single logical system for all deductive reasoning, to a linguistically (and ontologically) pluralistic regime, in which each application domain (each science or discipline) had its own logical system.
In these logical system principles which were factual rather than logical were adopted and results could be derived which were true results in the relevant empirical domain, but not logical truths.

It is not probable that Mike Gordon had any knowledge of these aspects of Carnap's philosophy when he sought to apply formal reasoning to digital hardware, but when he did do so these apparent difficulties in applying purely logical formal systems to reason about physical systems did not impede the adoption of pragmatic and sound ways of formally reasoning about hardware.

Philosophical qualms about the possibility of logically reasoning about physical systems did not pass unremarked by the engineers who adopted the methods, not only in Cambridge but across the world, as the resulting tools were adopted more widely.
Doubts about the status of logical proofs concerning the properties of physical systems moved from the research labs into the courts when claims about a commercial microprocessor whose design had been partially verified by these methods were contested in court.
Avra Cohn, a member of the Cambridge Hardware Verification Group (which had been contracted to contribute to the supposedly mis-described microprocessor verification), wrote a paper clarifying the issues: ``The notion of proof in hardware verification''\cite{cohnPIHV}.

Its worth looking a bit closer into the practice in the HVG and its relation to the difficulties which Carnap perceived in applying the new logical methods to the empirical sciences.

The Fregean prescription for the logicisation of mathematics required that mathematical concepts be introduced exclusively by \emph{definition}, and Frege was particular about what constituted a definition.
Using arbitrary axioms to characterise mathematical concepts or structures was not acceptable, primarily because of the risk it posed of compromising the logical consistency of the resulting theory.

Definitions are safe in this way, for they simply name some entity which in the existing system demonstrably has the required properties, rather than baldly asserting properties which are not already known to be realisable.
Because of this characteristic this kind of extension is called \emph{conservative}.

Definition only suffices in a logical system which is sufficently rich.
This is because definitions serve only to name something which already exists rather than to introduce something which was not previously in the domain of discourse.

The LCF system, which had previously been used for reasoning about software and which was adapted at first by Gordon for use in hardware verification, was not rich enough to work by conservative extension.
New concepts had to be introduced axiomatically, with the attendent risk of compromising consistency, which would become more severe as more complex systems were addressed.
When the group moved to work with a higher order logic\footnote{A fuller account of this story is given by Gordon in ``From LCF to HOL''\cite{gordon2000lcf}.} they fell in line with the Frege/Russell universalist conception of logic, and fully accepted the discipline of working exclusively with definitional  extensions, ensuring that all the resulting theorems were logical truths in that broad sense (of ``logical truth'') which was needed for mathematics to be assimilated into logic.

Notwithstanding this apparent turn to universalism, it is hard for anyone immersed in computer science to imagine that any one language can be sufficient, and the desire to support diverse notations and languages soon made itself apparent.
The universalist fundamental paradigm in practice became the use of a single logical system in which all other notations and languages could be interpreted, or ``embedded'' in the parlance which emerged (though without any explicit doctrinal underpinning).
An early illustration of techniques for interpreting other logical system in HOL may be found in \cite{gordon1989mechanizing}.
An account of a more elaborate example is given in Arthan's ``Z in HOL in ProofPower''\cite{arthan2005}.

The possibility of using HOL to support reasoning in other languages can said to arise from two features of the language.
The first is that the \emph{abstract syntax} of HOL is universal for a large class of languages, and the second is that the HOL language is sufficiently expressive that an abstract semantics of these languages can be rendered in HOL.
The concepts of abstract syntax and semantics arise from abstracting away from certain details of the syntax and the semantics, and from the possibility that they can be rendered entirely within the abstract ontology which is available in a purely abstract logical foundation such as HOL.

In \emph{abstract syntax} it is details of presentation which are discarded in favour of a simple representation of the structure of an expression or sentence which reflects the ways in which meanings of the elements of a syntactic category are compounded from the meanings of its constituents.

In \emph{abstract semantics} it is the details of how the expressions denote things in the real world (if the language does indeed do that), on the basis that the semantics has been rendered in the first instance in terms of an abstract model, a model in which all the entities involved are purely abstract entities.

With one further caveat we may then talk about HOL providing a language which is universal in practically important ways.
That caveat comes from two important results from mathematical logic and philosophy.
The first is the demonstration by Godel\cite{godel31a} of the incompleteness of arithmetic, and the second is Tarski's result on the arithmetic undefinability of arthmetic truth\cite{tarski31}.
The implication of these results is that no single logical system can be universal in its semantic expressiveness or complete in its deductive system.

We nevertheless conjecture that the HOL language and its deductive system can be indefinitely extended, both in its semantics and in its deductive system, and that it is universal in the sense that any other language (in a large class which will be defined) can be in practice reduced to a sufficiently strong version of HOL, sufficient strength being obtainable by progressively stronger axioms of infinity without other modification to the system.
HOL is in this sense universal, and it is also, I suggest (but cannot prove) practically complete in the sense that the probability of there being any practically applicable result which is not a theorem at close to the base level of this heirarchy.

Though the claim to universality is clearly refutable unless made for the heirarchy, the informal notion of \emph{practical universality} can probably reasonably be claimed for any single member of the heirarchy above a certain level.
This can be related to the default axiomatisation of first order set theory, generally regarded as a practically adequate foundation for mathematics.
HOL with a strong axiom of infinity which states or entails the existence of inaccessible ordinals.

This claim to practical universality extends beyond the provision of an adequate semantic and proof theoretic foundation, it includes a claim that the resulting support for languages is efficient.

The ideas presented here can be seen as providing a unification of the universalist and the pluralist attitudes to logical foundations.
Though we assert the existence of a universal foundation, we do not claim that it is unique.
I accept that there are many alternatives, but note that if they do have the required characteristics, then they will be equivalent.
I also believe that this provides a natural basis for the support of plurality of languages.
In fleshing out this account, I will abstract from the concept of language, completely divorcing it from concrete syntax, with the result that it becomes natural to identify a \emph{language} with a logical context which determines a vocabulary and its meaning, retaining the deductive system which extends that of the HOL system with the formal constraints which define the meaning of the vocabulary.


\footnote{Some references for future use:
\cite{arthan1991formal}
\cite{beeson2012foundations}
\cite{centrone2019reflections}
\cite{dzamonja2019}
\cite{gettier1963justified}
\cite{jones1992a,jones1992b}
\cite{kline1990mathematical1}
\cite{kline1990mathematical2}
\cite{kline1990mathematical3}
\cite{kumar2016self}
\cite{kuhn2000structure}
\cite{kuhn2012structure}
\cite{oliveira2006unifying}
\cite{shapiro1991foundations}
\cite{shapiroHPML}
\cite{tarski31}
\cite{tarski56}
}

\phantomsection
\addcontentsline{toc}{section}{Bibliography}
\bibliographystyle{rbjfmu}
\bibliography{rbj2}

\end{document}

% LocalWords:
