% $Id: p035.tex $ﬁ
% bibref{rbjp035} pdfname{p035}
\documentclass[10pt,titlepage]{book}
\usepackage{makeidx}
\usepackage{turnstile,amssymb}
\newcommand{\ignore}[1]{}
\usepackage{graphicx}
\usepackage[unicode]{hyperref}
\pagestyle{plain}
\usepackage[paperwidth=5.25in,paperheight=8in,hmargin={0.75in,0.5in},vmargin={0.5in,0.5in},includehead,includefoot]{geometry}
\hypersetup{pdfauthor={Roger Bishop Jones}}
\hypersetup{pdftitle={Positive Philosophy}}
\hypersetup{colorlinks=true, urlcolor=red, citecolor=blue, filecolor=blue, linkcolor=blue}
%\usepackage{html}
\usepackage{paralist}
\usepackage{relsize}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{longtable}
\usepackage{url}
\newcommand{\hreg}[2]{\href{#1}{#2}\footnote{\url{#1}}}
\makeindex

\title{Positive Philosophy}
\author{Roger~Bishop~Jones}
\date{\small 2023-08-08}


\begin{document}
%\frontmatter

%\begin{abstract}
% This is now a portmanteau of sketches of writings in the crossover between philosophy and artificail intelligence, in general more specifically relating to the philosophical areas of epitemology and logic, and knowledge management and automated deduction.
% It is being massaged in the direction of a book on "Synthetic Philosophy", via, in the first instance, God willing, an essay on Synthetic Epistemology for Artificial Intelligence.
%\end{abstract}
                               
\begin{titlepage}
\maketitle

%\vfill

%\begin{centering}

%{\footnotesize
%copyright\ Roger~Bishop~Jones;
%}%footnotesize

%\end{centering}

\end{titlepage}

\ \

\ignore{
\begin{centering}
{}
\end{centering}
}%ignore

\setcounter{tocdepth}{2}
{\parskip-0pt\tableofcontents}

%\listoffigures

%\mainmatter

\section{Preface}

This is a portmanteau document in which I have collected together many feeble attempts at philosophy into one place with the intent of massaging the materials until they can be bullied into one decent monograph articulating the main elements of my thinking.
It remains to be seen whether it serves that purpose.

I have used the \LaTeX{} book format so that my current principle effort can be cleanly separated out as the first part, and my abortive prior essays can each fit into a single chapter in some other part.

\part{Synthetic Epistemology atticks}

These are previous writings belonging to what is now p036.


\part{Attic}


\chapter{Introduction 2024-05-10}

The epistemological synthesis presented in this monograph anticipates and plays into a \emph{deductive paradigm shift} in the management and application of knowledge.
It primary purpose is to address the philosophical underpinnings of that paradigm shift, but I will also try to explain why I think such a paradigm shift is desirable, and why I suspect it may be inevitable.



\chapter{Introduction 2024-05-08}

The epistemological `synthesis' presented in this monograph is a projection into the future of multiple historical threads, most of them in some sense (if not strictly Darwinian) evolutionary.

It is my challenge for this introduction to give a first sketch of these various threads and the ways in which they contribute toward the synthesis, to provide a context in which a first description of the synthesis may be intelligible and well motivated.

In many of these threads the present seems to be a point of inflection, in some cases not merely a brief period of elevated acceleration
but a fundamental transition in character.

Before that first sketch of those threads, I must say something about the language which we have for the presentation.

\begin{itemize}
\item Earth's Biosphere
\item Knowledge
\item 
  
\end{itemize}



\chapter{Introduction 2024-05-03 to }

\section{Varieties of Epistemology}

Since I have styled this monograph as `synthetic epistemology' I feel bound to give some kind of account of what I intend by that phrase.
I proposed to do that by comparing and contrasting it with other approaches to epistemology.

The following approaches will be useful for that purpose:

\begin{itemize}
\item natural epistemology

  This approach to epistemology takes knowledge as a natural phenomenon which can and should be studied exclusively using the methods of the natural sciences.
  
\item evolutionary epistemology

  Since evolution is a natural phenomenon, this may be considered properly to be a part of natural epistemology.
  There are two principle varieties.
  The first variety considers the body of accepted knowledge as one determined by evolutionary processes which are a part of the evolution of culture, the second is concerned with the evolution of the mechanics of knowledge acquisition, i.e. of cognitive machinery.
\item normative epistemology
  By contrast with natural epistemology, the predominant traditional forms of epistemology are often described as \emph{normative}, i.e. as prescribing what ought to count as knowledge rather than what in fact is so considered.
  I have some doubts about this, since it seems to me rare that philosophers think of their theories as prescriptive, and that more often, even when an epistemological theory seems to diverge from common sense the philosopher will think of himself as exposing the true nature of knowledge rather than as engaging in any kind of prescription.
  In that case, how might one construe this kind of epistemology?
  The following two categories are possibilities.
\item analytic epistemology
  In recent times, particularly in the 20th Century which has been called `the age of analysis', philosophy has construed itself as achieving insights through the analysis of the meaning of language.
  This represents one way in which a philosopher might think of himself as seeking the true nature of knowledge which nevertheless coming up with some original insights.
  There are many varieties of philosophical analysis which might be seek epistemological insights in their particular ways.
  \item metaphysical epistemology
\end{itemize}


\section{A Philosophical Thought Experiment}

Though it is not essential to the content of the my epistemological synthesis, it is my suggestion that the synthesis is worth considering in the present in part because in some future scenarios it will prove optimal, so optimal that evolution may favour it and promulagate it.

The future scenarios are those sufficiently far removed from the present that the dominant  intelligence

The space talk is part of setting a scenario for the epistemology.
It is intended as philosophical thought experiment, I should have made that clear, so the criteria for acceptability are much less onerous than for a serious proposition about what is actually going to happen.
It does not have to be plausible or likely, it just has to be (a) logically coherent and (b) helpful for the philosophical purposes to hand.
I would also say, in this case, that I would like it not just to be logically coherent but also consistent with the generally accepted laws of physics and not a completely implausible scenario (but of course, such judgements are inevitably subjective).
A well known example of a philosophical thought experiment is talk of brains in vats, which is used in some epistemological arguments.
My hypothetical scenario is more elaborate.

The thought experiment is not essential to the rationale for the proposed epistemology, but it plays into the question whether there is a chance that the proposed epistemology would ever be adopted, which looks doubtful on its face.
It plays into that through the hypothesis that evolution makes it inevitable in the context of the thought experiment.

The thesis of inevitability comes from the fundamental characteristic of evolution that those things come to predominate which are most effective at proliferation.
That biological evolution on earth has involved random variation and `natural' selection are arguably important in divorcing evolution from divine intervention.


idea that, however radically the process of evolution (which encompasses the kinds of knowledge which are prevalent) may change, the fundamental characteristic of evolutionary change is the tautology that those things which come to be prevalent in the relevant sphere (on earth, our biosphere, but as post-human intelligence begins to spread across the galaxy, the galaxy as a whole) are those which are most efficient in promulgating themselves, and the epistemology considered is that which is most ruthlessly focussed on the development of the science and technology which advances replication across the galaxy.

What I am trying to establish in advance of the epistemology is a context in which the evolution of knowledge has become variation and selection by design and has become completely de-antropocentrised (if that neologism makes any sense).
Even though in the future scenario evolution is radically changed from past biological models it remains



\chapter{Introduction 2024-04-24}

Knowledge makes us what we are.
But we get to decide what it is.

That wasn't always the case, we, \emph{homo sapiens} are late arrivals.
A blink of the eye on evolutionary timescales.
And we are only just approaching a time when we might claim to have made rational decisions on the matter.
That is the pretension which underpins this `epistemological synthesis', and the reason why I style this epistemology ``synthetic''.

That epistemology hos not hitherto seemed a matter of choice may be attributed to evolution, and its place in the creation of knowledge, helping to give credence to naturalistic conceptions of epistemology as study a part of the natural world.

The concept of evolution may be used to speak of any more or less continuous process of change which can possibly be seen as progressive (in any sense).
It has come to prominence of late (in the last couple of hundred years) for the light it has cast on the origins of humanity (and the rest of the biosphere).
This is \emph{biological} evolution, by contrast with, for example, \emph{cultural} evolution.

Though a distinct domain of evolution, biological evolution has not been a single unchanging mechanism, and it may be argued that these distinct evolutionary mechanisms which have themselves evolved do not all conform to a single concept of evolution.
Not only have the mechanics of biological evolution evolved, but the concept of evolution most appropriate to describing these mechanisms has evolved at the same time, as well as being distinct from the most appropriate characterisations of evolution in other domains, such as cultural evolution and pre-biotic evolution.

A simplistic characterisation of Darwin's first ideas is that the evolution of species takes place in the context of species of self-reproducing organisms where the reproductive process has a good degree of fidelity but nevertheless is imperfect, and a process of natural selection separates out those variations which are advantageous from those which handicap.
The effect is to gradually transform the species in ways which optimise the species for reproduction in some particular enviromental niche.

\chapter{Introduction 2024-04}

The `epistemological synthesis' which is offered here is a foundational enterprise set in an evolutionary and historical context.
It is concerned with knowledge in a broad sense, which is subsumed within a foundation system which simple but powerful, both in semantic expressiveness and deductive strength.

Many have criticised epistemology as a foundational enterprise.
'epistemology naturalised' progresses such reservations by repudiating foundational aspirations and proposing that epistemology adopt the methods of the natural sciences.
More long standing objections to foundational epistmology include arrguments from regress both in the description of meanings and in the justification of claims to knowledge.
Arguments from regress will be taken seriously here below, and are taken here to be conclusive in denying absolute precision in meaning or absolute confidence of truth, but not as impinging upon the achievement of more than adequate standards of precision and confidence.

Though this synthesis is foundational, it is not constructed or offered \emph{in vacuo}.
A distinction is made between \emph{context} which gives us the knowledge to construct the synthesis, and the background against which it can be made intelligible to others, and the elements of the construction itself.
It is the latter which form a well-founded structure.



\chapter{Introduction 2024-04-14}

Knowledge make us what we are, it has enabled \emph{homo sapiens} to shape the world, largely if not entirely to his benefit.
We might also say that \emph{homo sapiens} has made knowledge what it now is.
The dominant forms which knowledge takes could not have appeared at any earlier stage in the evolution of life on earth.
It depends upon language and culture, which in turn depend (among other things)  upon capabilities of the central nervous system which matured only with the appearance of \emph{homo sapiens}.

Forms of knowledge and of living species may therefore be thought of a co-evolving over 3 billion years on Earth.
During that time evolution itself has changed, not only have the characteristics and mechanisms of biological evolution advanced, but completely new kinds of evolution have come to be important, notable the cultural evolution which has resulted in our present body of scientific and technical knowledge (\emph{inter alia}).

In that evolution of evolution, and in other important areas, we are

\chapter{Introduction 2024-04-04}

Knowledge makes us what we are.
It has many forms, which have evolved along with the evolution of life on earth.
Evolution also has diverse forms, which test the limits of any conceivable definition.
Though biological evolution made man, it took billions of years.
The cultural evolution that followed, though slow at first, has transformed the world in a few hundred thousand years, and appears to be accelerating, some say toward a singularity.

There are many contemporary developments which are rapidly transforming our world, and which promise even more radical transformation in the future.
Those most significant for this evolutionary perspective on the future of knowledge include our approach to Artificial Intelligence, the impact on the evolution of our ecosystem (including homo sapiens) of synthetic biology, and the anticipated stimulus to the development of autonomous, intelligent and perhaps self-reproducing systems arising from interplanetary and ultimately interstellar exploration and proliferation.

This monograph anticipates the effects of these transformations and takes cognisance of the most recent advances in our understanding of language and logic to offer an epistemological synthesis designed for a world in which the dominant intelligence originating in life on earth is synthetic and is distributed across a vast but still fractional part of our galaxy.
To draw out the supposed implications of these expectations for the nature of knowledge, how it is gathered, managed and applied it may be useful to look in greater detail about that distant future.

\section{3000 AD}

I'm going to paint a picture of certain aspects of the continued evolution of intelligence into the future.
It will sound like a fantasy of the distant future, but evolution is on a roll, it won't be on the same timescale as the billions of years evolution took to come up with homo sapiens.

Life on earth has been evolving for over 3 billion years.
Intelligent life is pretty much the last arrival in the scene, dating back maybe 300 thousand years.
Evolution has not been a uniform process, the nature and mechanics of evolution have evolved along with the evolving ecosystem.
The advent of language fuelled cultural evolution, which moves much more rapidly than biological evolution.
It yields technologies likely to accelerate biological evolution as well as enabling the evolution of non-organic intelligence, and synthetic self-reproducing systems and ecosystems.

As of writing, other technological capabilities are emerging which will place pressures on all these accelerating forms of evolution.
Efforts are under way to turn homo sapiens into a multi-planetary species, and there is little doubt that the rationale for that adventure will motivate a subsequent escalation to interstellar settlement.

Already it is clear that just to establish a settlement on Mars, a considerable amount of work will need to be accomplished by autonomous non-biological agents (probably including android robots) in preparing the ground for human beings.
At the very least, it is expected that a return journey will have to be fuelled from resources at the destination, and the manufacturing process will need to established and completed.
On the first missions it is likely that the spaceship may be the only human habitable environment.




The important thing is the trajectory, which is also speculative.
If that's completely implausible to you (and it is off the wall) then the sequel is in trouble.

The picture drawn here of 3000 AD is an extrapolation.
It begins with the expectation that certain technical developments under way early in the 20th century mature, probably within that century, and that these set in course the proliferation of intelligent (but not necessarily biological) progeny of homo sapiens across an expanding region of the Milky Way.

The technologies of particular significance here are:
\begin{itemize}
\item{Artificial Intelligence}
\item{Synthetic Biology}
\end{itemize}

The effect of these technologies, as they mature during the 21st century is to completely change and radically accelerate the nature of
evolution.
It does this in two ways.

Synthetic biology provides the final nail in the coffin of the Darwinian conception of evolution as arising from random variation and natural selection.
It enables evolution by design, involving intelligent design of variations and intelligent selection.
This is 



\section{Evolution}


Evolution selects those organisms which are motivated and capable of proliferation, and among the more advanced organisms one aspect of this is the desire to explore every part of our accessible environment and to extend the limits of our reach.

In the year 3000 AD radio emissions created by life on earth may have reached 1000 light years across the milky way.
Our telescopes will have continued to improve and will allow us to peer across much larger expanses of the universe, but the physical probes which we send out to examine distant objects more closely are unlikely to have reached further than 100 light years away.
Very early in the exploration of space it was realised that before human beings



\chapter{Introduction 2024-03-30}

Knowledge makes us what we are.
Knowledge comes in many forms, some more important than others.
The knowledge of protein structures, coded as DNA in the genomes of biological species, is a precursor to all the forms which followed.
After the first living organisms appeared, evolution crafted ever more elaborate life forms, knowledge of whose structure and operation were represented in their genomes.

After billions of years of this slow moving evolution, a species appeared which transformed the evolution of knowledge, \emph{homo sapiens}.
Among the enhanced intellectual capabilities enabled by an enlarged cerebral cortex was the gift of oral language, which put a fire under \emph{cultural evolution}.
Among more diverse applications, oral language was epistemologically transformative because of its declarative capabilities.

Declarative language is important because it is a vehicle for objective knowledge, and can convey information which retains its utility when far removed from its origin in time and space.
This provided a bedrock for a body of knowledge which could grow continuously, ultimately empowering \emph{homo sapiens} in ways which would otherwise have been inconceivable.

Knowledge which has accumulated and evolved over billions of years since the origin of life on earth.
The evolution of life on earth has also been the evolution of knowledge.
In that evolution there have been discontinuities.

One of those is that of abiogenesis, the stage at which the coding of protein structures as DNA in the genome of the first living organisms kicked off the biological evolution which eventually made man.
Another accompanies the evolution of \emph{homo sapiens} in the form of oral language and the culture which it enabled, a completely distinct form of knowledge from that aggregated in the genomes of living species.

The knowledge created by humans and propagated by culture did not remain oral, and its various manifestations since then have accelerated its spread and growth, while remaining confined to the product of just one biological species.
It enabled that species to master their environment by building tools and machinery which surpassed the meagre physical capabilities of its masters.

\chapter{Introduction 2024-03-28}

Epistemology is the theory of knowledge.
Much has been made in its history of the need for justification of a belief before it can claims to be knowledge.
But there is no single standard which will tell us when a belief can be of practical importance, and sometimes even the certainty of falsity does not diminish the utility of a theory.

This monograph is about the aggregation and exploitation of bodies of knowledge, considered as such in a broader and more colloquial sense than may be entertained by philosophers.
In this broader sense the term \emph{model} is important.
A model reflects only certain features of its subject, and will usually only provide a certain degree, short of absolute accuracy in its representation of those features.
Nevertheless models in general constitute knowledge of their subject matter, and have a utility which will vary depending on the modelling technique employed.
On the one hand mathematical models can be very precise, on the other, \emph{papier mach\'e} models may be very approximate and serve only to give a rough visual impression.



\chapter{Introduction 2024-03-26}

This monograph is presented as \emph{analytic philosophy} with a constructive agenda, as is suggested by the idea of \emph{synthetic} epistemology.
It is also foundational, a kind of \emph{first philosophy}, with an emphasis on the foundational status of epistemology, the theory of knowledge.


Epistemology cannot come before knowledge in a temporal sense.
If temporal precendent were a requirement for \emph{first philosophy}, then it would be a lost cause.
But the aggregation of knowledge is not linear.
From time to time it is beneficial to raise the bar, with the benefit of experience, and to restructure the knowledge we have before moving on.
This doesn't happen \emph{en masse}, it is more a continuous if intermittent process, in which some press forward with accepted methods while others doubt and seek more rigorous methods or transform parts of our heritage to address difficulties which have become apparent.
There are many examples of this, some of which will I will touch upon later.

In the earliest stages of such restructuring, epistemology may have something to contribute, and it is these early epistemological preparations which might then deserve the status of \emph{first philosophy}.

Those analytic philosophers who are willing to entertain foundational thinking differ among themselves as to which aspects of philosophy are the most fundamental.
Epistemology may often have held sway, but metaphysics, and the philosophy of language and of logic have had their enthusiasts and moments of primacy.
The story I have to tell, though presented ultimately as epistemology, involves elements from all four, none can provide an independent foundation.

Many concepts important to the discussion have very varied interpretations, not least the concept of knowledge.
The demand of epistemologists for justification as a condition for knowledge is often glossed over in other contexts, and the question of what constitutes justification is likely to elicit context dependent answers which belie its ostensibly binary character.
Similarly, knowledge may be characterised by philosophers as a kind of belief, while in other contexts it is accepted that libraries are repositories of knowledge irrespective of whether anyone believes or can prove the content of the volumes on the shelves.
Variation may be found in the application of the concept of \emph{language}, with linguists perhaps insisting than no species other than \emph{homo sapiens} has mastered language, with others accepting as rudimentary languages much more primitive systems of communication.

These and many other concepts with elastic boundaries contribute to  the informal discourse which is woven here around attempts to provide precise descriptions of fundamental concepts.
Because ultimately my synthesis rests upon a logical structure built with carefully defined technical terms, it is important to distinguish the technical terms which will be critical to this structure from the less formal and loosely defined terms which are used to describe background information such as the history of ideas without which the construction could not have been imagined.

In many cases, terms which seem crucial will left informal, which new terminology is introduced where precision is of the essence.
Notable in this regard is the term \emph{knowledge} which is of course the central concept of epistemology.
The term is used in the special language of classical epistemology for those truths which have been conclusively established, assuming a sharp, binary distinction according to the grounds we have for supposing something to be true.
That use of the term will not be adopted here.
A second concept which we will treat informally is that of language.
Instead of choosing between the narrow conception of language which is common among linguists and the broader usage among those with a general interest in means of representation and communication, I will use the term \emph{declarative language} and give to that term a precise meaning.

\chapter{Introduction 2024-03-23}

Knowledge and its prototypes come in many forms.

Among these are the knowledge of protein structures in the genomes of biological species, and the various kinds of memory which progressively evolved in the central nervous systems of vertebrates.

A very special kind of knowledge appeared in very recent times in \emph{homo sapiens}, expressed in declarative language, and facilitating that accelerating explosion of knowledge in human culture.




Evolution baked into the genome of each species knowledge of the chemical structure of the proteins necessary to construct, maintain and replicate that kind of biological organism.
This is a very special kind of knowledge which

After 3 Billion years, the evolution of life on Earth, crafted the neuron and the nervous system and the evolution of memory began in the vertebrates.
Memory must be memory \emph{of something} and if accurate, knowledge of it.
As the vertebrates continued to evolve over the following 500 Million years, the size and complexity of the nervous system grew.



The evolutionary mechanisms which lead to this form of knowledge are not well understood, but the evolution of those mechanisms was to be a part of the ongoing evolution of life.
That ongoing evolution depended upon accurate replication of the genetic blueprint with just enough variation by design or accident.

Those living organisms depended for their reproductive success on their ability to respond appropriately to the opportunties and threats in their environment, demanding a, perhaps fleeting, sense of its key features.
These kinds of proto-knowledge could not have been realised without the prior evolution of the genetic knowledge store.

After perhaps three billion years of evolution of this kind of proto-knowledge, special biological machinery evolved in the vertebrates extending the time-spans over which local knowledge might be gathered and exploited, the neuron and eventually the central nervous system.


The pace of evolution of knowledge was advancing, and the next major transition occurred just half a million years later, when in \emph{homo sapiens} declarative language appeared.
The stages which followed were to be compressed into at most a few hundred thousand years, leading us to the present where we stand at yet another point of inflection in the trajectory of knowledge and knowledge processing.

Declarative language, when it first evolved, was an approximation to an ideal the articulation of which has itself continued to evolve to the present day, but which is first seen in the historical record in ancient Greece during the birth of theoretical mathematics and analytic philosophy.


@@@@@@@@@@@@@@


There are progressions among these forms of knowledge.
Some could not have appeared had not a distinct prior form of knowledge existed (a causal dependency).
Some of these forms of knowledge may be thought to subsume or supersede others.
These progressions are important to this work, which anticipates and seeks further progression.

\chapter{Deduction}\label{ChapDeduction}

Deductive inference and the related concept of logical truth are central to the ideas presented here, and play a fundamental role in the epistemological synthesis.

The word `deduction' or its Latin predecessor only began to be used to describe logical inference during the period of the Medieval Scholastic philosophers, but the practice of deduction is coeval with propositional language, perhaps 300,000 years ago, and the study of this kind of reason dates back at least as far as Aristotle in the 4th Century BC.

Deduction can be formal and syntactic (concerned with the structure of linguistic expressions)  or informal and semantic (concerned with the meanings of linguistic expressions).
Informal deduction, has been around as long as \emph{homo sapiens}, but formal deduction was unknown until about 150 years ago.

Deductive inference is peculiar to declarative or propositional language, so an account of deduction must begin with a few words about this kind of language.

\paragraph{Declarative Language}

Declarative or propositional language is the kind of language which makes use of sentences to make claims (or express propositions) about some domain of interest (sometimes, the world, more often some part of it or some fictions about it).

These propositions are sometimes true, depending on how the world is and whether the world conforms to the claim expressed.
We may therefore consider an important part of the meaning of a sentence to lie in the collection of possibilities under which the sentence is true.
This aspect of meaning is called its semantics, and this way of accounting for the semantics of a propositional language is called \emph{truth conditional}.

When we think of semantics in this way an important relationship between sentences or propositions can be made precise, which is called the relationship of \emph{entailment}.
A sentence A entails some other sentence B if the truth conditions of A are all truth conditions of B, so that whenever A is true, so is B.
This is to say, that any possible state of the subject matter under which A is true is also one in which B is true, whenever A is true, so is B.
This legitimises an inference from the truth of A to that of B, and under these circumstances, i.e. when A entails B, we call the inference from A to B \emph{deductive} (in the informal semantic sense which which exclusively characterised deduction for its first quarter of a millenium.

We will see later that often the term deduction is reserved for a kind of inference which is syntactically rather than semantically characterised, which characterisation is only possible in a formal language.
However, the invention of formal languages in which these criteria can be defined is very recent, and it is generally recognised that mathematical proofs for the two millenia prior to the invention of formal languages were deductive but informal, and it is convenient therefore for the purposes of this monograph to work with a semantic conception of deduction, while recognising the enormous importance of the transition from informal deductive reasoning to strictly formal deductive systems.

A brief historical narrative may help to make clear the development of these ideas up to the present, as a basis for the epistemological synthesis which follows.

\ignore{
Deduction is often now considered a syntactic feature of formal deductive systems.
These are however very modern, dating back less than 200 years, whereas the use of deductive reasoning in Greek mathematics dates backmore than two millenia before the first formal logics, and similar if less rigorous and elaborate use of deductive inference may date back as far as the origins of human language.

An explanation of deduction depends upon an idealisation of important features of natural languages, in particular on the supposition that indicative sentences have definite truth conditions, and that competence in a language necessarily yields insights into a special relationship between sentences which we call `entailment'.

This kind of language is presented here as an idealisation of important aspects of the kinds of natural language whose origin coincides with that of our species homo sapiens, and is found in no other known biological species.

First a few words about the nature of deduction and the importance it has had in the advancement of \emph{homo sapiens} to the levels of well-being and prosperity which we now enjoy.

Deduction is a form of reasoning which is available in declarative or propositional language.

Ludwig Wittgenstein spoke in his philosophy of the variety of language, going beyond Augustinian conception of language as working by the use of sentences which express relationships between things signified by the constituent words and phrases of the sentence.
His broadened conception of language likened the variety of linguistic discourse to the idea of games, in which rules govern the conduct of the game in which the moves do not necessarily have any significance beyond the role they play in the game.

While accepting this broadened conception of language, we are here concerned specifically with the kind of language in which deductive reasoning can take place, which has a very important place in human history and in the betterment of humanity.
That kind of language is called declarative or propositional language, the process of deduction being one in which from one or more propositions which are supposed true, further propositions may be inferred which will of necessity be true if the premises are indeed true.

@@@@@@@

The chasm between the linguistic abilities of humans and those known in any other species is so great that it is likely that the evolution of language went hand in hand with the rapid growth of brain size in genus homo leading up to the emergence of anatomically modern homo sapiens.
At this point the innate capabilities of homo sapiens must have laregly sufficed for the kind of complex propositional language which is now pervasive among humans.
There is close coupling between competence in such language and the ability to perform elementary deductive reasoning, because the most elementary deductions flow from an understanding of the meaning of taxonomically related concepts.
One cannot be said to understand the concept of horse if one does not know that horses are mammals, and that knowledge yields the natural (deductive) inference from knowledge about mammals to knowledge about horses.

}

\paragraph{Some History of Deduction}

Deduction has a long history.
This introductory account will describe it in terms of four stages in the advancement of our understanding of its practice and theory.
It is convenient to consider deductive inference under the following four headings:

\begin{itemize}
\item informal
  
  The informal practice of deductive dates back to the origins of language early in the history of homo sapiens, and continues to the present day.
  It requires no understanding of the nature and significance of deduction, but is simply a part of what constitutes a good understanding and competence in a natural language.
  It is practically useful, but will fail if pressed too far, and can be manipulated to yield whatever results we find convenient.
  
\item systematic

  Elaborate application of informal deductive inference is perilous, but subject to certain rules of hygene concerning the context in which deduction is conducted, can be made safe and productive.
  The first documented use of systematic deduction subject to appropriate and effective guardrails appears in the development of mathematics as a theoretical discipline in ancient Greece, beginning with the philosopher Thales, round about 600 BC.
  Over a period of approximately 300 years, mathematics and the axiomatic deductive method were expanded and refined, culminating the Euclid's compilation of his ``Elements'' \cite{euclidEL1}, in which the axiomatic method was clearly documented and 300 years of productive research were gathered together and demonstrated by that method.

  The key features of the `systematic' deduction found in Euclid's Elements serve in the first instance:
  \begin{itemize}
  \item to limit the language to some clearly intelligible domain
  \item to limit the premises from which deduction may proceed to a fixed set whose truth is clearly warranted in that chosen subject matter, and
    \item to ensure that every deductive step proceeds from an identified small set of these premises or from theorems previously proven from them
  \end{itemize}
  This was done using explicit \emph{definitions}, \emph{postulates} and \emph{axioms} which characterise the subject matter and provide the premises from which all theorems are ultimately derived, and by ensuring that in each step of a proof the premises from which the conclusion is derived are identified and directly entail the conclusion.

  Systematic deductive reasoning, with varying degrees of rigour in the observation of the kind of strict axiomatic structure adopted by Euclid, have a long history of success in the continued development of mathematics since Euclid down to the present day.
  They have also been attempted occasionally in other domains, generally with much less reliable and convincing results.
 Factors mitigating against success include the difficulty in providing good definitions or axiomatic characterisations of the key concepts, and the dependence of reasoning in the field upon wider considerations (e.g. an understanding of the empirical world
   
\item formal 

  The axiomatic method provided a context in which informal deduction could be applied safely and productively in much more elaborate and complex derivations than would otherwise be reliable, primarily by ensuring very definite subject matters and strictly limited premises from which deductions could proceed.
  The nature of the deduction which is involved was nevertheless, just that informal deductive inference which is intuitively practised by any competent speaker of a natural language.
  This remains to this day the predominant way of demonstrating mathematical theorems.

  The advent of modern science, most particularly in the theories of mechanics and gravitation of Newton, opened up a plethora of practical applications as the industrial revolution approached and gathered steam.
  Those applications depended on new mathematical techniques, notably on the differential and integral calculi and the whole new mathematical discipline called \emph{analysis}.
  These seemed to depend on innovations in the concept of number which were poorly understood, of doubtful coherence, and which were more difficult to characterise in coherent principles from which mathematical results could be deduced.
  At the beginning of the 19th Century mathematicians set about putting their house in order, first of all reformulating the required mathematics in clear terms which avoided the controversial idea of infxsinitesimal numbers, and proceeding from there to the clarification of the system of real numbers and of the mathematical concept of \emph{function} which had assumed greater significance as functions morphed from certain kinds of mathematical expression to bona-fide mathematical entities upon which operations like differentiation and integration could be performed.

  This progression ever deeper into the foundations of mathematics then took a more philosophical turn, as the philosopher Gottlob Frege entered into a disagreement about the status of mathematics which had appeared in the previous century as a claim by David Hume, rejected (in different language) by Immanual Kant, which was re-asserted by Frege to the effect that mathematics was the body of knowledge derivable from the definitions of mathematical concepts by purely deductive inference, and consequently that the truths of mathematics are \emph{logical} truths.
  
\item foundational

  A formal language with a sound formal definition of proof rules provides a reliable account of deductive inference, but deductive inference is only as good as the context in which it is conducted.
  If the premises from which deductions are made are logically inconsistent, then valid deductive reasoning does not yield true results.

  A logical foundation system solves the problem of ensuring that the context in which deduction takes place is coherent, that deduction is undertaken from true premises or definitions constructed in a safe context.
  Inconsistency is thereby prevented, and the truth of the results is assured.

  A further important feature is practical universality.
  An abstractly codified foundation system provides deductive capability over an abstract syntax suitable for the representation of any sufficiently well-defined finitary language in which deductive reasoning is in principle possible.
  This makes possible a comprehensive knowledge base and permits reasoning in the context of systems or subsystems defined each defined using the most suitable language for the type of system, hence facilitating (for example) the verification of designs of highly complex systems.
  
\end{itemize}

\section{Informal Deductive Reasoning}

There is no evidence of an appreciation of the distinction between deductive and other ways of reasoning until about 600 BC, but the ability to undertake elementary deductive reasoning may be seen to flow from or be implicit in competence in the use of propositional languge.

This has its limits, an appreciation of which motivates the later stages in the development of deductive methods.

The elements of language comprehension which are closely related to deductive competence can be illustrated under three headings:
\begin{itemize}
\item \emph{propositional} reasoning, which flows from an understanding of the propositional connectives, \emph{and}, \emph{or}, and \emph{not}.
\item \emph{taxonomical} reasoning in which an understanding of taxonomical heirarchies yields inference between semantically related classifications.
\item \emph{generality} and \emph{specialisation}, in which inference from general truths to particular cases or general principles are inferred from indefinite truths.
\end{itemize}

The practical impact of deductive reasoning has been effected so far only under the first two headings.
Formal deductive systems have only recently been invented (within the last two centuries) and have so far proved too onerous for widespread use.
Informal deductive reasoning is however ubiquitous and dates back as far as declarative language itself (perhaps 300,000 years).

\subsection{Propositional Reasoning}

The propositional connectives `and', `or' and `not' may be used to form complex propositions by combining or modifying simpler propositions, thus from the propositions `grass is green', `roses are red' and `black is white' we can infer `grass is green and roses are red', and conversely, from the latter, the two former propositions.
We can also conclude from `grass is green' that `grass is green and black is white', and from that compound truth together with the fact that black is not white, we infer that `grass is green'.
These words could not serve their normal purpose in language if the hearers were not capable of making these elementary deductions.

It should nevertheless be noted that the word are not confined in their use to these purely truth functional applications, and in some contexts may carry additional information.
This `and' may sometimes connote temporal sequence, having a sense similar to `and then', and `or' may be exclusive, indicating that exactly one of the two alternatives which it connects is true.
These are some of the pitfalls of informal reasoning which have lead to the progressive refinement of deductive reasoning which is our present theme.

\subsection{Taxonomic Heirarchies}

Closely related to propositional reasoning is reasoning about taxonomical heirarchies.
It is biology which provides the best source of examples of such taxonomic heirarchies.
Thus in biology animals are classified into species, which are classified in turn into genus, family, order, class and so on up to domain.
Within this heirarchy distinct members at each level are required to be disjoint, and their defining characteristics must therefore be logically incompatible.
The relationship between one classification and the classification at the next level to which it belongs is that of conceptual inclusion.
Any member of the lower classification must satisfy all the criteria of the higher level as well as some additional criteria which characterise the lower level classification.
Consequently, a deduction may be made from membership of the lower to membership of the upper.

For example, the domestic horse (equus caballus) and the zebra (equus grevyi) are species of the genus equus.
Thus being a zebra entails being equine, and justifies the deduction from the former to the latter.
It also contradicts being a domestic horse (i.e. entails \emph{not} being a domestic horse) rendering that further deduction sound.

These kinds of deductive reasoning justified by a good understanding of these conceptual heirarchies also apply to relational inclusion.
Examples of these are likely to be more complex and will be less common in everyday discourse, but may be significant in the more elaborate informal reason found in mathematics and science.

\subsection{Generality and Specialisation}

An understanding of the words \emph{all} and \emph{some} also comes with some elementary deductive reasoning.
If something is true of some collection, e.g. all men are moral, then it is true of any member of that group, for example, if socrates is a man, then he is mortal.
The two premises together entail the conclusion and the inference is therefore deductive.

One might also infer from the mortality of all humans that there are some mortal humans, but this depends on there being humans.
We can also establish general principles by reasoning about indefinite individuals of some group, or by transitive reasoning with generalities.

Thus, if all rodents are mammals and all mammals are vertebrates, then we may infer that all rodents are vertebrates.
The premises entail the conclusion and the inference is deductive.
This kind of inference is immediate to those who understand the language, whether or not it is understood that an inference is involved.
This was the case long before we had the concept of or a word for deductive inference.

\section{Systematic Deduction}

There may have been, since the beginnings of declarative language, more than a quarter of a million years of elementary deductive reasoning on the part of homo sapiens before anyone reflected on what was going on.
The first clear evidence of self conscious application of deductive reasoning in a systematic way is drawn from the earliest stages of the development of mathematics by philosophers of ancient Greece, in whose hands a theoretical discipline emerged from what had previously been mostly an aggregation of arithmetic and geometric techniques in Mesopotamia and Egypt.
Thales was the first of those philosophers, working around 600 BC.

The evidence of those first developments is scant, no writings having survived, but as the body of mathematical knowledge grew, and the methods of proof were refined and documented (notably by Aristotle in his Organon), consolidations of key results (``Elements'') began to appear culminating in the Elements of Euclid \cite{euclidEL1}, a classic text whose contents have weathered the test of time and were used in teaching right through into the 20th Century.

The method articulated and practised in the Elements was \emph{the axiomatic method}, and remained for the next two thousand years the highest standard of rigour in the deductive derivation of the principles of mathematics.

Rigour to that standard was to be probably the exception rather than the rule.
For the most hectic periods in the subsequent development of mathematics, the work was driven by the opportunities and needs of scientists and engineers, and more practical criteria than the demand for rigour determined the acceptability of the mathematical discoveries and innovations.

Nevertheless, the ideal was in place, and from time to time, often provoked by conceptual muddle, the more philosophical mathematicians or or mathematical philosophers would offer their criticisms and sometimes provoke or contribute to a retreat from confusion and reinforcement to the foundations.

It was in the wake of one of the most important periods of retrenchment that the next important advancement in the story of deduction.

\section{Formal Deduction}

The success of systematically organised informal deduction in furthering the development of mathematics in the two thousand years after Euclid was remarkable, but it is not the end of our story.

The next important step in that story is the \emph{formalisation} of
deduction.
The formalisation of deduction involved the discovery of ways of checking the correctness of the inferences which made up a deductive proof.

The idea is that without recourse to semantic insights, but merely by the use of effective, well defined, checks on the syntactic form of the inference, its soundness could be conclusively established.
In order for this to be possible, the syntax of the language in which the mathematics is presented must be itself formally defined, and then for that particular formal language, the permitted rules of inference could also be defined in such a manner as to ensure that every inference thus permitted was deductively sound.

Aristotle had made some first steps toward formality in the description of his syllogistic logic, but up until the beginning of the Scientific Revolution in the 16th Century, mathematics was conducted in natural languages, albeit supplemented by a growing lexicon of mathematical concepts.

Three key developments paved the way for a formal conception of deduction adequate for the kinds of informal deductive reasoning required for mathematical proofs.

\begin{itemize}
\item the introduction of symbolic notations
\item the introduction of the concept of a mathematical \emph{function}
\item the interpretation of sentences as \emph{truth valued} expressions, and the mathematical treatment of sentential connectives as forming a boolean algebra
\end{itemize}

The first was the transformation of mathematical discourse through the introduction of symbolic notation, which served mainly to make complex expressions more concise and hence more easily grasped and manipulated by the intellect.
Early steps included the use of symbols for the various operations on numbers, and for equality.
In the the development of algebraic methods, the use of letters for variable quantities or for fixed but arbitrary quantities permitted the articulation of general principles, which could subsequently be used to transform expressions in applications of mathematics to science engineering or commerce.

Symbols for roots, notation for exponents, and quotients, combined with the use of letters for variables or arbitrary constants. 


This required a regimentation of mathematical notation (syntax) sufficient to admit precise syntactic rules for what constituted an elementary deductive inference.

This was first done by the Philosopher Gottlob Frege who devised a notation which he called \emph{begriffsschrift} \cite{frege79}, which he conceived as a completely general system in which deductive reasoning about arbitrary (not necessarily mathematical) concepts could be undertaken.

\begin{quote}
  During the scientific revolution, in the 16th and 17th century, considerable advances in the language of mathematics took place including the first use of symbols for mathematical concepts and entities permitting more concise arithmetic expressions and the transition to algebraic expressions in the work of Viète.
  Early in this period, the study of mechanics and other natural phenomena brought out the concept of function which became central to the developments leading to the differential and integral calculus ultimately perfected by Newton and Leibniz.
  These developments to the language of mathematics paved the way to a reconceptualisation of logic through the perception of sentences as truth valued expressions, and logical connectives as truth functional sentential operators.
  The use of variables in construing expressions as functions then made the treatment of "all" and "some" as quantifying operators over truth valued functions natural and powerful overcoming the many weaknesses in Aristotelian syllogistic which rendered it impotent for the kinds of deductive proof required in mathematics.
\end{quote}

\chapter{Logical Foundation Systems}\label{ChapLFS}

\section{The Inconsistency of Frege's System}

Frege's mission to show that mathematics is logic was underpinned by the slogan:

\begin{quote}
{\it Mathematics = Logic + Definitions}
\end{quote}

This connects with the systematic use of informal deduction for the derivation of mathematics in the following way.
The Logic which Frege had in mind was the formalisation of the notion of deduction which he had published as his \emph{Begriffsschrift} \cite{frege79}.
The main effect of the systematisation offered by adoption of an axiomatic method such as that found in Euclid is to ensure that the context in which deduction takes place is coherent, by ensuring that the principles from which the deductions begin are principles which are true of some subject matter.
This is reflected in Frege's dictum through the requirement that the formal deductions for the derivation of mathematics begins exclusively with the definitions of the mathematical concepts.

Frege undertook the derivation of mathematics using his Begriffsschrift over the next two decades, which was to appear in a multi-volume work entitled \emph{Die Grundgesetze der Arithmetik}\cite{frege93}.
But towards the end of this period, just as the second volume work was in the final stages, the philosopher Bertrand Russell, who was independently working toward similar ends, became acquainted with the work and noticed that a contradiction could be derived in it.

Though Frege's Begriffsschrift was a game changer which ultimately would herald the transformation of logic through the discipline of \emph{mathematical} or \emph{symbolic logic}, its use in Frege's |emph{Grundgesetse} inconsistency 
The lesson learned from the inconsistency of Frege's system

\section{Two Resolutions of the Paradoxes}

By 1908 two quite different resolutions to the inconsistency of Frege's system were published.
Ernst Zermelo's published an axiomatisation of set theory \cite{zermelo08}, and Bertrand Russell published his \emph{Theory of Types}\cite{heijenoort67,russell1908}.

These were very different resolutions, the first belonged to the extand tradition of mathematics as systematic informal deduction, the second was a revised assault on the problem of basing mathematics on a formal deductive system.

\subsection{Zermelo's Set Theory}

The key requirement for a systematically deductive mathematical theory is clarity about the subject matter delivered through a fixed set of axioms and definitions.

In a way Frege's ambition had been a departure from that tradition insofar as he envisaged one axiomatic system in which all the diverse parts of mathematics could be derived by adding only the definitions of the concepts peculiar to that branch of mathematics.
The foundational work on analysis had shown that analysis could be reduced to arithmetic, and Frege's aim was to show that arithmetic could be reduced to logic.

That Frege had chosen to do this via a formalisation of logic was incidental to this central purpose.
It served that purpose by making it easier to see in such a formalisation that the required mathematical theorems really were logically derived from nothing but the necessary definitions.

Zermelo's set theory, as published in 1908 was not then known to be adequate as a foundation for mathematics, and was criticised at the time for the lack of clarity about what `properties' could be used in the formation of subsets using the separation axiom.
It would later be made precise by formulation in what became first order logic, and more powerful through the addition of the replacement axiom due to Fraenkel.

\subsection{Russell's Theory of Types}

Zermelo's axiomatisation of set theory would eventually provide an alternative foundation for mathematics, as it was in due course discovered how more or less arbitrary structures could be modelled as pure sets, but at this stage the motivation was rather different.

Russell, on the other hand, was fully engaged in an enterprise very similar to that of Frege, and at the time he discovered his `paradox' was well advanced in writing \emph{The Principles of Mathematics}\cite{russellPRM} an informal account of the ideas which intended to  supplement with a formal account in a second volume.
After his discovery of the paradoxes, and coming to an appreciation of the scale and complexity of the task, he entered into the collaboration with A.N.~Whitehead which bore fruit as \emph{Principia Mathematica} \cite{russell1913}, a formalisation of mathematics based on Russell's \emph{Theory of Types} \cite{russell1908}.

\section{The Simple Theory of Types}

\section{Church's Simple Theory of Types}

\section{Gordon's HOL}

\section{Definition as Conservative Extension in HOL}

\chapter{Foundational Responses to Scepticisms}\label{ChapFRS}

Scepticism has an important role to play in motivating a focus on rigour which enables us to separate scientific knowledge from psudo-scientific fantasy.
Absolute certainty is nevertheless neither necessary nor achievable, and the kind of radical scepticism which discards the possibility of knowledge because there is always room for doubt, however slender, is corrosive and counterproductive.

In this defence against scepticism I begin by acknowledging that absolutes are unachievable.
The two particulars which I will look more closely at are scepticism about the possibility of precise language, scepticism about meaning, and scepticism about the establishing with certainty the truth of any proposition.

My defence will be qualified.
I will argue that:
\begin{itemize}
\item a certain fraction or aspect of meaning can be made very precise in important languages,
\item that truth in such languages can then be established with very high degrees of certainty,
\item that the certainty extends to the validity of deductive reasoning in any domain in which the language is adequate to support such reasoning,
\item and that the benefits of these certainties spill over into every aspect of the establishment, management and exploitation of knowledge.
\end{itemize}

A relevant phenomenon which I mention in support of these ideas is the difference in attitude which is observable between the attitude to these matters on the one hand of philosophers and on the other among mathematicians, scientists and engineers.

This can be illuminated by observing the work done on foundations by mathematicians in the 19th Century, and the research on ``full stack verification'' by Computer Scientists in the 20th Century.

\subsection{The Rigorisation of Analysis}

After a period of stagnation during the middle ages, mathematics once more began to progress under the stimulus of the Scientific Revolution.
The late 15th and 16th Centuries saw unprecedented levels of original work in mathematics.
Among the stimulae for this were military applications during a period of proligfic warfare in Europe after the invention of gunpower had made an understanding of ballistics crucial to the effectiveness of artillary.
There was also great demand for improved methods of navigation for shipping, at a time when measuring longitude was neither reliable nor precise.
Understanding and predicting the motions of the moon and the planets was crucial to improving navigation.

Mathematical notation became symbolic, the concept of mathematical function originated, and a century of research by many prominent mathematician progressed methods for manipulating functions which were finally perfected by Newton and Leibniz in the differential and integral calcului.

\chapter{Knowledge Through a Deductive Lens}\label{ChapKTDL}

\part{Attic}

\chapter{Introduction 2024:03:22}

Knowledge comes in many forms, which have evolved along with the evolution of life on earth and its various capabilities for gathering and, retaining and exploiting knowledge about its environment.
One important line of development follows the evolution of memory as the central nervous systems of vertebrates evolved over the last 500 Million years, in which many new capability of the brain enabling the adaption of the organism to new environmental challenges and opportunties demanded new kinds of memory for the necessary environmental knowledge \cite{murray2017evolution}.

Towards the end of this long evolutionary path, after a period of rapid growth in the size and capabilities of the hominid brain, just a quarter of a million years ago, came homo sapiens and a new kind of knowledge.
That kind of knowledge came with the sophisticated linguistic talents of homo sapiens and is called \emph{declarative language}.

This revolutionary new way for animals to communicate with each other enabled an oral culture in which important knowledge could be passed between cooperating humans and from one generation to the next.
It made the transfer of physical skills such as the making of stone tools more efficient, boosting that early manifestation of culture, and enabled the growth of a much richer oral culture, which would ultimately transform the condition of man on earth.


\chapter{Introduction 2024:03:21}
\section{First Philosophy}
This ``epistemological synthesis'' is an exercise in what has been called, after Aristotle, `first philosophy'.

The idea that there should be a \emph{first} philosophy is supported by there being a part of philosophy, \emph{epistemology}, which studies knowledge, and usually involves elements which are explicitly or implicitly normative.
Philosophy (from the Greek for \emph{love of knowledge}) as a whole, might be expected to benefit from any insights into epistemology and to be subject to any norms which it might promote.
This is not the only rationale (and it was not Aristotle's) for first philosophy, and much more support for the idea will be presented in due course.

Against the idea, the principle arguments may be those sceptical arguments which in various ways deny the possibility or desirability of first philosophy.
A major contemporary tendency has derived from the idea that epistemology, as the study of an empirical phenomenon, should be studied by the methods of empirical science, and is in no way prior to empirical science.
This is, of course, what is done in the interdisciplinary field of cognitive science, but its not clear why this would rule out a contribution from philosophical analysis rather than from experimental science, or why one might not consider such analysis independent of empirical science or even prior to it.

As a first philosophy, epistemology might be expected to proceed \emph{in vacuo}, independently of any other source of wisdom.
This is of course, impossible, \emph{pace Descartes}.
It is a problem for all \emph{foundational} enterprises, that foundations must be themselves supported in some way.
A distinction may be made between the large amount of background knowledge in the context of which foundational work is conducted, and those features which are identified or selected as the fundamental components of the construction.

The epistemological synthesis here is undertaken in the context of and under the influence the extended history of the evolution of life on Earth.
This history does not play a formal role in the synthesis, but it is crucial in informing and motivating its features, and in making clear their nature and role.

The most fundamental features of this synthesis do not belong exclusively to epistemology.
There are four philosophical disciplines which contribute to those fundamental features, \emph{metaphysics}, \emph{philosophy of language}, \emph{logic} and \emph{epistemology}, but in their joint contribution to the foundations minimal contributions from each of these are required.

The synthesis builds an approach to all kinds of knowledge upon one very special kind of knowledge.
That kind of knowledge appears very late in the evolution of life and of knowledge as it appears on Earth, and will be spoken of here as \emph{propositional} knowledge, by which is meant, that kind of knowledge which is found in true declarative sentences.
It appears therefore, only with fully fledged languages, probably co-eval with homo-sapiens.

\chapter{Introduction 2024:03:19}

Epistemology, the philosophical study of knowledge, has often been anthropocentric and psychologistic.
This may be seen in the not so distant past in the debate around the conception of knowledge as `justified true belief', which if accepted would place knowledge exclusively in the mind or the brain of human beings.

The epistemological synthesis which I offer here is neither.
It is a conception of knowledge compatible with inorganic intelligence, operating with knowledge often held in widely accessible shared repositories.
In it knowledge is not a psychological phenomenon, and is not the exclusive prerogative of homo sapiens, or even of the living.

In detaching my synthesis from human psychology I take knowledge to encompass all those cases where information about some entity or system is represented or modelled in a distinct structure.
From a more traditional epistemological perspective the locus for such representation would be the human mind or brain, but my synthesis will lean in other directions.
There are many uncontroversial examples of such knowledge representation, such as the knowledge to be found in printed books, and that we now find in electronic data processing systems, sometimes as different physical renderings (\emph{in silico} perhaps) of books also published on paper, sometimes as purely electronic publications.

You might therefore suppose that I have drifted from epistemology into information science, but I affirm here that semantics, truth, and assurance (related to `justification' but not so binary) are essential ingredients in the proposed synthesis; bits and bytes are all we need for information.
I am positioned at the philosophical end of cognitive science, despite the difficulty in detaching the word `cognition' from its mental connotations and associations.
A more easy association is with that field of Artificial Intelligence known as \emph{Knowledge Representation an Reasoning}, with its 35 year history of international conferences\footnote{See \href{http://kr.org/}{kr.org}}, relative to which, styling this work \emph{epistemology} reflects a broader and more fundamental philosophical perspective.

My epistemological synthesis is not just a perspective on what knowledge \emph{is}, and how it may be acquired managed and applied.
It is a suggestion about what knowledge \emph{might be}, and how we might evaluate, manage and apply it in the future.
Though not one intended for universal application, not for everyday use, but rather for science, engineering, commerce and any other domain in which deductive reasoning might sometimes seem appropriate.

\section{Epistemic Evolution}

The possibility (or inevitability) of change in these matters is underpinned by evolutionary thinking.
The particular synthesis offered will be presented as a natural continuation of the evolution of knowledge, of language, culture and of evolution itself over the last four billion years.

In that long evolutionary history there are a small number of significant landmarks which will help me sketch the central features of my synthesis.
These are major advances in the ways in which knowledge can be represented, in which a novel manner of representation becomes available  because of the achievements of evolution.
Sometimes these new methods are sufficiently advanced that they subsume and expand everything which has gone before.
It is just such an advance which I believe to be upon us, as the world teeters toward the realisation of self-reproducing artificial super-intelligent systems.

\begin{itemize}
\item{The Genome}
  
  A repository of detailed knowledge of the chemical structure of proteins essential to each life form, a blueprint for the construction of self replicators.
  
\item{Memory}
  
  A dynamic store of knowledge about each individuals environment essential for survival, evolving to provide special memory facilities for each new capability of these life forms.
  
\item{Language}
  
  A means to share knowledge facilitating the growth of culture as a shared survival resource moving faster than biological evolution.
 The mastery of declarative language also comes with a competence in elementary deductive inference.
  
\item{Axiomatics}

  Many factors contributed to that exceptional environment in which the ancient Greeks laid the groundwork for the philosophical study of knowledge, and transformed mathematics into a theoretical science through elaborate and extensive deductive reasoning in the context of \emph{the axiomatic method} seen in Euclid's \emph{Elements}\cite{EuclidEL1}.
  This paved the way for modern science and engineering and that human prosperity which we now enjoy.
  
\item{Foundations}


  
\item{The Deductive Paradigm}
  
\end{itemize}


\paragraph{The Coding of The Genome}
The first of those milestones in the evolution of knowledge is the origin of life, abiogenesis.
Not only does pre-biotic evolution craft an organism capable of surviving and reproducing in the primordial bioshpere, it devises a specific and precise way or recording the exact chemical composition of the complex proteins necessary for those living organisms to survive and for their self-replication.
This is the coding of protein structure as DNA molecules, a coding with a clear semantics which is implemented operationally by ribosomes which in the context of a living organism construct the proteins coded by these DNA sequences.
The complete hereditary information which determines the structure and development of an organism (the genome) is not confined to the DNA coding sequences.
Other aspects of the genome are not all understood as well, and may not have quite so precise and intelligible a semantics as the protein codings, but they may nevertheless be thought of as integral parts of a body of knowledge which enables the construction and development of viable organisms.

The encoding of protein structure in DNA has some of the key ingredients which linguists identify as distinguishing human language from the interactions of other species, notably that arbitrarily complex expressions can be formed, there being infinitely many possible chemical structures which a protein might take, and a DNA sequence corresponding to each one.
The codings are also discrete or \emph{digital}, a characteristic which comes to prominence again much later in the evolution of knowledge when information technology is invented.

In that extended conception of knowledge, evolution at that time begins to evolve life through the evolution of the genomes of living species, and it is those genomes which constitute a body of knowledge which is gradually accumulated as life evolves.
The clearest aspect of this is the representation in the genomes of the proteins necessary to the development of the individuals of the species and to their life and reproduction, encoded in DNA.
This includes the proteins which form the ribosomes which manufacture the protein's according to their specification in the DNA.
Full details of how the genome determines the construction and operation of the individuals of the species are not yet known, but advances continue to be made, and are being accelerated by AI as evidenced by Deep Mind's \emph{alphafold} which has automated the previously highly labour intensive process of discovering how a protein folds, a determinant of the properties of the protein.


\paragraph{The Evolution of Memory}
The knowledge which evolution encapsulates in genomes is very slow moving.
It serves the evolution of species adapted to particular environmental niches, but does not enable those species or individuals to adapt to the particular opportunities and threats in their own special nook in that niche as it changes from day to day or on any timescale smaller than the lifetime of an individual.

That kind of adaptive behaviour depends upon individual organisms learning about their own particular situation so that their behaviour can be adapted to it in ways which maximise their reproductive success.
The evolution of neurons and of the central nervous system serves that purpose and creates a new form of knowledge which progresses on much smaller timescales than the evolution of the genome.
Among its many roles in the regulation of the organism, the central nervous system gathers information from the sense organs about the environment of the organism which it makes use of in its struggle to survive and reproduce.

The knowledge in this case is physically represented by the structure of the nervous system, the synaptic connections between neurons and the synaptic weights which determine the significance of each connection.
Whereas in more primitive life forms, behaviour patterns would be largely `hard wired' by the slowly evolving knowledge in the genome, in these more sophisticated organisms could adapt more rapidly by storing important knowledge in the more volatile repository of the nervous system.
It is now thought that, as the central nervous system evolved supporting the adaptation of species to new environmental opportunities and challenges, new kinds of memory were an integral part of those cerebral adaptations.
The history of the evolution of memory, is also part of the evolution of knowledge, for memory must be memory of something and constitutes a kind of knowledge.

\paragraph{Language and Culture} are the next stop on our journey.
The knowledge which an individual garners during his lifetime is valuable not only to that individual, but also to those around him, but is not readily passed to them, and disappears when that individual dies.
It is a part of the adaptability of that individual, but not part of the evolutionary progression of the species (though the capability is).
There is some limited ability to transfer such knowledge, which is evident in precursors to the next major step in the evolution of knowledge.
Culture begins, in all probability, before language, as tools development begins perhaps a million years before language.
for it is likely language co-evolved with homo sapiens a process in which rapid growth of brain size is conspicuous, suggesting an end point (homo sapiens).

\paragraph{Axiomatic Precision}

In the accelerating evolution of Western Culture, the civilisation of ancient Greece marked an inflection point, in which multiple aspects of the evolution of knowledge advanced including the first known systematic thinking about knowledge which can be thought of as the beginnings of epistemology.

For the present history it is the transformation of mathematics from a body of numerical and geometric methods into a theoretical discipline which is most significant.
That transformation depended upon more elaborate deductive reasoning than had been hitherto undertaken, which in turn, in defence against the ubiquitous fallacy of equivocation required precision of language which could only be fully realised through what became known as the \emph{axiomatic method}, perfected after its gradual establishment in 300 years of mathematical reasearch in the compilation known as Euclid's \emph{Elements}\cite{euclidEL1}.

\paragraph{Foundational Universality}

\paragraph{The Deductive Paradigm Shift}

\section{Prospectus}

In the chapters which follow I aim to fill out that preliminary sketch in various ways.
My first objective is to add detail and clarity to the ideas of foundational universality and the deductive paradigm shift, which I will approach through a more detailed historical account of the more recent stages in the development of the key ideas.

A principle result of that more detailed account is reliable methods for making certain aspects of semantics precise, and of then establishing conclusively when sentences in the relevant languages are true.
It is not part of the thesis that absolutes are ever achievable, that one can ever make meaning absolutely unambiguous or truth absolutely certain, but that standards of clarity which are exceptional are possible in a very special if narrow semantic domain, and that within this same small but crucial domain unprecedented reliability of truth can be achieved.

My next aim is to underpin the fundamentals of these ideas, by offering by describing how those claims to clarity and truth can be supported.
These are arguments against philosophical scepticisms in relation to meaning and truth, the most important of which are those which deal with the problem of regress in semantics and justification.

Having in this way underpinned the proposed foundations, my effort will then turn to exposition of how these foundations can serve to underpin knowledge in a very broad sense, arguing that knowledge in general would benefit from being seen through the deductive lense which the foundations provide.

\section{Evolutionary Epistemology}

The synthesis which follows is substantially coloured by evolutionary thinking, and its therefore appropriate to say something about the philosophical tendencies which are known as \emph{evolutionary epistemology}.
As in most corners of philosophy, there is here a diversity to which I cannot myself do justice, and my main aim here is to clarify the kinds of evolutionary thinking which informs my synthesis, to distinguish is from some of the main tendencies in evolutionary epistemology as I understand them and to give an idea of how they contribute to the synthesis.

Knowledge is an empirical phenomenon, and it is therefore appropriate to study by the methods of empirical (or natural) science.
However, though knowledge is not so recent a phenomenon, epistemology, the philosophical study of knowledge dates back less than three millenia to ancient Greece when its most famous pioneer, Socrates asked ``what is knowledge'' and answered \emph{virtue}.

Empirical science had not then been invented, and the philosophy of Socrates, Plato and Aristotle was closer to what later became known as philosophical analysis, far removed from empirical science.
The epistemological debate which ensued over the following two millenia, were not debates about what could be found 

@@@@

Hitherto evolutionary epistemology has fallen into two prominent kinds, according to what it is whose evolution is centered.
Some philosophers have seen truth as emerging from a competition between different theses for assent and promulgation which is analogous to the struggle for survival which drives biological evolution.
Others have recognised that the mechanisms of cognition are a part of the evolving physical nature of biological organisms, and that, along with all other aspects of the ecosphere, are shaped by evolution.

\chapter{Introduction}

Epistemology is the philosophy of knowledge.

Various kinds of knowledge have appeared during the course of the evolution of the ecosystem on this planet.
Evolution itself may be considered as a process which creates knowledge of how to construct organisms which can flourish in the various environmental niches provided by the ecosystem as it develops.
That knowledge is encoded in the genome of each biological species.
Science is gradually discovering how to read the language in which this knowledge is represented, having achieved the first step of recognising how the protein molecules essential to the organism are coded as DNA sequences and the ribosomes which interpret the DNA and construct the specified proteins.

Many forms of knowledge form a part of the evolution of life, and these forms of knowledge may therefore be thought of as having co-evolved with life on earth.
For example, the representation of protein's in DNA was a prerequisite for life to emerge.
Subsequently, the evolution of the central nervous system provided a new substrate for knowledge, and we may think of the evolution of memory over the 500 Million years since vertebrates appeared as a continuing evolution in the nature of knowledge.
When oral and then written language appeared, they provided new kinds of knowledge, which for the first time could be aggregated and propagated throughout a culture and across large geographic and temporal expanses.
The invention of information technology created new media for and new kinds of knowledge.


\chapter{Introduction}

My aim in this monograph is to present some ideas about the future of knowledge.
Epistemology is the discipline which engages in philosophical theories about knowledge.
In offering an epistemological \emph{synthesis} I am putting forward some ideas about how the nature of knowledge might evolve in a future where evolution, both biological and cultural, proceeds increasingly through intelligent design and selection rather than purely by chance variation and natural selection.
This is the world of artificial intelligence.

The synthesis I propose is underpinned by my own perspective on the evolution of knowledge and epistemology over the history of planet Earth, and is a speculative projection on my part of where that evolutionary trajectory will lead us in the future.

It is my aim in this introduction to give as concise a first sketch of the broad picture which constitutes my thesis in as concise and intelligible form as can be hoped for.

That broad picture is evolutionary.
What evolves in that picture in a coupled way are two phenomena, closely related to but more broadly conceived than, \emph{knowledge} and \emph{language}.
The broader idea, within which various kinds of knowledge are encompassed, is that of encodings or models.
These fulfil a role similar to that of sentences in a language in which knowledge may be expressed, but appear in the evolution of life very much earlier than language, which linguists recognise only in \emph{homo sapiens}.
Examples of such encodings appear at the very beginning of life on earth, since DNA provides an encoding of the structure of proteins essential for life, and the genome of an organism may be thought of as a repository of knowledge (or at least an encoding of information) about how that organism is constructed and operated.

The evolution of life on earth takes us at the same time through an evolution of the forms of such encodings of information, and at the same time constitutes a continuous aggregation of data about how the various products of evolution are structured to thrive in the environments provided in a changing ecosystem.

In the early stages of this process it is evolution which is aggregating these kinds of proto-knowledge, and in the process engaging in the design of organisms.
Even the earliest organisms are more likely to flourish if they can respond to the specific opportunities and threats in their immediate environment, and to do that evolve sense organs which collect knowledge about that environment and nervous systems which transform that knowledge into appropriate actions to guard against perils and seize opportunities.
In this way genetically hard wired behaviours become more flexible and the survival knowledge aggregated by evolution becomes supplemented by information gleaned by the organism.
The transient encodings which mediate in this kind of stimulus/reponse behaviour then evolve toward patterns of behaviour supported by persistent memory of aspects of the environment, which demands new methods of encoding information, and new kinds of proto-knowledge.


\chapter{Introduction}

Epistemology is sometimes `analytic' in which case it may be primarily concerned with conceptual analysis, or it may be `natural' in which case it investigates knowledge as an empirical phenomenon by methods similar to those of empirical science.

In speaking of an epistemological \emph{synthesis}, I am offering to contribute to the ongoing evolution of epistemology, in part through departures from what empirical observation or conceptual analysis may reveal of its past or present.

To that end I begin with the suggestion that knowledge consists in representations or models, of some other thing or system, which satisfy relevant criterion of adequacy or accuracy appropriate to their role or purpose.
The representations might be considered linguistic, but linguists have sufficiently rigorous conceptions of what a language is, that a long history of phenomena which are intended to fall under my definition will not involve language as it is construed by linguists.
One example of this is the consideration of a molecule of DNA as a representation of the chemical structure of a set of proteins which are needed for sustaining life in some particular biological species.

Some other terminology for those non-linguistic representations might be appropriate, perhaps in general one might talk of them as `codings', hoping that knowledge represented using \emph{bona-fide} languages would fall under that more copious umbrella.

Just as the linguists conception of language does not suit my purpose, my synthesis does involve stretching the idea of \emph{knowledge} beyond what is likely to be generally acceptable, and for that and other reasons which will become clearer in due course, the epistemological synthesis will to a large extent sideline the perhaps hitherto crucial questions about what exactly knowledge is and how it differs from related ideas such as true belief.
Under the propose conception, knowledge need not be a belief at all, and the binary distinction between knowledge and true belief will go the same way as the distinction between `hot' and `cold', to be replaced by other more exact measures of relevant characteristics.
Confirmation theory is one way in which philosophers have attempted to place metrics on confidence in a scientific conjecture, though this will not feature in my synthesis.







\chapter{Introduction}

In the main current of Western philosophical thought, which is often spoken of as beginning in Ionia with Thales around 600 BC, we find an idealisation of declarative language which is remarked upon by Isaiah Berlin in his writings on \emph{the roots of romanticism} \cite{berlinRR} as extant before The Enlightenment, qualified by thay `age of reason' and then challenged by romanticism and its pre-cursors.

Two principle elements of this were, (among three mentioned by Berlin), that all genuine questions have answers (which can be discovered), and that all true answers to those genuine questions are logically compatible.
I will later talk about a space where these conditions are realised, but this is a tiny part of that very large domain in which 



\chapter{Introduction}

\emph{Epistemology} is the philosophical discipline concerned with the theory of \emph{knowledge}.

In this monograph I will take a broad view of what knowledge and epistemology are.
This conception of knowledge embraces any way in which one thing is represented or modelled by some other thing (usually in an incomplete and/or approximate way).

In order for one thing to represent or model another, there must a correlation of some kind between the states of these distinct entities, perhaps but not necessarily mediated by a causal connection.
So we are not here talking of a single fixed structure, for there to be a correlation we must either be talking about a type of entities correlated with some other type of entities, or at least of a single entity whose state varies in a way which correlates with variations in some other entity or system.
It is this correlation that gives meaning to the representatives or models, and this connection with meanings suggests a broad conception of language and a way of thinking of knowledge in general as linguistic in character.

A broad conception of knowledge may in this way be associated with a similarly broad conception of language.
Doing so does some violence to the meanings hitherto associated with the terms `knowledge' and `language'.
Central concerns of epistemology for most of its history include the distinction between belief and knowledge, the question of how we can know, and indeed whether knowledge is possible.
Though the expansion of academia through the 20th Century resulted in much greater variety in epistemological research, the question whether knowledge could be characterised as justified true belief could still generate controversy.
Similarly, in modern times, many if not most linguists do not regard the ways in which organisms prior to homo sapiens might communicate or constructively collaborate to constitute languages.

It might therefore be appropriate for the broad conceptions of knowldge and of language which I will be addressing to adopt a more cautious nomenclature such as proto-language or proto-knowledge.
I believe that would be cumbersome and ineffective in disarming terminological critique, and will therefore rest exclusively on being as clear as I can about the language I am using, without proliferating too many neologisms in the process.

Some terms I will deliberately use in a broad and imprecise way, in a manner similar to that in which a scientist might use the terms `hot' or `cold' alongside more precise observations or prescriptions of temperature.

Knowledge, language and epistemology are among those terms which I will stretch convention in imprecise ways, but some related vocabulary will in particular contexts be given more precise meaning.

\chapter{Introduction}

\emph{Epistemology} is the philosophical discipline concerned with the theory of knowledge.

For most of its history, epistemology has had an anthropocentric focus, concerned primarily with the kinds of knowledge possessed by human beings with well founded true beliefs, and with the question whether these ever do amount to knowledge.
With the expansion of academia over the last century, epistemological research has broadened considerably, and the epistemological synthesis here offered is another perspective on how that might be done.

From this perspective knowledge is concerned with representation or modelling.
In this it is closely connected with a very broad conception of the idea of \emph{declarative language}.
The broad conception of knowledge which we seek encompasses a similarly broad conception of language sufficient for us to interpret all knowledge as captured by language.

Sometimes a structure (physical or abstract) conveys information about some other structure which, to a greater or lesser extent, is accurate or useful.
In order for this to happen there must be some correlation between the state of the representation and that of the system about which it is representative.
One way in which that correlation might happen is through a causal relationship between the structure and the representation.
Perception is such a causal relationship.
One way in which it might be useful is through a causal relationship between the representation and some action upon the thing represented.
An organism reaches out to capture and consume some nourishment, an action possible only because of the knowledge of its environment mediated by its sense organs, and transiently represented by intermediate states of its nervous system.

The distinction between abstract and concrete entities, a metaphysical distinction, turns out to be important to this enterprise.
When we talk about knowledge, we can talk of abstract representation systems, which can also be understood as representing other abstract structures.
But to possess knowledge, we must have it in concrete form.
The abstract structures might be infinite in size, but a concrete representation will be finite (though not \emph{necessarily} so!).


  








It is not certain that a theory of knowledge can be entirely objective in character, insofar as the attribution of knowledge is a mark of social approval not only intended to signal veracity, but often also a substantial level of justified confidence in that veracity.
As well as this question of veracity, there may be a broader concern with utility, which may attract social approbation even where strict veracity is known to fail.
An example here is the very useful knowledge conferred by Newton's laws of motion and of gravitation despite the present consensus that they are false.

This monograph is intended to offer ways of thinking and talking about knowledge (and of gathering, storing, sharing, and exploiting knowledge)  which may have advantages, and which therefore I hope deserve consideration for the future, in those contexts where the veracity and utility of knowledge are of particular importance.

There are many phenomena which may be considered exemplars of the concept of knowledge in its broadest sense.
The nature of these exemplars has evolved over the last four million years in the context of planet Earth, and most of the knowledge we now benefit from only exists in consequence of the evolution of intelligent life on earth, and of the artefacts made possible by the accumulation of progressively more sophisticated forms of knowledge in the cultures of homo sapiens.

\section*{About}

This book attempts an epistemological synthesis.

I think of this as the kernel for a complete philosophical system which is perhaps easiest understood as a 21st century successor to Logical Positivism, particularly as exemplified by the philosophy of Rudolf Carnap, whose central project was to do for science what he supposed Russell and Whitehead had done for mathematics in their \emph{Principia Mathematica} \cite{russell1913} and to progress a conception of scientific philosophy articulated by Russell in his book ``\emph{Our Knowledge of the External World, as a Field For Scientific Method in Philosophy}.'' \cite{russell21}
That achievement was to realise Frege’s conception of mathematics as the result of deductive reasoning from the definitions of the relevant mathematical concepts.
I will gloss over for the moment the controversy about whether this really was achieved, and the question how any such achievement could be extended to empirical sciences.

Carnap’s conception of philosophy was exclusively as ``logical analysis'' which he construed in terms of the new logical systems introduced by Frege and Russell, and he consequently regarded broad swathes of philosophy as it was and had been practiced as meaningless, particularly the whole of that most fundamental of philosophical disciplines, metaphysics.
There are many differences in my own philosophical perspective, and I am disinclined to police the boundaries of the discipline, but in important ways I share Carnap’s standpoint, and one of those concerns the importance of logical rigour, which depends upon clarity of language and demands strict conditions on the formulation of problems before the questions they raise can be considered meaningful.

Carnap’s concern for precision and clarity place language at the centre of his philosophical system, whereas I have chosen to place epistemology in that place, while addressing those kinds of knowledge which are dependent upon language.
I acknowledge the foundational interplay between all four most theoretical branches of philosophy, metaphysics, philosophy of language, logic and epistemology.
Despite my own sense of proximity to Carnap and the vehement rejection of metaphysics which he maintained throughout his life, I am relaxed about metaphysics and see in Carnap’s philosophy the roots of my own more accommodating stance.

Epistemology is that branch of philosophy which is concerned with the theory of knowledge.
In the last century there has been controversy about whether epistemology should be a part of some philosophical prelude to science, possibly called “first philosophy” (following Aristotle), and therefore conducted \emph{a priori}, or be a part of empirical science, “epistemology naturalised”.
Two developments seem to me to have shaken that dichotomy.
One is Postmodern philosophy, which denied the possibility of objective knowledge (except where politically convenient), and construed “episteme” as a cynical tool for exploitation (and hence as a matter of choice).
Another is the advance of information technology towards the engineering of intelligent artefacts.  The latter development made the representation of knowledge, and hence potentially choices about what knowledge is into an aspect in the design of synthetic cognitive systems (though most contemporary AI research does not do this).

Notwithstanding debate about how AI should be achieved, the aspiration to a systematic coherent repository of scientific knowledge, which is centuries old, is likely to be advanced with the aid of intelligent machinery, and questions about the best way to organise such a repository will, at the most abstract levels, be matters into which philosophical considerations enter.

Though the central thrust of the proposed work is an epistemological synthesis, this is a projection into the future of a process of epistemological evolution without which the proposed synthesis would be unthinkable.  An understanding of that history is an exercise in natural epistemology and a very selective account bringing out the key features which are important to making, understanding and reasoning about the proposed synthesis will be the subject matter of the first part of the book.


\chapter{Introduction}


\section{A}

At the heart of this monograph is the idea of \emph{deductive inference} and the closely related ideas of \emph{entailment} and \emph{logical truth}.

Deductive inference is best defined as a feature of propositional language.
A proposition is a claim about some subject matter which may be true or false, depending on the characteristics of that subject matter which are expressed by the proposition.
In this way a proposition divides the possibilities relating to that subject matter into two groups, those in which the proposition is true and those in which it is false.

Propositional language is that kind of language in which propositions can be expressed, usually by structures called \emph{sentences}.
In such languages it is sometimes possible to make deductive inferences, which allow us to infer from the truth of one proposition the truth of some other semantically related proposition.
Semantics here is that aspect of the meaning of sentences which concerns the truth conditions of the propositions expressed by the sentences.

The details of how a sentence expresses a proposition may be quite complex, the meaning of the language and various aspects of the context in which the sentence is asserted are likely to influence exactly what proposition is expressed and the resulting truth conditions.

Entailment is the relation between two propositions A and B which obtains when the truth conditions of B subsume those of A, i.e. when B is true for all those possibilities in which the A is true.
In this case we say that A \emph{entails} B and we may deduce (deductively infer) B from A.
This scheme can be extended to inferences made from multiple premises.
If the truth conditions of some proposition P subsume those of collection or set of propositions SP, i.e. if P is true in every possibility for which every member of SP is true, then we say that the set of propositions SP entails the proposition P, and the inference from SP to P is deductively sound.

In some cases a proposition may be entailed by the empty collection of propositions.
That would be the case in which the proposition is true in every possibility (for every possibility satisfies the empty set of propositions).

\section{B}

The purpose of this monograph is to present a case supporting the use of a particular \emph{logical foundation system} as a framework within which to organise a comprehensive body of declarative or propositional knowledge.

This is presented as a natural continuation of the historical development of our knowledge about:
\begin{itemize}
\item the foundations of logic, and
\item the technology for information processing and its applications to problems which require human levels of intelligence and super-human capabilitites in brute processing power, algorithmic reliability and accurate total recall.
\end{itemize}

It is not intended that this monograph progress the technical issues involved.
Established knowledge of these matters suffices for my purposes.
My aim will be to limit the detail presented to such as may be needed to make the presentation intelligible, with references to fuller details for those who seek them.
Consequently, this tract is substantially philosophical in character.
The philosophy is oriented towards explaining and supporting what is in effect an architectural proposal for a  logically structured intragalactic shared distributed body of declarative knowledge (though few of the ideas depend upon the suggestion of scale given here).

\section{Prospectus}

At the heart of this monograph is a specific system which I describe as a \emph{quasi universal logical foundation} around which an epistemological architecture is constructed.

The presentation of this foundational system is given in the first instance through an historical sketch in which aims to introduce key features of the system as they evolved.
In Chapter \ref{ChapDeduction} the discussion takes us from the beginnings of language and informal deductive reasoning through the first systematic applications of deduction to the development of formal deductive systems.
In Chapter \ref{ChapLFS} the idea of a \emph{logical foundation system} is introduced, and single line of development is traced through to the present day.
The idea of semantic reduction is introduced, whereby one language may be interpreted in another, leading to the concept of \emph{quasi universal logical foundation system}, together with some reasons to suppose that the chosen system is quasi universal.
In Chapter \ref{ChapFRS}
In Chapter \ref{ChapKTDL}

\chapter{Introduction}

The prosperity and well-being of humanity is largely attributable to the large brain which evolved over 3 Billion years of life on earth.
It took a while to pay off.
For most of the 300 thousand years since modern humans evolved the main benefit of that large brain was to enable humans to migrate across the globe and survive in almost every available habitat, however far removed from the conditions in Africa where we first appeared.
The brain gave us adaptability, made it possible for us to find ways to survive.
Cultural evolution, which was facilitated by those large brains, and by the linguistic capabilities which co-evolved with them, made it possible for humans to find, refine and preserve the tools and other resources needed to migrate around and settle throughout planet earth.

It was not until the advent of modern science, and the industrial revolution which followed, that the average human was lifted much above subsistance living, just a few hundred years ago.
The modern science upon which the industrial revolution depended was numerical, and therefore required a well developed discipline of mathematics.
Mathematics had been established as a deductive science some two millenia earlier in ancient Greece, the rigorous deductive methods codified by Euclid in his compilation of the Elements \cite{euclidEL1}.
At this time, the theoretical discipline of mathematics was distinct from the useful work which we might now consider applications of mathematics, which were known by the Greeks as \emph{logistikē}.
The theoretical discipline, along with the whole of what Aristotle referred to as theoretical science, was regarded by Aristotle as superior to logistikē because it was undertaken purely for the love of knowledge (which was the meaning of the term \emph{philosophy}).

was later described as using \emph{hypothetico-deductive}\footnote{In which a hypothesis is adopted and its consequences for some experimental or observational context are inferred and checked against what is observed to see whether the hypothesis conforms to reality.} and/or \emph{nomologico-deductive}\footnote{In which an established law of nature is applied to a problem by deducing from it, together with the particulars of the problem, a forecast for the behaviour to be expected.} methods, in which general scientific laws (of which the most influential and fertile were Newton's laws of motion and of gravitation) are tested or applied (respectively) by \emph{deductive} inference.

The industrial revolution was a child of The Enlightenment, which was a high point in the status of reason.
What we would now describe as a peak in the \emph{hype-cycle}, when the expectations of what could be achieved by modern science were at their most unrealistic, neither tempered by any sense of limits on what phenomena would prove predictable nor by much appreciation of conflict between scientistic social engineering and democratic governance.

Notwithstanding the hubris of the age, it was on the cusp of major improvements to the human condition, of an acceleration in the rate of beneficial change the pace of which continues to advance.
Though the conception of \emph{reason} on which this was predicated was not purely deductive, embracing the whole of the then new modern empirical scientific method, deduction plays a fundamental role in both the verification of scientific hypotheses and in the applications to engineering design which drove forward the industrial revolution and its sequel to the modern day.

Despite its fundamental role, deduction was not then well understood, and major developments in understanding the nature and scope of deduction which are crucial to its further exploitation were yet to come.
It is the purpose of this monograph to contribute to an understanding of how deduction can best be exploited in a future transformed by intelligent machinery, and to that end a sketch of the development of deductive reason and its applications will help us understand the opportunities which are now open to us.

The intellectual importance of language to humanity, and the vast gulf which separates human competence with language from our evolutionary predecessors make it unlikely that language could have arisen subsequent to the evolution of modern humans.
The overwhelming probability is that linguistic capabilites were an important if not predominant part of the advantages which drove the rapid expansion in brain size during the evolution of hominins.

\chapter{Introduction}

Though enthusiasts for Artificial Intelligence anticipate that intelligence \emph{in silico} will surpass human intelligence, the emphasis has seemed to be substantially upon \emph{replicating} human intelligence, predominantly by methods which mimic the anatomy of human intelligence, notably ``neural nets''.

Typically it does result in overshoot.
AI is often presented as superior in those areas which it has mastered, beating grand masters at chess and world champion go players.
But I hear few aspirations for doing things in novel and potentially superior ways enabled by Artificial Intelligence and building artificial to 

\chapter{Introduction}

I had at first intended that this monograph, which is in essence a perspective on the logical structure of knowledge, would fall into two parts.
The central message is futuristic.
To understand the ideas and their supporting rationale it is necessary to understand the history leading to them.
Though knowledge may be thought of as a peculiarly human phenomenon, the history of knowledge and its predecessors stretches back a long way, perhaps even beyond the origins of life on Earth, billions of years ago.
That history is best understood through an evolutionary lense.
One thread in the story, for example, is the history of memory.\footnote{Examined through an evolutionary lense by Murray et. al. \cite{murray2017evolution}.}
Memory of course, must be memory \emph{of something}, however imperfect, and to the extent that it is accurate it might reasonably be construed as \emph{knowledge}, no matter how primitive the animal whose nervous system captured and preserved it.

My first intention had been to separate my presentation into one part covering the relevant history, and a second which looking to the future from that base.
This would leave the moment of truth far too late, and I have concluded that a completely different approach is essential.

This introductory chapter is the first fruit of my change of heart, in which I begin with the simplest presentation of the most threadbare history from which an elementary account of my synthesis can be based.
For some readers that will be more than enough, but at least before we part ways they will perhaps have had some inkling of what I am about, rather than tasting only preliminary histories which contain nothing of the ideas which they are intended to support.

\section{Hume's Two Forks}

In the eighteenth century, during ``The Enlightennent'', David Hume, philosopher and historian, made two important epistemological distinctions.

The first, took centre place in his ``An Enquiry Concerning Human Understanding'' \cite{hume48}, which was his condensation of what he thought most important in his prior \emph{Treatise} \cite{hume39}.
In his words, the distinction he drew was between propositions expressing ``relations between ideas'' and those expressing ``matters of fact''.
The first of these is that same category which was later to be embraced in the philosophy of Rudolf Carnap by the word 'analytic', and phrase 'logical truth', a category which had a fundamental importance in his philosophy and will be equally if not more important here.
Hume's ``matters of fact'' correspond to Carnap's ``synthetic'' 

\chapter{Introduction}

We are in interesting times, in many different respects.

The phenomenon of greatest interest in provoking the ideas presented here is the imminence of intelligent machines, and the possibility that these machines will enable a transformation in the ways in which knowledge is gathered and applied.
The interest to me is not so much in how intelligent machines might be devised which replicate those human capabilities which we consider to demand or exhibit intelligence, but in the prospect that information processing technology might facilitate the adoption of better ways of gathering and exploiting knowledge.
The formal basis for the proposed methods is known but is too cumbersome to be widely applicable without considerable advances in the available automation.

I was born into a world which anticipated machine intelligence, but had a limited understanding of how that might be achieved, and of machines which fell short of the brute compute power which would be needed. 
My own interest in the problem dates back to my first days as an undergraduate (though it was not a subject which undergraduates could then study).
It was then that I first learned a little of the work of Alan Turing, a famous logicism who subsequently exerted subtantial influence over the field of artificial intelligence (in ways which he probably did not intend).

That first spell as an undergraduate student of engineering proved short lived, and years later I returned to academic studies to obtain a BA in mathematics and philosophy.
My taking up of philosophy was a surprise to me.
Philosophy struck a deep chord, and gave me a sense of vocation not diminished by my perception that I was completely unsuited to progressing an academic career in philosophy.
This was the beginning of a lifetime of philosophical rumination, and a more or less continuous search for a way to turn those ruminations into substance.

My interests in formal logic, computer languages, artificial intelligence and under it all, philosophy, all date from that time.
They coloured my thinking many years later when I had the opportunity to work with the Cambridge HOL system, then recently developed by Mike Gordon and his embryonic Hardware Verification Group at Cambridge University.

The epistemological synthesis which I present below was seeded by the perception that the Cambridge HOL system possessed many of the desirable characteristics of a representation system for knowledge suitable for use in the kind Leibnizian project for the mechanisation of scientific knowledge which the advent of machine intelligence demanded and would facilitate.

The synthesis is \emph{foundational} in multiple senses, and appropriately for an epistemological enterprise, it digs deep in its response to the kind of academic scepticism to which it is inevitably subject.
Though not so intricate as G.E.Moore's defence of common sense \cite{moore1925,moore1993}, it is in its way similarly grounded, not in the good sense of the common man, but in that of the practical engineer.
Because of its inspiration in (rather exotic) engineering practices, my account begins there, before before starting afresh from a philosophical ground zero.

\section{Purpose and Structure}

\section{Varieties of Epistemology}

This monograph comes in two parts, the first of which is presented as \emph{natural} epistemology and the second as an epistemological \emph{synthesis}.
These are two distinct kinds of epistemology of which the former provides background intended to make the latter intelligible, and identifies particular epistemologically relevant developments upon which the following synthesis is built.

Epistemology is viewed through an evolutionary lens, the synthesis constituting a projection of the evolution of knowledge and epistemology into the future.
The presentations of epistemology and of evolution are reciprocal,
evolution is seen through an epistemological lens, as a progressive accumulation of various kinds of knowledge.
It is not a mere aggregation, the kinds of knowledge gathered progress with the evolution of the cognitive systems involved.

 We are at a point of accelerating advancement (an inflexion point) in the evolution of evolution itself.
Genetic engineering, synthetic biology, artificial intelligence and the prospect of self reproducing systems of artefacts, or even of artificial life, all promise to transform and accelerate evolution.
It is natural if we see evolution, knowledge and epistemology as co-evolving phenomena, to expect similarly radical transformations in our conception of knowledge and our understanding of how it can be gathered, assessed, managed, shared and applied.

Epistemology has hitherto been largely anthropocentric.
It is defined by what philosophers take to be the meaning of the concept of knowledge and related concepts in various of the natural languages spoken by \emph{homo sapiens} (mostly in English).
Idealised conceptions of knowledge arising in that way have centered around the idea that knowledge is \emph{justified true belief}, if only by distancing themselves from that idea.
We have concepts arising from the evolution of life on earth which are psychological or mental in character, essentially linked to human nature.

The evolutionary perspective I adopt here takes us back beyond the beginnings of life on earth, and forward to a time at which most knowledge will be gathered and applied by evolutionary successors to homo sapiens or by inorganic or hybrid systems.
Conceptions of knowledge and hence of epistemology which are decoupled both from human nature and human language are necessary to support a credible projection beyond the geographic and temporal scope of humanity.

\subsection{Knowledge, Episteme and Epistemology}

Once significant bodies of knowledge became a part of shared cultures, the evolution of culture intertwined with that of knowledge and epistemology.
From the earliest days of philosophical reflection on the nature of knowledge, cultural relativism, the observation that distinct cultures disagree about what is and is not true, has been an important source of scepticism about the objectivity of claims to knowledge.
A first exemplar of cultural relativism is found in the sophists who taught philosophy in Athens at the time of Socrates.

A modern philosophy involving such cultural influence in science is found in the idea of a \emph{scientific paradigm} within which `normal science' is conducted, and the idea that such paradigms are overthrown only by infrequent scientific revolutions.\footnote{These ideas are most closely associated with Thomas Kuhn \cite{kuhn2012structure,kuhn2000structure}.}
Because of its specificity to science the scientific paradigm may possibly be a way to think of the epistemological synthesis presented here, which is rooted in logic and mathematics.

Another kind of cultural relativism became influential through the writings of the French postmodern philosopher Michel Foucault \cite{foucault1966order}.
Though he rejected the attribution of cultural relativism, Foucault gave the name \emph{épistémè}, a French derivative of the ancient Greek word for \emph{knowledge}, to those aspects of a culture or subculture which determine what is accepted as knowledge.
In Foucault's hands hegemonic cultures establish and entrench cultural norms cynically devised by dominant groups for the purpose of maintaining an oppressive dominance over less privileged social groups.

In the following I will adopt Foucault's term for the subject matter (if not the entire content) of my epistemological synthesis, dropping the accentuation in consideration of its tenuous correspondence with Foucault's ideas (against which it may be considered a counter).

An episteme is not itself knowledge, but is \emph{about} knowledge, and hence a step towards a metatheory of knowledge, without constituting a full blooded theory of knowledge.
The epistemological synthesis which follows is intended to constitute a proposal for a kind of episteme, supported by sufficient philosophical metatheory to reasonably be offered as epistemology.

\subsection{Precursors of Knowledge}

The progress of science depends upon the refinement of language as necessary to provide the concepts upon which the statement of natural laws depend.
Many concepts belonging to  mathematics are among those necessary for empirical science, but other necessary innovations include the names of structures too small to be perceived by the naked eye, and hence not spoken of in pre-scientific language, and scientific refinement of concepts present in ordinary discourse but spoken of with insufficient precision.
A simple example is the language around heat, which in pre-scientific language may be hot, warm, tepid, cool or cold, concepts inadequate for expressing the kinds of laws which govern the behaviour of gasses or the performance of thermodynamic systems.
For these purposes numeric temperature scales are essential, for which the scientist has to create new terminology not derived from everyday language.

In the study of that natural phenomenon which we speak of as knowledge, philosophers may be reluctant to look beyond the normal scope of knowledge attributions, but science may find that the most enlightening theories may address phenomena of broader extent and demand new terminology.

One motivator for broadening the ordinary concept is the evolution of memory in our ancestors.
A memory one naturally expects, is a memory \emph{of something}, perhaps something observed, and serves to bring back into consciousness something similar to the experience of which it is a memory.


In extending the idea of knowledge beyond the bounds of human cognition, a first place to consider is the nature of memory and its role in those parts of the animal kingdom which benefit from a central nervous system.

\ignore{
The following concepts are useful in characterising the kinds of knowledge and its pre-cursors which become a significant part of the evolutionary history beyond its particular manifestations in homo sapiens.

\begin{itemize}
\item information
\item representation, model
\item declarative knowledge
\item episteme
\item epistemology
\end{itemize}

}%ignore

\subsection{Epistemologies Compared}

Some brief comparative observations may help to place the kinds of epistemology which is intended.

\paragraph{The Western Tradition}

The ``Western'' philosophical tradition is normally held to have begun in ancient Greece at the same time as the initial development of mathematics as a theoretical discipline, which over a period of about 300 years from 600 BC established the axiomatic method and applied it most notably to Euclidean Geometry but also to other parts of mathematics which were gathered and perfected by Euclid in his ``Elements'' \cite{euclidL1}.
A distinctive feature of the mathematical developments was their employment of deductive reason in axiomatic theories, which was part of a broader investigation of nature and the cosmos by reason.

The use of reason beyond mathematics was less successful, failing to secure consensus or to yield results which could stand the test of time, and readily capable of obtaining contradictory results which formed a part of the resulting controversy.

Despite the failures of reason beyond mathematics, philosophers (a designation at the time which encompassed the science of the day) sought the imprimateur of deductive proof for their theories, and Plato and Aristotle both attempted syntheses which made a place for deductive reason beyond mathematics.

Plato's system was 

\paragraph{Evolutionary Epistemology}

\paragraph{Scientific Paradigms}

\paragraph{Foucauldian Episteme's}

\paragraph{Epistemology Naturalised}

\subsection{Epistemology as Cognitive Engineering}

\section{The Application of Science}

It is my aim here to present an \emph{episteme}, which is to say, a conception of a certain kind of knowledge, of how to maintain and apply that kind of knowledge, and to offer various epistemological theses about that episteme which speak to its suitability for the development and application of science and the conduct of commerce and a broad range of other aspects of intelligent life in the 21st Century and beyond.

The presentation will include a number of historical sketches showing the features of the  episteme as natural continuations of well established progressions, of which this is the first lightweight sketch.

The emphasis is here on the application of science, which of course depends upon the nature of that science and evolves with it.
Three stages are considered in which substantial advances in science and in scientific methods were realised which are relevant to the further developments which are embraced by the proposed synthesis.

Those periods are:


\begin{itemize}

\item[600-300 BC]

  The Ionian philosopher Thales (624–546 BC) is generally credited as the first to 

  This is the period from the beginnings of Greek Mathematics and Philosophy in the work of Thales through to the philosophy of Aristotle with his syllogistic conception of demonstrative science and the consolidation and axiomatic systematisation of Greek mathematics in the Elements of Euclid.

\item[1473-1779 AD] The beginnings of modern empirical science beginning with Copernicus through the revolutionary quantitative laws of motion of Newton and the infinitesimal calculus of Newton and Leibniz (which paved the way for the industrial revolution) and into the Enlightenment and the French Revolution.

\item[1800-2100 AD] The period of modern logic, information technology and artificial intelligence which make possible the transition to a conception of logic

\end{itemize}



\subsection{Pre-theoretical Mathematics}

It is generally recognised that before the initiation of axiomatic geometry by the ancient Greeks in the sixth century BC, mathematics primarily consisted in a body of arithmetic and geometrical technique developed by the Babylonian and Egyptian civilisations.

It was thus primarily a body of technique for solving a variety of practical problems, knowledge of \emph{how} to do things rather than knowledge of a body of mathematical truths.

\subsection{Mathematics as a Theoretical Discipline}

It is with the earliest Greek geometers, beginning with Thales and Pythagoras, that a different kind of mathematics was created.
Mathematics became a theoretical discipline building a body of propositional knowledge by the systematic deductive method eventually to be documented in Euclid's \emph{Elements} \cite{euclidEL1}.

The axiomatic method articulated by Euclid was the product of his own systematisation of perhaps 250 years of progress by earlier Greek mathematicians.
As such, it continued a progression in the composition of comprehensive texts was found in works by Hippocrates of Chios, Leon, and Theudius of Magnesia, the latter of which, compiled in Plato's Academy, was probably the standard text for Aristotle's peripatetic school.
Aristotle give many mathematical examples in his writing which are now taken to be from the elements of Theodius and provide our best source of knowledge about the innovations found in Euclid's elements.

From this history we may speculate that though the earliest work of Thales was deductive, the necessary principles for properly organising and conducting were but gradually refined and articulated over that 250 year period, and though the 



This new theoretical approach was to dominate the development of mathematics thenceforth, with varying standards of rigour which we will further discuss.

In the earliest days, the strict features of the axiomatic method were not yet clearly delineated, but 

\subsection{Plato and Aristotle}

\subsection{Empirical Quantitative Science}

During the Renaissance, after a period of revival of interest in the works of classucal antiquity modern science gradually moved ahead of that tradition.
It was during this period that science became unambiguously empirical.
Twentieth century philosophers of science have used the terms ``hypothetico-deductive'' and ``nomologico-deductive'' to describe the emerging model of science.
The former term concerns an approach to testing scientific hypotheses, which consists in deducing particular consequences of a proposed general hypothesis and examining whether those consequences correspond to observed phenonena.
The latter speaks to the application of established scientific laws which proceeds in a similar manner, enabling the prediction of phenomena by deduction from the laws and the particular circumstances of interest.

In this period we see a progression from geometrical to numerical cosmological theories takes place, as we move from geocentric models of the cosmos based on the conception of orbital trajectories compounded from perfect circles with so called ``epicycles', through the similar heliocentric conception of Copernicus, Kepler's more quantitative models involving elliptical orbits (all of which are descriptive rather than explanatory) and ultimately to Newton's theories of motion and gravitation which enabled the motions of planets to be explained in terms of the underlying forces which determine those motions.

The explanation of planetary motions (and many other natural phenomena) provided by Newton's theories fall fully within the nomologico-deductive analysis of later philosophers, and differs from the earlier Aristotelian conception of \emph{demonstrative science} (though the establishment of the natural laws is more systematically empirical than Aristotle had envisaged).



\subsection{The Impact and Application of Information Technology}

Already when the theories of Newton were formulated.



\part{Natural Epistemology}



\chapter{Culture}
\subsection{Epistemology and Rationality}

The philosophical `historian of ideas' Isaiah Berlin, in talking of the roots of romanticism \cite{berlinRR}, gives the following characterisation of 'the Western Tradition' as it stood before the enlightenment of the late 17th/18th Century.

\begin{quotation}

First, ``three legs upon which the whole Western tradition rested'':
\begin{enumerate}
\item All genuine questions have answers.
\item The answers are knowable.  By someone.
\item All the answers are compatible.
\end{enumerate}
\end{quotation}

The extra twist added by the enlightenment as the epistemological dominance of the Catholic Church gradually waned, was that reason rather than revelation, scripture or any other authority, is the method by which these answers should be sought.

This is a remarkable distillation of essence in a period of considerable intellectual diversity,\footnote{For an equally controversial perspective drawing out that diversity, the work of Jonathan Israel is valuable, e.g. \cite{israel2002radical, israel2013democratic}}%
 but is nevertheless a valuable perspective, if only as an aunt sally from which we can learn by trying to understand both its merits and its limitations.

 For the story I have to tell here, it is the first and last of those `legs' which are important, it does no damage to the ideas presented here if some questions cannot be answered.
 The restriction to `genuine' questions is clearly essential, nonsense questions are easy to find, but there is here no hint of how one is to distinguish those genuine questions which have answers from the others.
That true answers to `genuine' questions are logically compatible is closely related to the Law of Contradiction usually attributed to Aristotle, and is essential to the application of deductive reasoning.

 
 This idealisation can be seen to have originated as far back as the ancient Greeks, where its weaknesses had already been exposed, and partially addressed.
It is a picture which holds good in the narrow domains of logic and mathematics, in which not only the theory but also the practice have been substantially realised, and in which the need for logical coherence, and hence the compatibility of all conclusions, is essential. 
In consequence of the achievement in those special domains,  philosophers have sought, unsuccessfully, to apply the deductive (axiomatic) method to other domains, with equivocal results.

In about 600 BC the development of mathematics as a theoretical discipline, and the broader speculations of the pre-socratic philosophers began.
The successes of what we now call \emph{deductive} reason in mathematics, ultimately resulting in a comprehensive articulation of the axiomatic method and the compilation of hundreds of years of solid mathematical progress in Euclid's Elements \cite{euclidEL1}, substantially complied with the ideal which Berlin spoke of above.
But the application of reason in other domains failed, most conspicuously in yielding a consensus on any coherent body of doctrine, even involving the exploitation of contradictory conclusions in demonstrations of absurdities such as the impossibility of motion.

The period of participatory democracy in Athens placed a premium on success in public debating which spawned a class of professional philosophers who earning a living by teaching the oratorical and logical skills.
These were known as `sophists' a term which continues to have derogatory connotations, and among their number were prominent figures whose experience of different cultures made them sceptical of objective truth and adherents of forms of relativism, most conspicuously Protagoras whose most famous dictum is probably ``man is the measure of all things''.

The great classical philosophers, Socrates, Plato and Aristotle, contructed progressively sophisticated systems in response to these precedents.

\part{Synthetic Epistemology}

\chapter{Setting the Scene}

My aim in this part of my monograph is to project the evolution of kneowledge, episteme and epistemology into the future by the description of an episteme, together with philosophical arguments presenting a supporting epistemological synthesis.

To an extent the synthesis stands on its merits in the context of the present, but its fullest support includes elements resting on projections as to the nature of the universe as it unfolds into the future.
In this chapter I present those projections, not of episteme or epistemology, but of the relevant aspects of the future they are intended to serve.
Later chapters will present the episteme and epistemology.

\section{The Future of Evolution}

Technologies are already in place which enable fundamental changes to the way in which biological evolution takes place.
The core mantras of Darwin's account of the evolution of species, random variation and natural selection, need no longer obtain.
Genetic engineering and synthetic biology give us evolution by design.

As well as the accelerated evolution of the intelligent life on earth by these new means, we are now entering the era of intelligent artefacts and likely to see increasing integration of biology and technology.
Our intelligent interventions will not be confined to the transformation of existing organisms, or creation \emph{de novo}.

The direction and speed of future evolution, both of natural and artificial life and intelligence is likely to be further stimulated by the migration of human progeny (both natural and technological) across the solar system and through the galaxy, leading to environmental pressures very different to anything found on earth.

\section{General Artificial Intelligence}

The current bleeding edge of AI is generative Large Language Models, which have shown unanticipated ``emergent'' capabilities which before close scrutiny might lead you to think that the general intelligence sought is almost upon us.
Despite impressive capabilities, it is easy to discover how shallow their understanding of the materials they have mastered is.
Some imagine that by pressing forward this line of technological development genuine intelligence will be realised, but at the same time a great diversity of effort is being undertaken to improve the capabilities in a variety of ways while staying broadly within the paradigm of training neural nets with very large volumes of data.

Google Deep Mind, initially one of the staunchest advocates of neural learning by watching (particularly in games) has made its most impressive recent advances by breaking from that paradigm, in a series of programs sporting the ``alpha'' prefix.
The capabilities of interest here are ones which involve problem solving in simple but combinatorially explosive solution spaces.

\chapter{A Priori Truth}

This part is devoted to defining certain concepts culminating in the idea of a \emph{Universal Foundation for Logical Truth} and to two such foundation systems, first order set theory and the Cambridge HOL logic, which are shown to be equivalent as universal foundations.

As normally presented these logical systems are not equivalent.
There are more logical truths (in the sense defined below) expressible in the standard interpretation of the ZFC axioms for set theory than in the Cambridge HOL logic.
But the notion of Universal Foundation we expound consists in an unbounded sequence of increasingly expressive systems distinguished primarily by the cardinality of their minimal ontologies, i.e. in how many things they presume there to be.
The claim to equivalence is the claim that every logical truth which is expressible in some member of one of the the family is also expressible in a member of the other family.

It is a hypothesis that there are no more expressive universal foundation systems, which is treated in a manner similar to Church's thesis.
Rather than being proven, it survives until some convincing counterexample may be found.
There is of course a difficulty in any logical system which abjures semantics, as to whether an interpretation in set theory can be correct.
Formally there is little doubt that in any system formally consistent, if only in the sense of Post, can be given an interpretation in these universal foundations which is based on identifying truth with provability, but whether this would be philosophically acceptable to those who prefer constructive foundations is moot.

In defining these ideas an important element is the choice of reduction, particularly in relation to semantics and hence logical truth.
Two formal languages are considered equivalent if they a mutually interpretable by functions which preserve meaning, so a relevant notion of meaning must be articulated.

Once the definitions are complete there are two directions of further discussion, which flow from the distinct roles envisaged for the two Universal Foundations which have been chosen to exemplify the concept.

The Cambridge HOL logic is chosen because there is a natural way in which we can define the abstract semantics of some arbitrary language and then in practice be able to reason with that language in the resulting context in HOL.
This is a consequence of the abstract syntax of the simply typed lambda calculus being effectively a universal abstract syntax (aided and abetted by the ability to define new type constructors which correspond to syntactic categories in the required abstract syntax).

First order set theory on the other hand, is more convenient for establishing the semantics and proof rules of these universal foundations.
This helps in addressing sceptical arguments from regress in relation to both meaning and proof in the foundations.
In addressing the regress in meaning and justification, careful but ultimately circular arguments prevail.

\chapter{First Philosophy and Foundationalisms}

I am here engaged in something which may reasonably be considered ``First Philosophy'' and which shares something of the motivations and purposes which earlier philosophers have considered under that heading.

That kind of philosophy which us thought of above all else as a rational enterprise must surely first of all put in place the necessary pre-conditions for those kinds of knowledge which are amenable to deductive reason.
I am therefore concerned with \emph{knowing that} rather than \emph{knowing how}.

I think of \emph{knowing that} generally as consisting in the possession of some kind of model of aspects of its subject matter.
Such models or representations may be of particulars states or of patterns of change  in the form of scientific laws. 
The role of deduction is in the application of such models, which may consist in permitting future states to be inferred from present states given knowledge of the laws, or may establish the suitability of some design by demonstration of its behaviour in its intended use.

This kind of application of deduction is often mediated by mathematics, the role of which is to enable the construction of abstract models which can be applied in the above ways.

For these purposes we need knowledge represented by sentences in a declarative knowledge which has a definite semantics, i.e. in which there is a clear interpretation of the sentences of the language as to what they say about the intended subject matter.
Languages which are suitable for expressing this kind of knowledge may also be equipped with detailed syntactic rules which govern sound deductive reasoning with sentences of the language.

\section{On The Nature of First Philosophy}

To give a sense for what I here offer as ``first philosophy'', or of why I find that term appropriate I will say a few words about how ``first philosophy'' has been construed by some earlier philosophers, namely Aristotle, Descartes and Carnap.

\subsection{Aristotle's Conception of First Philosophy}

``First Philosophy'' is the phrase used by Aristotle to describe the subject of his writings which were to be labelled ``Metaphysics'' by later editors.

Aristotle's con

In Aristotle's time, the scope of ``philosophy'', the love of knowledge, embraced the science of his day, and most of Aristotle's work was concerned with the various sciences, which he classified as theoretical if

The term \emph{First Philosophy} was first used by Aristotle, who used it to describe the matter of the volume which later editors named ``Metaphysics''\cite{aristotleMetap}.

Aristotle had a hierarchic conception of knowledge, lower levels concerned with brute facts close to our practical experience of the world, and higher levels involving gradually increased abstraction.
At the highest level, completely divorced from practical applications,and  exhibiting the highest level of abstraction and degree of true wisdom, was \emph{First Philosophy}.
He saw first philosophy as concerned with `being qua being' or being \emph{in  itself}, rather than being \emph{as matter} or being \emph{as mind} or any other kind of being, and thus as a study prior to any of the sciences concerned with their special subject matters.

Descartes, much later, had a distinct emphasis and character to what he offered as first philosophy \cite{descartes2013meditations}, which connects it more directly with the foundational approach to an epistemological synthesis attempted here.
DesCartes wanted to discard all that had preceded him and start a rational reconstruction from scratch, for which first philosophy provided a starting point.

I use it here loosely, rather than in close correspondence with Aristotle's Metaphysics, because it is suggestive that philosophy must begin somewhere, and hopefully progress onward, rather than being a amorphous mass incapable of orderly presentation.

As well as being appropriately presented in a progressive way, my conception of ``First Philosophy'' includes the idea that some aspects of philosophy are also logically prior to others, and indeed, that a presentation which constitutes a logical progression is desirable (in a primordial sense of ``logical'' which is mere suggestion prior to any conception of what logic might be).

It is furthermore, here coupled with notions of foundation, and thus with something like the kind of doctrine which has been called ``foundationalism'', but which lacks some of the absolutism which is commonly associated with that term, and which makes such foundations susceptible to radical scepticism.

I do not seek an epistemology which looks for absolute certainty, but rather for one which may be an effective basis for the future prosperity and well being of humanity.

If I were to launch immediately into the most fundamental elements of this supposed logical progression, the reader might be left in a state of suspense as to how the various sceptical arguments which might be marshalled against it could be answered.
This preliminary discussion is intended to mitigate the suspense.

One modern response to various scepticisms may be found in G.E.Moore's ``A Defence of Common Sense'' \cite{moore1925,moore1993}.

\subsection{Some History of Foundations}

In contrast with Philosophy, Mathematics is that domain of intellectual endeavour which has the very highest reputation for clarity and certainty.
Since the transformation of Mathematics into a theoretical discipline (rather than a collection of practical methods), realised by deductive reasoning, its reputation has been the envy of many philosophers and has inspired a number of attempts to usurp its deductive imprimatur for the sake of methods and doctrines which fall short of deductive rigour.

When we look under the hood we find that reputation has been achieved by ideas which are broadly (but sometimes explicitly) \emph{foundational}, that there have been wide variations in the rigour of the proceedings and that, arguably, the more advanced mathematics has become, the more it has depended upon clearly articulated and elaborately constructed foundations.

The point of this section is to suggest that the interest of mathematicians and some branches of empirical science and engineering in foundational issues is distinct from that of philosophers.
Philosophers are more likely to take foundational studies as intent on achieving absolute precision of meaning and certainty of proof, making of ``foundationalism'' a strawman readily refuted.
A defence against such foundational scepticism might then be analogous to Moore's rejection of sceptical arguments on the basis of the certitudes of common sense.
It might serve such a defence to examine the practice of mathematicians and engineers, for whom foundational advances are a vital part of how their discipline may be progressed.

Sir Thomas Heath observes, in the preface to his history of Greek Mathematics \cite{heath1921}, that the foundations of mathematics are Greek, and consisted in first principles, methods and terminology.
Our knowledge of early Greek mathematics and of the rigour of its demonstrations comes to us primarily in the compilation by Euclid in his elements, which is the culmination of 300 years of Greek mathematics, and probably not representative of standards uniformly adopted \emph{ab initio}, but rather the result of gathering together 2-300 years of the results of work if varied standards of rigour into a single body of work presented to a uniformly high standard of rigour based on the same fundamental principles and methods.

What we see there, and what we will describe in greater detail concerning more modern foundational work, was not a starting point from which all the research was undertaken, but the result of a period of evolution of methods which ultimately could be codified to provide the body of work with a highly rigorous derivation from a single  foundation.

Axiomatic Euclidean geometery was a high water mark in the rigour of mathematics which was not to be surpassed for over two thousand years, probably because the need was not then felt, despite considerable further development of mathematics since the time of Euclid.

Eventually developments transpired which propelled mathematics at the same time, into its greatest practical significance and beyond the confines of rigour.
This flowed largely from the work of Newton and Leibniz.
Newtonian mechanics necessitated the kinds of operations on mathematical functions which were to be supplied independently by both Newton and Leibniz.
The procedures of differentiation and integration, which operate on functions to give on the one hand, the slope of the graph of the function and on the other the area under its curve.

The definitions of these concept made special demands upon the number system, particularly in the use by Leibniz of infinitesimal quantities, but also in Newton's account of fluxions as `last ratios'.

Doubts about the rigour of the procedure were voiced by Berkeley
who challenged the coherence of the ideas of fluxion and infinitesimal quantities\cite{berkeley2018analyst} in 1734 but it was not for another century before mathematicians became seriously embarrassed:

\begin{quotation}
  \emph{There are are very few theorems in advanced analysis which have been demonstrated in a logically tenable manner. Everywhere one finds this miserable way of concluding from the special to the general, and it is extremely peculiar that such a procedure has lead to so few of the so-called paradoxes.}

Abel, 1826
\end{quotation}

There ensued the most fertile period of foundational innovation in the history of mathematics.
This was not a search for absolute certainty, but for conceptual clarity and deductive rigour.
It occurred in several stages, most of which were instigated by Mathematicians and represented real advances in mathematics.

The first stage in this process was to show that the central notions of the calculus could be made precise without resort to infinitesimals or fluxions.
This aspect of the work had already been undertaken by Cauchy who's 1821 book ``Cours d'analyse''\cite{bradley2010cauchy}, rigorously defined the concept of convergence for sequences and series using what are now called Cauchy sequences.
Cauchy used these sequences to provide a precise epsilon-delta definition of limit and continuity for real functions which provided the means to  define differentials and integrals as limits \cite{cates2019cauchy}, giving precision and clarity to procedures which had evolved less formally over thousands of years going back to the work of Archimedes.

Having made these important advances in the rigour of analysis, there remained significant areas of uncertainty.
There was still no clear conception of the number system on which this work depended, though the position had improved with the elimination of infinitesimals from the theory.
The development of analysis had made it necessary for mathematical functions to be regarded as abstract entities rather than syntactic expressions, but the coherence of that position depended on clarification of the exactly what these abstract functions are.

From a modern perspective the clarification of these two points rests on the theory of sets which was begun by Cantor, but the reduction to set theory was a later development.

\section{Some Preconditions of Deductive Rigour}

The centrepiece of this synthesis is a formal logical system, consisting of a declarative language with an abstract denotational semantics and effectively decidable deductive rules.

The presentation here is informal, but describes formal treatments many of which are already in place in the literature, and all of which would generally be accepted as entirely feasible.
This preliminary discussion concerns the key requirements that will have to be met, with a first indication of how those requirements will be met.

Deduction can only take place in a declarative language which has a definite truth conditional semantics.
It is possible to formally reason in languages which lack such a semantics, and there are technical conceptions of consistency (e.g. consistency in the sense of Post) which give minimal criteria of correctness for such a deductive system, but they nevertheless fail to qualify as deductive if it cannot be shown to preserve truth.

\chapter{Logical Truth}

In due course I will address some of the sceptical challenges to the idea of first philosophy, and to many aspects of the synthesis which is proposed, but at this stage I am concerned only to precisely describe the first stages in the synthesis by offering some informal definitions.

The single most important concept around which all else is constructed is that of \emph{logical truth}.
Many will be inclined to contest my use of that term for the concept which I will now describe.
It is the concept which is important, not the name I use for it.

Ultimately, in the synthesis which I provide, there is a place for addressing all the forms which knowledge may take, but insofar as epistemology is concerned with subject matters belonging to the natural sciences, it will receive no special attention beyond the considerations later advanced for the empirical sciences.

Placing the epistemology of logical truths (logically) prior to that of empirical science reflects the important role which mathematics plays in the natural sciences, which we consider in its broadest sense as the interpretation of science as constructing abstract models of physical systems and phenomena.

The concept of logical truth will be defined in the following stages.
\begin{itemize}
  \item
    In the first instance it is defined as a characteristic of sentences in languages of a particular kind (declarative languages with fixed vocabulary and a well defined truth conditional semantics).
  \item
Then the definition will be augmented to admit languages with an extendable vocabulary, whose truth conditional semantics is appropriately augmented by the process of extension.
In these latter we may say that the truth conditions for sentences in the language are sensitive to a context which reflects the extensions to the vocabulary and the constraints incorporated by the extensions.
\item
At this point we may speak of relationships between these 
\item

\end{itemize}


\section{Ontology}

I divide those things which may exist into three kinds as follows:

\begin{itemize}

\item concrete objects

  Those things which have spatio-temporal location and may be causally related to other concrete objects.

\item Purely abstract objects

  These are entities which do not have spatio-temporal locations and are not causally related to other objects.

\item Hybrid objects

  These are complex entities which are built from or built into both concrete and abstract objects, for example, a set of concrete objects will be such a hybrid.
  Thus, for example, if in the Simple Theory of Types, the individuals are held to be concrete objects, and the propositions are abstract, all other types will consist of hybrid entities.
  If on the other hand, the individuals are purely abstract, then the entire ontology will be purely abstract.
\end{itemize}

For the purposes of defining the notion of ``logical truth'' abstract ontology will suffice.

Some philosophers will think it reasonable to ask whether any abstract objects exist, and how we can possibly establish that they do.
By engaging in ``first philosophy'' I arrogate to myself the right to articulate without restraint whatever concepts will provide the basis for the epistemological synthesis which will follow.
This includes the concept of existence.
It is open to us to decide what that concept means and how it will be used in our system.

The position I adopt on the matter of existence is sensitive to the kind of entity concerned, and I need only at this point speak to the purely abstract entities at stake, in relation to which I proposed a wholly conventional stance.
This may be considered analogous to a fictionalist stance, differing from it in crucial ways.

The fictionalist is going to tell a story which he acknowledges is not factual, but which he considers instructive or entertaining in some other way.
The statements he makes in his fictional narrative will sometimes be false, and may not even be mutually consistent.

The conventionalist stance I take differs from fictionalism in these two principle respects:

\begin{itemize}
\item Within the scope of the relevant convention, the consequences of the convention are true, not false.
\item It is very important in the adoption of conventions, that the principles adopted be logically consistent, even when these conventions are adopted en route to the establishment of the concept of logical consistency.
\end{itemize}

In what follows hybrid objects will have no role, and it is very likely that I will tire of prefixing ``abstract'' with ``purely'', which should therefore be understood as implicit.

Normally in what follows, the adoption of some such ontological convention is a part of the definition of a formal language, and its scope is primarily in determing the meaning and truth value of expressions in that language.
More broadly in the less formal conduct of mathematics and perhaps other sciences, a convention may be simply contextual rather than being a part of the adoption of a language.
For example, there are multiple different formal axiomatisations of set theory, which disagree among themselves about the domain of which they speak, about which sets must, may or cannot exist.
The ``standard'' conception is of a domain exclusively of well-founded sets, but no axiomatic system guarantees that this is the case.
On the other hand, the axiom of separation, which guarantees the existence of all subsets of a set obtained selecting the members satisfying a given formula, makes it possible to prove that there is no universal set which contains all other sets.
Notwithstanding that difficulty with the universal set, many consistent set theories are available in which the universal set does exist and the principle of separation is not valid.

I may as well note in explaining my adoption of this conventionalist convention, that if I were to prove mistaken in disbelieving that there is any objective truth about what purely abstract entities exist (including the possibility that there are none), that the value that I associate with this position would be unabated.
Since abstract entities are causally unconnected with us, there existenve or otherwise in any absolute sense is immaterial to the utility which attaches to adopting the convention that some appropriate collection of abstract entities does exist.

Among the very many connections between the synthesis I present here and the philosophy of Rudolf Carnap, there is a direct connection here between my ontological stance and that which Rudolf Carnap attempted unsuccessfully to explain by distinguishing ``internal'' from ``external'' questions \cite{carnap50}.
``Internal'' questions are those posed in the context of some well defined language, and which are to be answered on the basis of the principles explicit or implicit in the definition of the language, ``exernal'' questions are those ontological questions which might arise in the process of defining the semantics of such a language, and were regarded by Carnap as meaningless pseudo questions.
Of course, we can ask what is the meta-language in question (which would normally be an idiom of a natural language such as English aumented by the vocabulary of some special discipline, perhaps mathematical or philosophical logic.
This is unlikely to provide any definitive ruling on the kinds of ontology which Carnap was considering,

The ontological conventionalism with respect to abstract entities which I am adopting here is not sensitive to the ontology of the metalanguage (which is indeed, in the first instance, English).
It is possible to describe an abstract domain whether or not the existence of that domain is demonstrable in that context, in just the same way that one can write a novel about an entirely fictional population.
If we then define the meaning of existence in the object language as membership in that possibly fictional domain, then ontological questions posed in that object language will be answered according to the structure of the chosen domain rather than by reference to any ontological principles which belong to the meta-language.

Though this discussion has addressed only abstract ontology, conventionalism in respect of concrete ontology is at least implicit in what will be said later about the representation of scientific and engineering knowledge.
It should also be noted that modern logic is generally agnostic about what things are, insofar as all properties and relations are external, and so long as those externalia are preserved, the substitution of a completely different collection of individuals in the domain of discourse has no effect on the truth values of any sentence.
This apparent indifference to identity is reinforced in the logical foundations which are proposed by ontological ambiguity in type constructors, the effect of which is that though various complex types of abstract entities are constructed in way similar to the constructions which would be used for mathematical objects in a pure set theory, there is no basis for identifying these newly constructed entities with the constructions themselves.
It is a foundational scheme in which numbers are not \emph{identifieed with} sets, or any other type of universal foundational ontology.

Though fully in the spirit of positivism, this epistemology is not nominalistic, a departure from the mainstream first appearing in Rudolf Carnap.

\section{Language}

It is not impossible to reason about the world without making use of language, but as soon as we talk about truth, language is presumed.
A generic definition of logical truth therefore depends upon an explicit and general account of some class of languages of which logical truth can meaningfully be predicated.

Though linguists and mathematical logicians may have conceptions of language which are purely syntactic (e.g. ``a set of sentences'') for the purpose of defining the concept of logical truth (as it is here construed), some semantics is essential.
The kind of semantics which is necessary is \emph{truth conditions}.

The essence of declarative language is the use of sentences to make claims which discriminate between various possibilities which constitute the subject matter of the language.
For the barest account of logical truth, we therefore require of the truth conditional semantics:

\begin{enumerate}

\item that a range of ``possibilities'' be identified, which are those for which the truth conditions settle a truth value.

\item that for each sentence in the language, a subset of possibilities is given, which are the cases for which the conditions expressed by the sentence under the semantics hold, in which case the sentence is then deemed true.
\end{enumerate}
    
\section{Logical Truth as Tautology}

For the broad swathe of languages which satisfy the conditions given above, the logical truths are those sentences which are true in every possibility, hence necessary.

It may be noted that there is a special case arising when there is only one possibility.
This is exemplified by the language of first order arithmetic when given with the standard semantics which admits of just one possibility (up to isomorphism).
In that case, the idea that a sentence conveys information when asserted by eliminating possibilities is not as instructive as it might otherwise be.
That we have a clear intuitive understanding of the structure of the natural numbers clearly does not entail that we know what arbitrary sentences of arithmetic are true, and it is therefore informative to know that a sentence is true even if it has not ruled out any possible structure of the natural numbers.

The sceptically minded will further note that this conception of logical truth is just the identification of logical truth with analyticity, and that the concept is thus rendered wholly conventional and lacks the absolute character which some would wish to see in the notion of logical truth.
These consequence arise however, not from any defect in the notion of logical truth thus articulated, but rather from the fact that the syntax and semantics of languages are completely conventional, and hence that, however absolute a conception of logical truth you might have, the question of which sentences express logical truths must be determined by those conventions (though I do not deny that the relevant ``conventions'' may in the case of natural languages have been fixed, if indeed they are fixed, by biological and cultural evolution rather than by the whim of language designers).

\section{Defining Foundational Universalism}

A ``Universal Foundation'' for logical truth would be a language to which logical truth in any other language is reducible.
The idea is therefore dependent upon what concept of reducibility is intended.

My motivation in addressing this topic comes from the desire to support a linguistic pluralism which includes the ability to reason about systems whose subsystems or components may be given in distinct languages.
In that case one needs a logical environment in which claims about subsystems in different languages can all be comprehended coherently.

A natural way to consider this kind of universality is by translation from each language into the universal language  which reduces truth in the source language to that in the target language.
The weakest constraint one could put on such a translation would be that it is an ``effective'' mapping which preserves logical truth.
This introduces a constraint on languages which it was not necessary to include in the definition of logical truth above, that the syntax, i.e. the set of sentences, be countable.
The kinds of reduction which arise in this way are those which are studied in \emph{recursion theory} and the the structure created by these reductions are various hierarchies of degrees of recursive unsolvability \cite{rogers1967theory}.

Regrettably, this extensive;y researched branch of mathematical logic will not give a convincing account of how logical truth can be encompassed in one universal family of languages unless we were prepared to regard all logically true sentences as expressing the same logical truth.
In that case we should be prepared to find, when attempting to prove a logical truth which had some material bearing on the problem we are addressing, that we were actually reasoning about some other proposition which happened to have the same logical status.

To progress this agenda we need much stronger constraints on reductions so that they can reasonably (and practically) be understood to preserve the meaning of the original.
For this purpose we need a more refined semantics than sentential truth conditions.
To accomplish this we need translations which not only preserve truth value, but preserve subject matter.
To that end we need more detailed structure in the kind of semantics which is to preserved by the reductions.

Not only should the mapping preserve the truth values of sentences, but it should preserve the values of the non-sentential expressions  which are the constituents of the sentences in the language.
We therefore appeal to the idea that the semantics should be presented as a homomorphism defined over the abstract syntax considered as a many sorted algebra, in which the sorts correspond to the various syntactic categories or phrase types in the phrase structure grammar of the language.

This more elaborate conception of semantics defines a new class of languages relative to which a claim to universality of some family of languages will be judged.

The notion of ``family of languages'' is quite restrictive, it is not some arbitrary collection of languages, but a series of very closely related languages, which have the same syntax but a progressively refined semantics.

\section{Two Universal Foundations}

Two putative universal foundations serve the purpose providing intelligible foundational underpinning for practical foundation system.
There is no suggestion that either of these is unique, but the two choices are made to be appropriate in distinct ways.

The first is chosen for the kinds of simplicity which contribute to underpinning the semantics and the proof rules through the deepest penetration of formality supported by intelligible but less formal semantic refinement.
The second is chosen for the convenience with which it can provide an underpinning for the variety of formal notations which are used in practice in mathematics, science and a full variety of practical applications in all domains.

The first language is the language of first order set theory, the second is the variant of Church's \emph{Simple Theory of Types} used by the Cambridge HOL system.
In both cases the families are generated by progressive refinements of the semantics which eliminate smaller interpretations.
In the proof theory these are effected by large cardinal axioms.
Given the standard semantics which consists primarily in demanding full power sets (in the set theory) or full function spaces (in the type theory) these axioms suffice to refine the semantics appropriately.

\chapter{Digging Deeper}

In this chapter I consider the credentials of the simpler of the two foundational systems, with the intention to examine how well defined is its denotational semantics and proof rules are, how solid are our reasons for believing that the proof rules are sound with respect to the semantics (that it proves only logical truths), and in what ways possible ambiguities might be reduced and certainty of soundness improved.

First order set theory is a first order language, a concept which has been central to much of the work in Mathematical Logic in the last century.
This means that the semantics and proof theory is well understood and many result have been established to the highest standards of rigour found generally in modern mathematics.
For the purpose of this synthesis there is very little added to the standard account.

The theoretical study of logical systems often treats semantics in a strictly analytic rather than a prescriptive manner.
By this I mean, that a formal language and its deductive system is addressed by determining the full range of \emph{models}, where a model is any interpretation of the language for which the logical system is sound, rather than expecting that the definition of the language should stipulate the intended subject matter (possibly by describing one or more intended interpretations).

If the semantics is analytic rather than prescriptive, then it renders any consistent logical system sound and complete.
The point of semantics in this case may well be to establish consistency, which is achieved by furnishing any model.

If the semantics is prescriptive then it is not only possible, but likely, that the deductive system will not be complete, since all but the simplest mathematical subject matter do not have decidable truth conditions, and cannot therefore have a complete axiomatisation in relation to which proofhood is decidable.

It is not usual to talk of first order languages in terms of denotational semantics, the study of semantics in this case is done using the language of \emph{model theory}.
Languages are distinguished by their vocabulary, and meaning is assigned to the names in the vocabulary by means of a structure called an interpretation, which consists of a domain of discourse (a set of values which constitute the range of the first order quantifiers) and an assignment to each name of a value defined over that domain of discourse.
The names in question are called constants, and represent either predicates which may take as values any subset of the domain of discourse which is the set of elements which are deemed to satisfy the predicate, n-ary relation symbols which denote sets of tuples of values from the domain of discourse, or functions which represent mappings from n-tuples to values drawn from the domain.

The standard account of the semantics of a first order language determines how the value of an expression in the language can be obtained for each interpretation of the non-logical constants in the language.
Since the expression may contain free variables, it is necessary to have a value for these variables before a value for the expression can be obtained.

A denotational semantics can be determined for a first order language by stipulating a particular set of interpretations as intended interpretations.

In first order logic with equality, the relation of equality is fixed and represents equality of pairs in the domain of discourse, and therefore does not appear in the structure defining an interpretation.
In first order set theory there is just one other relation symbol, which is the binary relation of membership.
The 



\chapter{Connecting the Families}

In this chapter the equivalence of the two families is argued.
In the first case this is done by reference to the various treatments of the semantics of CHOL in FOST.
This is supplemented by a discussion of the interpretation of OFST in CHOL using a strong axiom of infinity which in effect asserts that the cardinality of the 'individuals' is inaccessible.

Both families consist of a linearly ordered set of languages indexed by cardinal numbers which represent the minimum cardinality of the ontology.
For lower cardinalities, CHOL and FOST may not be equivalent (this depends on the detail of how these ideas are defined), but once we reach a sufficiently large cardinality they will be equivalent (i.e. bi-interpretable in the relevant sense).

For practical purposes, i.e. for the purposes of science (excepting some parts of set theory), engineering and commerce, large cardinals are not needed, the relevant mathematical structures being modest in size, and the heirarchy of languages becomes irrelevant.
This is evidenced in practice by the sufficiency of CHOL with the standard axiom of infinity (ensuring only a countable infinity of individuals) for engineering purposes.

\chapter{Building Higher}

In this chapter the way in which CHOL provides universal support for the class of languages with a well defined denotational semantics is discussed.

The idea here is that no injection between the language for the application and the CHOL language is necessary, because the use of the standard mechanisms for conservative enables the denotational semantics to be renedered in CHOL in a manner which results in the abstract syntax for CHOL containing the abstract syntax of the application language, and the semantics of CHOL with those extensions which define the denotational semantics of the target language will correctly render the meaning of expressions in that abstract syntax.

\chapter{Beyond the Limits}

The universalist conjectures which I have presented have their limits and in this chapter I mention some languages and deductive systems which either are technically not within the scope of these universal foundational families or which are based on philosophical ideas which are incompatible with our perspective.

When adequate formal systems for mathematics were first devised the philosophical attitudes toward these and other foundational matters fell into three main camps known as Logicism, Formalism and Intuitionism.
The hard core logicists were Gottlob Frege and Bertrand Russell, both of whom were also \emph{universalists}, holding that there should be just one universal conception of logic which is sufficiently rich to define all the concepts of mathematics and to formally derive mathematical theorems.
It is with Russell that type theories like Cambridge HOL originate, though Frege's logical system is generally regarded as constituting a second order logic, which prefigures the more elaborate type system contructed by Russell (and later simplified en-route to HOL).
The formalists lead by David Hilbert were linguistically pluralistic.

The story I have given above attempts a reconciliation of these two approaches to foundations, following the precedent of Rudolf Carnap in whose spirit this synthesis has been constructed.
It represents a compromise, proving a reconciliation of the pluralistic and universalistic foundational perspectives.

Intuitionism is more difficult to reconcile, because of its outright rejection of much of the mathematics of its time, most particularly of Cantorian set theory.
If intuitionism were merely a weaker logical system adequate only for a part of mathematics it might still reasonably be supported in the more full blooded system which I have proposed, but it is unlikely that its advocates would be content with any such accomodation.
Philosophical reservations (to say the least) would remain.

One reason for the philosophical incompatibility of intuitionism is its rejection of formal systems of any kind, which however was not replicated across the many logical systems which shared some of its concerns.
When formalisation was admitted (as later became normal), semantics remained prohibited, for truth was then, in many systems, identified with the existence of a constructive proof.
So the idea of rooting logical foundations in the concept of logical truth defined in terms of semantics remains unacceptable.

A technical reduction might nevertheless be effected through the identification of truth and proof, but its not clear that repairs the philosophical breach.

Intuitionism, spawned a broad range of logical systems which radically departed from the classical models, often with proponents without a uniform philosophical perspective, but with technical reasons for preferring the novel logical systems which emerged.
An important innovation in these constructive logical systems appeared in the work of Martin L\"of in the shape of a dependent type theory inspired by a correspondence between the types of terms in a type theory and the structure of propositions.

\chapter{Empirical Knowledge}

Concerning the representation and application of empirical knowledge.

\chapter{The limits of Language}

Every bit of information stored in an information processing system is represented by a material structure which involves very many submicroscopic parts, the nature of which is the subject of complex ongoing research in fundamental physics.
It is therefore inconceivable that we could ever have a complete description of the material universe.

It is not impossible that there might be a finite description of what kind of state the universe might be in, and of the ways in which the state of the universe progresses from one moment to the next, but contemporary physics must surely cast doubt upon that possibility too, notwithstanding the insistence of some physicists that a ``theory of everything'' is a real possibility.

This suggests that our knowledge of the physical universe of which we are a part, or even of any part of it, must be incomplete.

It is nevertheless possible to construct theories about, or models of the universe which prove of considerable value in progressing the prosperity and well-being of ourselves and those we care about.

Isaiah Berlin has characterised ``Enlightenment Thought'' as involving the belief that all truths are logically compatible, but the reality is that we routinely make us of ideas about the way the world works which are logically incompatible.
A clear example of that is the continued use of Euclidean space time and Newtonian physics after the discovery or invention of Einstein's theories of relativity.

%\appendix

\chapter{Varieties of Epistemology}


My purpose here is not to survey the many varieties of epistemology which philosophers and others have engaged in, but to describe the two particular variants which I address in this monograph.

The work falls into two parts which I describe as \emph{natural} epistemology and \emph{synthetic} epistemology, terms I have adopted for the purpose of this exposition rather than reflecting any established nomenclature.
Of those two, the latter provides the substance of the work, the former plays a supporting role, intended to give background essential to an understanding of the synthesis which follows.

The main purposes of the preliminary natural epistemology are:
\begin{itemize}
\item to present the following synthesis as a natural future development in a long standing evolutionary history
\item to identify those elements in that evolutionary progression which are most important for an understanding of the synthesis
\end{itemize}

In taking knowledge and epistemology as having evolved over the entire history of life of earth and as now entering into a period of more rapid and radical change than ever before, I am stretching the terms beyond their previous application.
One aspect of this is the abandonment of the predominant anthropocentrism of traditional epistemology.
Contemporary epistemology has moved away from anthropocentrism by considering knowledge in animals, artificial intelligence, and the implications of posthumanism.

In my first part, the evolution of knowledge is thought of as beginning with the prebiotic evolution which culminates in abiogenesis, and as progressing throughout the evolutionary history of homo sapiens ultimately morphing into the cultural evolution which in modern times lead to information technology, the prospect of artificial intelligence, and the progression of biological evolution to intelligent design and selection.

\subsection{Knowledge and Mind}

Epistemology has traditionally been anthropocentric, regarding knowledge as as essentially human enterprise.
This perspective on knowledge is exemplified by its conception as justified true belief, and hence as in part a kind of psychological or mental phenomenon.

\section{Bard}

Contemporary epistemology often emphasizes human cognition and understanding. However, it also ventures into broader realms that explore knowledge beyond human boundaries. While historically centered on human cognition, contemporary epistemology is increasingly acknowledging non-human forms of cognition and knowledge representation.

Anthropocentrism in epistemology arises from its traditional focus on human perception, justification, and belief formation. But contemporary thought expands beyond this by considering:

1. **Animal Cognition:** Studies in animal cognition challenge the human-centric view by exploring how animals acquire, process, and apply knowledge. Research indicates sophisticated cognitive abilities in various species, altering traditional conceptions of knowledge acquisition.

2. **Artificial Intelligence:** Discussions on AI and knowledge representation highlight the development of systems that can store, process, and utilize information in ways akin to human cognition. This challenges the exclusivity of human knowledge processes.

3. **Extended Cognition:** The concept of extended cognition posits that cognition isn't confined to the brain but can extend to objects, tools, or technologies. This extension broadens the scope of what constitutes knowledge and cognition.

4. **Posthumanism:** Some branches of thought within epistemology, influenced by posthumanism, explore the possibility of knowledge existing and evolving beyond human existence. This perspective raises questions about the nature of intelligence and knowledge in synthetic or non-human forms.

While the historical foundation of epistemology might be anthropocentric, contemporary discourse increasingly acknowledges and explores knowledge frameworks that extend beyond human cognition. References to contemporary philosophical works like "Posthuman Knowledge" by Rosi Braidotti or "Animal Minds" by Donald R. Griffin can offer detailed insights into these evolving perspectives within epistemology.

Extended cognition challenges the traditional view that cognition is solely a process occurring within the boundaries of an individual's skull. It posits that cognitive processes can extend beyond the brain to include external objects, tools, or even other individuals. This concept was notably developed by philosophers Andy Clark and David Chalmers in the late 1990s.

Key aspects of extended cognition:

1. **Cognitive Extension:** External objects or tools are integrated into the cognitive process, effectively becoming a part of the thinking system. For instance, a notebook or a smartphone can serve as external memory storage, aiding in information retention and retrieval.

2. **Functional Integration:** The external objects or tools are not just accessories but are functionally integrated into the cognitive process. They are considered as parts of the thinking system and contribute significantly to problem-solving or decision-making.

3. **Cognitive Offloading:** Individuals use external resources to offload cognitive tasks. This offloading can range from simple memory aids (like notes or calendars) to complex computational tools that augment problem-solving abilities.

4. **Mutual Dependence:** The relationship between the mind and external tools is reciprocal. Just as the tools extend cognitive abilities, the cognitive processes of the individual shape how these tools are used or adapted.

Examples of extended cognition can be seen in various daily activities: using a calculator to perform complex mathematical calculations, relying on a GPS for navigation, or even utilizing language as an external system for thought organization.

This concept challenges the traditional internalist view of cognition, emphasizing that cognitive processes aren't confined to the brain but can incorporate external elements. References like Andy Clark's "Supersizing the Mind: Embodiment, Action, and Cognitive Extension" or David Chalmers' "The Extended Mind" delve into this concept, offering detailed arguments and case studies supporting the idea of extended cognition.

The concept of extended cognition, as traditionally understood, often focuses on human interactions with the environment rather than exploring non-human forms of cognition. It primarily addresses how humans use external tools to augment their cognitive processes.

However, recent philosophical discussions have expanded the scope of extended cognition to encompass a broader understanding beyond human-centric perspectives. 

One direction involves considering how non-human entities or systems, such as certain animals or even artificial intelligence, might engage in forms of extended cognition. For example:

1. **Animal Cognition:** Some studies suggest that certain animals use tools and incorporate external elements into their cognitive processes. This challenges the strict anthropocentric view by acknowledging cognitive extensions in other species.

2. **Artificial Intelligence:** Discussions on AI often touch on extended cognitive processes, exploring how these systems interact with their environment and utilize external databases or networks to enhance problem-solving.

Expanding the concept beyond human-centricity involves recognizing cognitive processes in various forms of life or intelligence, not solely limited to human beings. This extension challenges the exclusive focus on human cognition and opens avenues for exploring cognition across different life forms and artificial systems.

References such as Peter Godfrey-Smith's "Other Minds: The Octopus, the Sea, and the Deep Origins of Consciousness" or work by Philipp Kellmeyer on AI and extended cognition could offer insights into these broader perspectives on extended cognition beyond human-centric views.


\chapter{Varieties of Epistemology}


The purpose of this work is to construct, present and advocate for a \emph{synthetic epistemology}.
Notwithstanding such prior usage of that concept which there may have been, what I intend in using the term is that the ``epistemology'' which I present has been devised as suitable for future application in a world in which the corpus of systematic knowledge which includes both scientific knowledge of the principles which govern the world (including the logic and mathematics which enables those principles to be articulated) and also an ever increasing body of detail about the the world governed by those principles, is maintained and progressed in a collaboration between human beings and intelligent artefacts, the latter facilitating precise language and formal reasoning which might otherwise be too onerous for mere humans.

Though that central purpose is synthetic, the synthesis is not a fantasy constructed \emph{in vacuo}, but a stage in a historical process which has been underway for billions of years.
Some examination of that historical progression is therefore attempted for two reasons, and this I present as ``natural epistemology'', which you can imagine, may be something like but not entirely the same as ``epistemology naturalised''.

\subsection{}

The purpose of this work is to present some ideas for the representation of all those kinds of knowledge which are amenable to deductive reason.
Let us say \emph{declarative knowledge}, that kind of knowledge which is found in true declarative sentences.
My aim is to articulate certain ideas about how large bodies of knowledge can be organised in such a way as to maximise our confidence in the truth of the propositions and to facilitate deductive reasoning for the purposes of elaborating and applying the knowledge.

These ideas are \emph{foundational} in more than one respect, the most fundamental of those being very closely related to some of the important work done in relation to the logical foundations of mathematics over the last 150 years.
The focus on foundations is complemented by attention to applications.
A single logical system is identified as the pivot through which the foundational underpinnings support the full range of applications in almost every aspect of our lives.

That pivotal system is the logical system which has been adopted by Cambridge HOL and some other interactive theorem provers.
That system is itself a logical foundation system for mathematics with well established credentials, within which almost all known matheamatics can be derived by machine supported and checked deductive inference from formal definitions of the relevant mathematical concepts.

Most foundational research has historically been conducted by scientists (and sometimes engineers) seeking to improve the clarity and rigour of their work in the face of difficulties arising from perceived defects in those areas.
  It is a process of working backwards from the applications in need of support toward some more solid and stable basis on which they can be constructed, and there is usually a good practical sense of when enough has been done, though sometimes foundational advances expose deeper difficulties which are then in need of attention.
  This is the kind of process which was seen in mathematics througout the 19th Century and eventually provoked a crisis at the turn of the 20th as a result of Russell exhibiting a paradox which vitiated Frege's foundational enterprise.

  A quite different foundational enterprise occurs in philosophy when philosophers look to foundations for a response to radical scepticism.  The philosophy in this context may not be well connected with the practice of science, and the expectations of foundations may be less easily satisfied.

To be certain of the truth of a sentence, we must first establish a definite meaning for it.
The meaning of a sentence depends upon the language in which it is expressed, and on the semantics of that language.

The distinction between true belief and knowledge plays very little role in this project, despite a deep concern with the levels of assurance with which truth can be established.

**************************


I am primarily concerned with making meaning precise and enabling truth to be establish with the highest level of assurance.

Insofar as this discussion can be construed as 
The conception of epistemology as concerned specifically with the theory of \emph{knowledge} is therefore 

Either:
\begin{itemize}
  \item
Before going further I should confess that, though I will often talk about knowledge, I regard that term in much the same way that an Engineer might think of the concepts `hot' and `cold', as poor substitutes for a temperature.
In the 'epistemology' here proposed, the semantics of declarative sentences is factored into two parts.
The first of these is an abstract semantics interpreting the sentence in relation to some abstract model, and the second an indication of the concrete structure which that abstract model is intended to replicate.
Knowledge in this system consists of formal logical truths about the abstract model (usually established by machine checked formal deductive proof) and more elaborate and mixed information about how that abstract model is applicable to the concrete world, rooted in a bijection between those elements of the model which are intended to represent physical entities with the entities they represent.
\item
By contrast with much epistemology, the focus is not on the word \emph{knowledge}, which plays only a minor expository role.
Instead the emphasis is on \emph{truth} and the confidence with which truth can be established (primarily in relation to \emph{a priori} knowledge), and on utility or applicability in relation to theories about, or models of, parts or aspects of the material universe.
It is thus oriented toward scientific knowledge represented in abstract structures, rather than addressing the kinds of everyday knowledge which people acquire and exploit in their everyday lives.
\end{itemize}
- or some amalgam!


********************************

Notwithstanding this emphasis on science and engineering, the synthesis is applicable to any domain sufficiently definite that deductive reason is in principle possible, and it is expected that any practical difficulties in its use in domains less comfortable with formal notations will be mitigated by the intermediation of Large Language Models or Artificial General Intelligence.

An epistemological synthesis must begin somewhere.
A major consideration for this synthesis is the establishment of clear meanings, a necessary condition for truth to be definitively established, and the provision of contexts in which deductive reasoning can be rigorous and substantially automated.
In such matters sceptical arguments based on regress both of definitions (of concepts and languages) and of justification demand an answer for which we offer a foundational perspective.
For this reason there is a logical order in which the ideas are best presented, withstanding the inevitable circularity incurred in the most fundamental parts (for the risks in which, mitigations are offered).

There are two orderings of interest here.
There is the ordering of presentation so as to best communicate to readers the ideas.
There is also, in a foundational presentation, a logical ordering of the substance.
This is the kind of order which is required in definitions to avoid their vitiation by circularity.
As well as the ordering of definitions, we will be concerned with an ordering of languages, in which more complex languages are defined in or by reference to simpler languages.

The avoidance of circularity is desirable, since it contributes to ensuring that meaning is clear and well defined, the maintenance of coherence and the avoidance of contradictions.

\part{Natural Epistemology}

\chapter{Prebiotic Epistemology}

\chapter{Biological Knowledge and Knowing}

\chapter{Culture and the Advent of Epistemological Innovation}

\subsection{Some History of the Vision}

The ideas addressed by this epistemological synthesis seek to progress conceptions of knowledge which have developed over the last two and a half millenia, beginning perhaps with Aristotle's conception of ``demonstrative science'', a conception of empirical science modelled on the axiomatic method which had been successful in mathematics, particularly in what we now know as Euclidean Geometry.

There follows a brief sketch of some of the key development leading up to the present synthesis to provide a backdrop against which the account of purpose and the key requirements which flow from that purpose may be more clearly understood.

\subsection{Gottfried Wilhelm Leibniz}

Leibniz (1646-1716) was a polymath of the early Enlightenment, innovating in Philosophy, Logic, Mathematics, Science and Engineering. 

Leibniz conceived of and attempted to design a \emph{lingua characteristica} (a language in which all knowledge could be formally expressed) and a \emph{calculus ratiocinator} (calculus of reasoning) such that when philosophers disagreed over some problem they could say 'calculemus' (let us calculate) and agree to formulate the problem in the lingua characteristica and solve it using the calculus ratiocinator.

\paragraph{The Lingua Characteristica}
Leibniz engaged in four kinds work related to his proposed lingua characteristica, (or universal characteristic).
\begin{itemize}
\item Encyclopaedia - he sought the collaborative development of an encyclopaedia in which would be presented in non-symbolic form all that was so far known. This was to provide a basis for the lingua characteristica in which the knowledge could be formally expressed. This enterprise was not completed, but beneficial side effects were the foundation of new academies and of the journal Acta Eruditorum.
\item Universal Language - he promoted the development of a language universal in the sense of being spoken by all. There have been many such projects of which the best known today is Esperanto
\item The lingua characteristica - a formal language universal both in being understood by all, and in encompassing all knowledge.
\item The calculus ratiocinator - a method of computing the truth value of a proposition in the lingua characteristica
\end{itemize}

  The lingua characteristica was to be a language in which predicates were numerically encoded in such a way as to render the truth of subject predicate proposition (and Leibniz considered all propositions to have this form) could be obtained by arithmetical computation.

\paragraph{The Calculus Ratiocinator}
This is roughly how he proposed to do it. He believed that every predicate was either simple or complex and that complex predicates could be analysed into simple constituents. He proposed to assign prime numbers to each simple predicate and then represent a complex predicate by the two products, one of the primes representing its simple constituents and another similarly representing the simple constituents which occurred negated.
Complex predicates are therefore thought to be invariably conjunctions of finite numbers of simple predicates or their negations.
He also believed (following Aristotle) that every proposition had subject-predicate form, and that in a true proposition the predicate was contained in the subject, i.e. the set of simple predicates from which the predicate was composed was a subset of the set from which the subject was composed. This can be sorted out by numerical computation, you just check whether the first part of the predicate divides that of the subject without remainder, and that the second part (the negated constituents) of the predicate divides the second part of the subject.

His main difficulty in this was in discovering what the simple predicates are. Leibniz thought the complete analysis beyond mere mortals, but believed that a sufficient analysis (into predicates which are relatively primitive) for the purposes of the calculus ratiocinator would be realisable.

We now know much more about logic and its scope, and can see that this scheme could not work except perhaps for very simple fragments of scientific language.
This is primarily down to the limitations of Aristotle's Syllogistic logic within which Leibniz was working, but also because of limitations on what can be done even with modern logic requiring some qualification to any modern reconception of that vision, such as that in which I am here engaged.

Leibniz has nevertheless been an inspiration to many other philosophers, logicians and computer scientists and engineers who have progressed aspects of his ambition in the centuries which followed.

For my present enterprise the aspiration to completely general way of representing knowledge amenable to deductive reason and suitable for gathering together a comprehensive body of precise knowledge in a manner conducive to mechanised deductive reason is a primary aim of the proposed synthesis.

In order to tease out in simpler terms some aspects of the big picture here, its interesting to see the perspective of a modern philosophical historian of ideas on some features of Western thought up to and in the Enlightenment.

\subsection{Isaiah Berlin}

Isaiah Berlin's take on The Enlightenment\cite{berlinRR} comes in two parts.
First, ``three legs upon which the whole Western tradition rested'':
\begin{enumerate}
  \item All genuine questions can be answered.

    In principle, by someone.  Perhaps only God.
\item  The answers are knowable.
\item All the answers are compatible (with each other).
  It is a logical truth, Berlin says, that one true proposition cannot contradict another.
\end{enumerate}

and then, the extra twist added by the Enlightenment:
\begin{quotation}
That the knowledge is not to be obtained by revelation, tradition, dogma, introspection..., only by the correct use of reason, deductive or inductive as appropriate to the subject matter.

This extends not only to the mathematical and natural sciences, but to all other matters including ethics, aesthetics and politics.
\end{quotation}
and... that virtue is knowledge.

This is a simple description of an unattainable ideal, elements of which are important to this synthesis.
It will not be expected that all questions have an answer, for it is convenient sometimes to work with entities for which we have only incomplete descriptions, but it is an aspiration that any question definite enough to be amenable to deductive reasoning, either in its establishment or its application, can be accommodated within the synthesis.

There are now strong reasons to doubt that the answer to any properly formulated question can be discovered and established.
In many aspects of the proposed synthesis, absolutes are known to be unrealisable, and it is more important to be confident in the answers which do come than for such answers to be always forthcoming.

That all the answers be compatible is possibly the most crucial requirement in a system intended for large scale deductive elaboration, for in default of coherence, no result can be trusted.

\subsection{Hume and Kant}



\subsection{Frege}

\cite{frege1980}


\chapter{The Mechanisation of Logic}

\section{Meeting Cambridge HOL}

During the 1980s Mike Gordon and others at Cambridge University began research into the formal modelling and verification of digital hardware using software supporting the interactive development and checking of formal logical proofs.
This arose from prior work at Stanford and Edinburgh Universities on reasoning about computer software using a logic called ``Logic for Computable Functions'' (LCF) devised by the logician Dana Scott \cite{scott1993type}.
For the purpose of reasoning about hardware it was decided to adopt a Higher Order Logic instead of LCF.
After a number of adaptations, a logical system and a culture stabilised, based on a derivative of Church's ``Simple Theory of Types''\cite{churchSTT}.

This logical system is sufficiently powerful for the formal development of applicable mathematics, in a similar manner to that advocated by Frege and exemplified by Russell and Whitehead in their influential \emph{Principia Mathematica}\cite{russell10}.

The Fregean prescription (expressing his logicist thesis, contra Kant, that \emph{mathematics is logic}) was:

\begin{center}
  Mathematics = Logic + Definitions
\end{center}

To formally derive mathematics it suffices to begin with a formalisation of logic, add the definitions of the concepts of mathematics, and then derive in that logical system the theorems of mathematics.

Though Frege's prescription was specifically for mathematics, he considered his logical system to be of more general application.
Inspired by the ideas of Frege and Russell, Rudolf Carnap devoted his academic life to philosophically facilitating similar methods throughout science.
This was before the invention of the digital stored program computer, and the size and complexity of the formal proofs required would have been sufficient to make those ideas impractical for scientists.
The ideas, an important thread of the anti-metaphysical \emph{logical positivism} of the Vienna Circle, were even less palatable to philosophers.
In the hands of W.V.Quine a sceptical attack on Carnap's conception of logical truth, delivered in his ``Two Dogmas of Empiricism''\cite{quine53} (among other criticisms), served to de-throne positivism from the leading role in analytic philosophy which it might otherwise have occupied.

Despite the vicissitudes of Carnap's programme, when the Hardware Verification Group at Cambridge, with the advantage of computational support, looked to reason reliably about digital hardware, they advanced the programme pioneered by Frege, Russell, Carnap, Church and others.

Among those it is perhaps only Carnap who had attempted to address the problem of applying these logical methods in the empirical sciences.
He understood that Frege's prescription for the logical derivation of mathematics would not suffice for the empirical sciences.
Frege's prescription was specific to the \emph{a priori} sciences whose conclusions could be expected to be logically necessary.
These were understood to include mathematics, but not the empirical (\emph{a posteriori}) sciences.
For this reason Carnap had moved from the ``universalist'' stance of Frege and Russell, which had sought a single logical system for all deductive reasoning, to a linguistically (and ontologically) pluralistic regime, in which each application domain (each science or discipline) had its own logical system.
In these logical system principles which were factual rather than logical were adopted and results could be derived which were true results in the relevant empirical domain, but not logical truths.

It is not probable that Mike Gordon had any knowledge of these aspects of Carnap's philosophy when he sought to apply formal reasoning to digital hardware, but when he did do so these apparent difficulties in applying purely logical formal systems to reason about physical systems did not impede the adoption of pragmatic and sound ways of formally reasoning about hardware.

Philosophical qualms about the possibility of logically reasoning about physical systems did not pass unremarked by the engineers who adopted the methods, not only in Cambridge but across the world, as the resulting tools were adopted more widely.
Doubts about the status of logical proofs concerning the properties of physical systems moved from the research labs into the courts when claims about a commercial microprocessor whose design had been partially verified by these methods were contested in court.
Avra Cohn, a member of the Cambridge Hardware Verification Group (which had been contracted to contribute to the supposedly mis-described microprocessor verification), wrote a paper clarifying the issues: ``The notion of proof in hardware verification''\cite{cohnPIHV}.

Its worth looking a bit closer into the practice in the HVG and its relation to the difficulties which Carnap perceived in applying the new logical methods to the empirical sciences.

The Fregean prescription for the logicisation of mathematics required that mathematical concepts be introduced exclusively by \emph{definition}, and Frege was particular about what constituted a definition.
Using arbitrary axioms to characterise mathematical concepts or structures was not acceptable, primarily because of the risk it posed of compromising the logical consistency of the resulting theory.

Definitions are safe in this way, for they simply name some entity which in the existing system demonstrably has the required properties, rather than baldly asserting properties which are not already known to be realisable.
Because of this characteristic this kind of extension is called \emph{conservative}.

Definition only suffices in a logical system which is sufficently rich.
This is because definitions serve only to name something which already exists rather than to introduce something which was not previously in the domain of discourse.

The LCF system, which had previously been used for reasoning about software and which was adapted at first by Gordon for use in hardware verification, was not rich enough to work by conservative extension.
New concepts had to be introduced axiomatically, with the attendent risk of compromising consistency, which would become more severe as more complex systems were addressed.
When the group moved to work with a higher order logic\footnote{A fuller account of this story is given by Gordon in ``From LCF to HOL''\cite{gordon2000lcf}.} they fell in line with the Frege/Russell universalist conception of logic, and fully accepted the discipline of working exclusively with definitional  extensions, ensuring that all the resulting theorems were logical truths in that broad sense (of ``logical truth'') which was needed for mathematics to be assimilated into logic.

Notwithstanding this apparent turn to universalism, it is hard for anyone immersed in computer science to imagine that any one language can be sufficient, and the desire to support diverse notations and languages soon made itself apparent.
The universalist fundamental paradigm in practice became the use of a single logical system in which all other notations and languages could be interpreted, or ``embedded'' in the parlance which emerged (though without any explicit doctrinal underpinning).
An early illustration of techniques for interpreting other logical system in HOL may be found in \cite{gordon1989mechanizing}.
An account of a more elaborate example is given in Arthan's ``Z in HOL in ProofPower''\cite{arthan2005}.

The possibility of using HOL to support reasoning in other languages can said to arise from two features of the language.
The first is that the \emph{abstract syntax} of HOL is universal for a large class of languages, and the second is that the HOL language is sufficiently expressive that an abstract semantics of these languages can be rendered in HOL.
The concepts of abstract syntax and semantics arise from abstracting away from certain details of the syntax and the semantics, and from the possibility that they can be rendered entirely within the abstract ontology which is available in a purely abstract logical foundation such as HOL.

In \emph{abstract syntax} it is details of presentation which are discarded in favour of a simple representation of the structure of an expression or sentence which reflects the ways in which meanings of the elements of a syntactic category are compounded from the meanings of its constituents.

In \emph{abstract semantics} it is the details of how the expressions denote things in the real world (if the language does indeed do that), on the basis that the semantics has been rendered in the first instance in terms of an abstract model, a model in which all the entities involved are purely abstract entities.

With one further caveat we may then talk about HOL providing a language which is universal in practically important ways.
That caveat comes from two important results from mathematical logic and philosophy.
The first is the demonstration by Godel\cite{godel31a} of the incompleteness of arithmetic, and the second is Tarski's result on the arithmetic undefinability of arthmetic truth\cite{tarski31}.
The implication of these results is that no single logical system can be universal in its semantic expressiveness or complete in its deductive system.

We nevertheless conjecture that the HOL language and its deductive system can be indefinitely extended, both in its semantics and in its deductive system, and that it is universal in the sense that any other language (in a large class which will be defined) can be in practice reduced to a sufficiently strong version of HOL, sufficient strength being obtainable by progressively stronger axioms of infinity without other modification to the system.
HOL is in this sense universal, and it is also, I suggest (but cannot prove) practically complete in the sense that the probability of there being any practically applicable result which is not a theorem at close to the base level of this heirarchy.

Though the claim to universality is clearly refutable unless made for the heirarchy, the informal notion of \emph{practical universality} can probably reasonably be claimed for any single member of the heirarchy above a certain level.
This can be related to the default axiomatisation of first order set theory, generally regarded as a practically adequate foundation for mathematics.
HOL with a strong axiom of infinity which states or entails the existence of inaccessible ordinals.

This claim to practical universality extends beyond the provision of an adequate semantic and proof theoretic foundation, it includes a claim that the resulting support for languages is efficient.

The ideas presented here can be seen as providing a unification of the universalist and the pluralist attitudes to logical foundations.
Though we assert the existence of a universal foundation, we do not claim that it is unique.
I accept that there are many alternatives, but note that if they do have the required characteristics, then they will be equivalent.
I also believe that this provides a natural basis for the support of plurality of languages.
In fleshing out this account, I will abstract from the concept of language, completely divorcing it from concrete syntax, with the result that it becomes natural to identify a \emph{language} with a logical context which determines a vocabulary and its meaning, retaining the deductive system which extends that of the HOL system with the formal constraints which define the meaning of the vocabulary.


\chapter{Attik5}

\section{First Philosophy}

It is not uncommon for philosophers to decide that they need to start again \emph{from scratch}, because what they see does not provide a good base on which to develop a philosophy.
The most famous example is Ren\"{e} Descartes, rebuilding from his \emph{cogito}; a completely different collaborative re-start was attempted by the Vienna Circle with their manfesto of Logical Positivism.

An aspiration to revolutionary pre-eminence is not a precondition for seeking a benefit from fundamental innovation.

``First philosophy'' is a term coined by Aristotle for the philosophy which he undertook in the work which became known as the ``Metaphysics'', so called by later editors of his work simply because the volume came after his book on physics.

The idea that some ideas logically preceded others, and provide a foundation on which our knowledge of all matters can be built is both popular and popularly deprecated among philosophers.
It is easy to deride, for it is clearly impossible to really start from scratch, one must have language in order to say anything at all.

But certain kinds knowledge precede language, in the evolution of life on earth, by billions of years.
What arguably comes only with language, is that particular kind of communicable knowledge which may be expressed in declarative or propositional language, and which is my primary concern in this work.

This work is intended to address some the philosophical preliminaries to another stage in the evolution of intelligence in which the particular and exclusive role of human beings in the gathering, dissemination and exploitation of propositional knowledge is increasingly undertaken by synthetic, non-biological cognitive agents.

It is intentionally an example of ``first philosophy'', offering a basis for the further development of theoretical, practical and applied philosophy, for empirical science, technology and engineering, and it is foundational in character.

To call this ``first philosophy'' and to assert the value of and propose a structure for the foundations of logical and empirical knowledge is not intended in an absolute way.
Part of the ethos promoted here is the rejection of various kinds of absolutism, for example the idea that to have knowledge one must be able to justify a claim with absolute certainty.
We counter radical scepticisms in a way similar to G.E. Moore's common sense refutation of scepticism, offering an engineers practical realism to counter academic philosophical scepticism.

\section{First Philosophy}

This epistmological synthesis belongs to the positivist tendency in philosophy.
It promotes rigour in philosophy, science and beyond through critique of what is and the construction application of best methods in the context of appropriately engineered foundations.

It is therefore presented as a new start, a first philosophy and is inherently foundational and reductionist in character.
Those terms should be understood in an extreme or absolute sense.
It is easy to refute and reject first philosophy, foundationalism and reductionism if first make straw men of them.

\subsection{Words on First Philosophy, Foundationalism and Reductionism}

In talking of ``first philosophy'' as a fresh start, we of course do not imagine we can do anything in vacuo, this fresh start is engineered from a contemporary technical and philosophical perspective.
It may be instructive to relate this specifically to the history of mathematics and its foundations.

From the beginning of mathematics as a theoretical discipline, the achievement of logical rigour in the development of mathematics, and thence of the stability and reliability of the growing body of mathematical knowledge, has depended upon the adoption of precise mathematical language as facilitated by what came to be known as the ``axiomatic method''.
The engine driving the development of mathematics in that framework is deductive reason, which in the right context is highly reliable, even before anyone could give a precise characterisation of what inferences are deductive.

At the same time as these methods delivered spectacular success in mathematics, reason was applied to metaphysics and cosmology and proved impotent.
Ambiguity of language and incoherence of context are lethal to effective deduction, and the conditions for eliminating these are difficult to realise in any but the narrow confines of mathematical theories.


\chapter{Attik4}

\section{Introduction}

Epistemology is that part of philosophy which is concerned with the theory of knowledge.
It is not the only philosophical discipline which has something to say about knowledge, the philosophy of science, mathematics, language and of logic all make a contribution to our understanding of what knowledge is and how it can be discovered, established, expressed and applied.

I will here treat epistemology broadly as encompassing the relevant parts of all these disciplines.
By speaking of a \emph{synthetic} philosophy my intention is not to address what knowledge is or has been, but to propose a conception of knowledge suitable for the future, and limited in certain important ways.

Knowledge is, so far as we know, a phenomenon arising from the evolution of life on earth, and comes in a variety of forms which have themselves evolved over more than 3 billion years.
There is a fascinating story to be told about how knowledge evolved over the 3 Billion years before the advent of propositional language.\footnote{One aspect of this is illuminated by recent research on the evolution of memory systems \cite{murray2017evolution}.}

The existence of anything which could be called a \emph{theory} of knowledge is very much more recent.
In the first instance one might expect that a theory of knowledge would therefore be the result of an empirical investigation into a natural phenomenon.
This does not seem to have been the case.

Early signs of epistemology are reasonably clear in the philosophy of ancient Greece.
The word \emph{epistemology} has its origin in the Greek word for knowledge, \emph{episteme}.
 It is at that time that we see entire philosophical systems which are build around essentially epistemological theses, notably Pyrrhonean and other radical scepticisms.
 On a more constructive note, the notion of scientific knowledge, and the more elaborate Aristotelian conception of demonstrative science, show that a theory of knowledge could become a prescription for how knowledge should be sought, verified and applied.

Epistemology 
 

 \subsection{The Trajectory of Formal Language}

 This synthesis may best be understood as a projection of a historical trend which has advanced over the last three millenia.

 It begins with the simplification of language which enabled the transformation of mathematics into a theoretical discipline.


\chapter{Attik3}

\section{Introduction}
Epistemology is the branch of philosophy which is concerned with the theory of knowledge.
Knowledge is also studied in a variety of empirical sciences, including biology, psychology and cognitive science.
Understanding human cognition is primarily the province of empirical science, but already computing machinery and other technologies have substantially transformed the way knowledge is managed, and cognitive machinery is expected to take on a larger and broader role.

As human culture has advanced, so have the ways in which we gather, preserve, disseminate and exploit knowledge.
One tendency in those changes is toward greater abstraction from the particulars of human evolution which are the origins of knowledge as we know it.
These tendencies are particularly marked in pure mathematics and the recent discipline of mathematical or symbolic logic, in which completely abstract languages and modes of reasoning are adopted.

At the same time, the cognitive abilities which are essential to the advancement and exploitation of declarative knowledge are no longer exclusively seen in humans.
Though we may still have an edge in some important aspects, cognitive machines clearly excel humans in some areas, and in an evolutionary blink may soon dwarf human cognition.

Our development and exploitation of knowledge in science and engineering is a collaborative activity, and the resulting knowledge is a shared resource of which a large part is declarative knowledge held in non-biological media.
As we advance towards a society in which this collaborative enterprise becomes increasingly dominated by cognitive machinery it may be appropriate to think hard about the structure of that shared repository, and the ways in which it is expanded and exploited.

Synthetic epistemology, as it is here construed and considered, is devoted to that task, and offers some constructive suggestions about the logical structure of such shared repositories and the ways in which those structures support rationally underpinned reasearch, development and deployment.

Though the term ``synthetic epistemology'' has been used before, its use here is distinct as far as I am aware from that previous usage.
Possibly the most distinctive feature is that the present work is non-anthropocentric.
It is not concerned with analysis of natural language, or with the study of human cognition.
It is abstract and logical in character and is primarily concerned with ways to represent, organise and apply declarative or propositional knowledge so as to facilitate the fullest exploitation of deductive reason in all those domains in which such reasoning is possible.

Centralising epistemology around logical truth facilitates the evaluation and exploitation of empirical knowledge about the world around us and the laws which govern the behaviour of physical systems.
The use of formal rather than natural language makes precision of language possible and provides a context in which deductive reasoning on a very large scale can be trusted to preserve truth.

The discussion takes the concept of logical truth in two directions.
The first is foundational, not only defining logical truth and and identifying sound and reliable proof methods, but also provding a pragmatic response to the most important forms of scepticism about such foundations.
How do we make the concept of logical truth definite and precise?
How do we give meaning to the languages in which logical truths can be expressed?
How do we establish rules which reliably establish which inferences are sound and which propositions are true?

\begin{itemize}
\item Theoretical Foundations
  \begin{itemize}
    \item Truth conditional semantics
  \item Set theoretic truth
  \item Proof in set theory
  \end{itemize}
\item Practical Foundations
  \begin{itemize}
  \item Type theoretic truth
  \item Type theoretic proof
  \end{itemize}
\item Universality and Pluralism
  \end{itemize}

\section{Logical Truth}

A central aim of this exposition is to offer ways of representing and organising declarative knowledge which facilitate the large scale highly reliable application of sound deductive inference in assessing, elaborating and applying that knowledge.
There is a close connection between deductively sound inference and a conception of \emph{logical truth} which is the topic of this chapter.

In this foundational discussion of logical truth, as in many of the fundamental problems of philosophy, multiple philosophical specialities are involved in some way, these include, metaphysics, philosophy of logic, philosophy of language, and epistemology.
The topic of this chapter clearly belongs to the philosophy of logic, and is essentially concerned with language in which certain syntactic entities stand for other entities, among which are the propositions which might or might not be logical truths.
In order to clarify the nature of the entities which are involved both in the syntax and the semantics of this kind of language, we begin with some discussion of metaphysics.

\subsection{Metaphysics}

Logical truths are a feature of declarative or propositional language.
In this kind of language certain kinds of entity which we designate as \emph{syntax} are used to refer to other kinds of entity (which are the subject matter).
To define logical truth we must therefore be clear about what language is and in particular about the kind of language in which logical truths can be expressed.

Such a discussion will quickly involve reference to \emph{abstract} entities, and some preliminary discussion of the nature and status of abstract entities is desirable.

There are many things we want to discuss which do not seem to be material constituents of the universe around us (or any other, if there could be more than one ``universe''), such as the routes from Lands End to John o'Groats.
A more pertinent example is language and its semantics, both of which are abstractions from certain aspects of human behaviour.

It turns out, the way our languages work, that to conveniently discuss these abstractions we have to talk as though they exist, event though we may have difficulty in actually locating any examples.
For this reason, we find it convenient to allow for the existence of abstract entities, which have no spatio-temporal location and are causally independent of the material world.

We will find when we come to discuss the formal definition of languages which are suitable as \emph{foundations} for logical truth, that a choice of abstract ontology is important, and is an aspect of establishing the semantics of the formal language.
It is therefore tempting to think of that as a choice of convention, rather than as the identification of objectively existing realms.

This ontological conventionalism is not essential to the discussion that follows.
The considerations which speak in favour of the account would arguably hold good even if there was an objectively existent abstract realm, whatever that realm might be.
The arguments in its favour are pragmatic in nature.
It suffices for the utility of the ideas presented that it be logically coherent to talk about the chosen abstract realms, whether or not they objectively exist, and we may regard this talk as analogous to talk about coherently conceived fictional or hypothetical scenarios.
If we can imagine a route across the country which is consistent with the facts on the ground, then we can take the route even if the abstract entity does not objectively exist.

In talking of abstract entities I will normally be talking about \emph{purely} abstract entities, and this is to some extent captured by the tentative definition in terms of location and causal independence.
What this excludes is the kinds of abstract entity which are constructed from concrete entities, such as the set of Prime Ministers of the United Kingdom.
This kind of abstraction is essential in talking about the concrete world, but for pragmatic but formal purposes, it suffices to have a cleaner distinction between the abstract and concrete.

The idea that ontology should be considered conventional rather than absolute and objective is not exclusive to the abstract realm.
We have two distinct theories of gravitation due to Newton and Einstein.
It is generally accepted that Einstein's model is the more accurate and that Newton's is a good and more readily applicable approximation suitably in most circumstances and for most purposes.
Its a well publicised feature of Einstein's theory that it is based on a very different conception of space and time, an also that the conservation of mass and energy individually gives way in Einstein to the conservation of mass/energy.
Less widely publicised is the very different ontology of matter in
general relativity.
Whereas Newtonian mechanics thinks of matter as inhering in discrete particles with a finite mass, general relativity works with a field of continuously distributed matter.
In such a continuous model, there are no point masses with a finite mass, a finite mass is always spatially distributed over some volume of space time, and each point in that volume has a matter density but no mass.

\subsection{Language and Truth Conditional Semantics}

The use of the term ``logical truth'' has been a matter of controversy among philosophers, but it is not my intention here to enter into that controversy.
It is the concept which is important here, not the name used to refer to it.
Some may prefer to use a different name, for example ``set theoretic truth''.
Some reasons for preferring that term will quickly become evident in what follows and I have nothing to say against it, though I will seek to clarify my own terminological choices.


\ignore{
One of those reasons is the related distinction between propositions which make a substantive claim about the material world, and those which do not, but which may be seen to be true or false independently of material reality.
Typical of the latter category are the theorems of mathematics, and all propositions which speak only of abstract entities, i.e. of entities which are located in space or time, and are causally independent of the material world.
}%ignore


In describing ``logical truth'' I will be defining (substantially by reference to prior literature) certain languages.
These are ``foundational languages'', by which I mean languages which may be used to define all, or a large class of, other languages.
A problem of regress in the definition of such foundational languages must be addressed.

Either we have some language which itself is not defined or we must have a language which is directly or indirectly defined using that same language, or there is some infinite sequence of languages
We will discuss this further later, but for present purposes I will assume that the kind of language used to describe logical systems in the literature of mathematical logic is sufficiently clear.

\section{Declarative Language}

The concept of ``logical truth'' can only be given a precise definition in a suitable context.
A logical truth is a sentence in a certain kind of language, a declarative or propositional language.
This is the kind of language in which some subject matter is spoken of using sentences which make a claim about that subject which may be either truth or false, depending on whether the subject matter conforms to conditions which are understood to be expressed by the sentence.

These conditions are called ``truth conditions'' and for present purposes the truth conditions may be considered the proposition expressed by a sentence (though in other contexts a very different notion of proposition is called for).

\subsection{Some Metaphysics}

In defining a declarative language various entities are involved.
It is in the essence of such languages that there are things about which propositions are expressed (belonging to the \emph{domain of discourse}), and there are other things which are understood to express the propositions (\emph{syntactic} entities often called sentences).
Expressing a proposition about some subject matter, or domain of discourse, usually will involve referring to some (or all) of the things which exist in that domain, and will refer to them using syntactic entities (which are often called ``terms'').

Though my positive philosophical stance is ontologically conventionalist, this account of logical truth is ontologically neutral.

\subsection{Truth Conditional Semantics}

\section{Set Theoretic Truth}

In this chapter I give a definition of the concept of ``logical truth'' as it is used in this exposition.
This involves defining a class of logical systems each consisting of an abstract syntax, a truth conditional semantics and a semi-decidable deductive system which is sound relative to the semantics.
The sentences of these languages which are ``valid'' under the semantics are the logical truths expressible in the language in question.

I then defend the following claims:

\begin{itemize}

\item That there are languages which are \emph{practically universal} for logical truth, by virtue of the reducibility of logical truth in all other languages to logical truth in those languages.
\item That there are deductive system for these universal languages which are \emph{practically universal}, and which may therefore serve to reliably establish practically all logical truths.
\end{itemize}

In defending these claims I will offer arguments responding to the following contrary sceptical claims:

\begin{itemize}
\item Arguments from regress against the possibility of defining the semantics of languages, and against the possibility of conclusive deductive proof.
\end{itemize}

This is a \emph{foundationalist} enterprise, but one which affirms the utility of foundational thinking and foundational technologies, rather than promulgating absolutist claims about foundations.
It is not my position that it is possible to give absolutely precise or unambiguous definitions of semantics, or that we can ever demonstrate truth with absolute certainty, but rather that these things can be done to a degree of precision and certainty which greatly exceeds any practical need.

Foundations in the real world are solid bases on which important constructions can be raised.
Though they themselves must be sufficiently solid, they do not have to be absolutely solid, and their solidity does not depend upon them resting upon some other equally or more solid substrate.
They are often devised because their structure enables a solid foundation to be built on a more tenuous substrate, by techniques such as the use of rigid rafts or deeply penetrating piles.

Similarly, the advancement of precision engineering is possible because a machine tool used suitably can build another tool which has superior precision.
It is possible in semantics, to defined a language which has precise and unambiguous meaning using a language which is rather more unruly.
This is what has happened throughout the progress of science.
Mathematics has been a source of more precise language for science, and in recent times, mathematical logic has advanced the precision beyond mathematics beyond its former confines.

There are two logical systems which I nominate here as my preferred exemplars of the kinds of practical universality in my claims.
These are both well established systems of impeccable repute, but they are not unique in exhibiting the characteristics which I applaud.
In these basic systems I offer no novelty, though later in the discussion I will talk about some ways in which further elaboration might be beneficial.

That there are two, arises from my belief that the requirements for addressing the philosophical scepticisms which may be raised differ from those which are convenient in the application of the logical systems.
The more practical system is the logic implemented in the Cambridge HOL system, which was devised at the hardware verification group at the University of Cambridge for the purpose of reasoning about he properties of digital electronics, but which has subsequently been applied more broadly.
The other system which is convenient for the purpose of foundational discussions, and in which the semantics of Cambridge HOL has been rendered, is the first order set theory ZFC\footnote{Zermelo Fraenkel with Choice}.
These systems are have both been carefully defined and extensively studied and applied, the former being defined in terms which may be interpreted in the latter.
These descriptions will not be reproduced here except in those aspects which prove essential to the philosophical discussions which are my focus, though detailed references are provided below.
  


\chapter{Empirical and Other Truths}


\chapter{Attik2}

\section{Introductory material}

It is divorced from humanity in just those ways in which the work of mathematicians seeking solidity to their methods detached mathematics from the vernacular and made reasoning into the calculus which Leibniz had sought before him.
That calculus was a great achievement, but not a practical way to do mathematics, or any other kind of reasoning, for mere mortals.
Pushing through Frege's



The idea of \emph{logical truth} is therefore central to this presentation, and is used in a broad sense now widely deprecated.
The meaning of the word ``know'', and hence the difference between knowledge and belief is not something I consider, but I do address some of the classical concerns of epistemology, including sceptical arguments about meaning and truth.

The centrepiece of the ideas presented is the idea that ``logical truth'' can be given a broad and practically universal characterisation.
To give a sense for the use here of the term ``practically universal'' consider the significance of Godel's celebrated demonstration of the incompleteness of arithmetic.
There is no controversy about this theorem, insofar as it is generally accepted as true, but its practical consequences are not conspicuous.
We have available to us a heirarchy of theories of increasing strength in which to demonstrate recalcitrant conjectures, and though there are enture mathematical theories developed to exhibit the limitations of the erstwhile standard foundation for mathematics, they all fall under the sway of the stronger foundations invigorated by a suitable ``large cardinal axiom''.
Meanwhile, in the more prosaic realms of real world applications of mathematics, such extremes are not called for.


It has been used for epistemology conducted as a kind of empirical philosophy, concerned therefore with reaching synthetic rather than analytic results in the sense give to those terms by Kant.
By contrast, the usage here is closer to the more ancient usage (but still more broadly understood than Kant's wrinkle) in which analysis, breaking down, is contrasted with synthesis, putting together.

Hence, we are concerned here to synthesise epistemology in the sense of putting together a theory of knowledge, which is done with the intent of contributing not only to our understanding of what knowledge \emph{might be}, but also to inform the construction of more effective ways of gathering, evaluating and applying knowledge.



Its concern with knowledge is not a concern with any material phenomenon but a concern with the structure of explicit (propositional, declarative) knowledge as it may be represented in shared resources whose form is designed rather than evolved.
It is this concern which legitimises the idea an epistemology as a synthesis.
This kind of theoretical approach to epistemology should be seen as analogous to the elaboration of mathematical theories or the design of notations or languages, which it does indeed involve.

I am therefore here concerned with propositional or declarative knowledge, that kind of knowledge which consists in knowing that some sentence or proposition it true and which is therefore held expressed or manipulated in some language.
However, it not the knowing which concerns us, it is that which is known, for which we retain the word, for discursive purposes, while stepping back from some of its everyday connotations and associations, or even its everyday meaning.

The ways in which this kind of knowledge works have been transformed over the last few millenia as a result of advances in our knowledge, and this synthesis is intended to continue that progression.
A first step was the development of axiomatic geometry by the ancient Greeks, which showed that by draconian restriction of language and a focus on abstractions which can readily be given a concise (axiomatic) characterisation, deductive reason can be conducted with very high levels of accuracy and reliability.
Sharp language enables rigorous reasoning.

In the same time it was amply demonstrated that ostensibly the same rational methods failed to deliver similar results in a broader linguistic context.
The solid results obtained by the axiomatic method in reasoning about mathematics could not be reproduced in the applications of reason to metaphysics and cosmology.

Aristotle's pioneering investigations into logic were to prove influential for the next two thousand years, but failed to broaden the scope over which deductive reasoning would yield good results.
So impotent were they that Aristotelian Syllogistic was to be entirely dismissed by such important figures in the rise of modern science as Bacon and DesCartes.

Leibniz, on the other hand, was thoroughly convinced of the potential for application of Aristotelian logic, and saw in it a route to the mechanisation of ``demonstrative science'', which would have constituted a transformation in our conception of scientific knowledge if it had been realised.
His confidence in Aristotle's logic was misplaced, but his sense of direction was sound.
He was active in progressing scientific method in more realistic ways, including the establishment of scientific institutions and journals, a universal language.
These advances shifted science toward a globally cooperative enterprise managing and extending a shared corpus of knowledge.
It was but a few hundred years before technical advances made progression of his ideas feasible.

It was in the 19th and 20th Centuries that mathematicians, after two millenia of refinement and elaboration of mathematical notation turned their attention to logic, and hence to a conception of formal logic which was potentially applicable in any body of declarative or propositional language.
During that time, the computational machinery became of age, and rapidly progressed in its computational power, making feasible computation on a scale previously unimaginable.

There is in this sketch a progression whereby more is achieved by less.
To make reasoning more reliable, much simpler language is chosen and the domain of discourse is limited to a space in which meanings are precise because governed by fiat rather than being determined by prior discourse with its ambiguities and inconsistencies.

One last point in this progression flows from the progress which was achieved in addressing the precise description of languages both ibn their syntactic form and in their semantic content.
To achieve precision and clarity in this enterprise, mathematical languages are used, and this effects a factorisation in which languages speaking of the material world are rendered completely in terms of abstract entities and then connected to the material world by describing the relationship between material entities and the abstract entities which are their surrogates in the abstract semantics. 

The ``synthetic epistemology'' which is presented here provides a prescription for a distributed knowledge repository which is collaboratively developed, extended and applied in a manner not dissimilar to the share resource which is the internet, or the World Wide Web, and would indeed most likely be accessible from the world wide web (though that will not be further discussed here).

At the core of this synthesis is an abstract universal representation of \emph{logical truth}.
The definition of this notation includes semantics and detailed inference rules.
The system is not new, and much of the detail, which is not significant is identified by reference.
There are three main directions in which the discussion here progresses and which are undertaken in the following order:

\begin{itemize}
\item Foundational responses to sceptical doubts, particularly in relation to the problem of regress in the definition of the semantics and the soundness of the deductive system (and hence the soundness of the deductive closure).
  
\item A presentation of the scope of applicability of models constructed in the system.

\end{itemize}

\section{Logical Truth}

The system I describe here is an abstract language and deductive system for the broad conception of logical truth, which is often called ``set theoretic truth''.

I take a language to be a system of representatives.
These are tokens for entities, some of which are truth values.





\chapter{Attic}

\section{Why Synthesise?}

The advent of synthetic biology marks possibly the largest ever transformation in the way in which the ecosystem of planet earth and humanity evolves.
The transformation from an evolution primarily engendered the blind forces of chance mutation and natural selection to one in which \emph{intelligent design} plays an increasing role.

That we have reached this point is due to evolution itself, which has shaped not only our physical nature but our culture, the ways in which we seek and apply knowledge, and the nature of knowledge itself.

Just as the study of biology is moving from observation to design, the study of knowledge may also graduate, from analysis and observation to synthesis.
This may be the moment for epistemology naturalised to yield ground to epistemology synthesised.

Synthetic biology is not the most trenchant scientific advance to suggest this approach to epistemology, it is surely eclipsed by cognitive science and engineering.
Though much of the engineering of artificial intelligence is focused on replicating human capabilities, the expectation of going beyond is high, and the invitation to re-think the nature of knowledge and the role of intelligence in its genesis and exploitation is surely compelling.

Beyond the relatively conservative scientific academia epistemology has has been more radically transformed by Michel Foucault and ``post-modern'' philosophy.
From his perspective epistemology is always culturally relative and is a sel-interested power-play by the dominant groups in each society, a perspective which serves to justify activist groups in moulding meaning and truth to their agenda, and making truth and freedom a casualty of ideological zeal.
This involves the denial that knowledge and the ways in which it is established and applied are instrumental in advancing the prosperity of humanity, and invites an epistemological synthesis which supports a robust challenge to Foucauldian nihilism.

The rationale for synthesis faces both ways.
It is a defence against the kind of progressivism which would abandon the instrumental advantages of objective truth for the sake of seizing political power.
On the other hand it peers forward into a future in which the instrumental advantages of knowledge are leveraged not only using the intellectual power of \emph{homo sapiens} but also by an increasingly diverse array of intelligent machinery.

\section{The Approach}

Postmodern scepticism may have begun as a critique of ``hegemonic'' epistemes which characterised them as power-seeking or power-preserving rather than being rooted in instrumental approaches to objective truth, but it graduated to the more radical scepticisms associated with the denial of objective truth consequent on the denial of meaning to language.
It may be noted that similarly sceptical arguments about meaning appear in mid 20th Century analytic philosophy and miraculously served to discredit the scientifically oriented logical positivism without impinging on less scientifically oriented approaches to analysis.

This epistemology will therefore be responding first of all to scepticism about the meaning of language and the possibility of sound deduction.

************


Epistemology one may therefore argue, should not be seen as the theory of a fixed phenomenon of which we seek to advance our understanding, but rather of a developing phenomenon in relation to which we may play a role not merely in understanding the present but also in shaping the future.

The term ``synthetic epistemology'' is adopted here for an epistemology which is designed rather than hypothesised.
In undertaking such a synthesis one must surely have some criteria in mind which will guide the choices which must be made and any subsequent evaluation of the merits of the resulting epistemology.

The nature of knowledge has been changing throughout the evolution of life on earth.
Epistemology, the \emph{theory} of knowledge, is recent innovation, and has itself evolved over the two millennia since the writings of Plato and Aristotle.

The use of deductive reason in its simplest forms is probably coeval with \emph{homo sapiens}, since it depends on language.%
\footnote{It is not universally agreed that non-linguistic primates are \emph{by definition} unable to undertake deductive inference, but no definition of deduction seems possible which does not explicitly or implicitly refer to language.  To be deductive, and inference must be \emph{logically sound}, and must therefore be expressed in a language which has a definite semantics.}
It's explicit, systematic and elaborate use is much more recent, beginning with the transformation of mathematics into a theoretical discipline by the ancient Greek philosophers, most famously in the development of Euclidean geometry.

Over the next two thousand years the development of mathematics underpinned advances in science and engineering which enabled the industrial revolution and transformed the prosperity and well being of humanity.
During the last two hundred years, the continuing advances of mathematics included a renewed attention to the foundations of mathematical analysis which ultimately addressed the logical foundations of the discipline.
These advances lead to a new sub-discipline of mathematical logic, which had an epistemological significance not confined to the foundations of mathematics, but, as will be argued here, thoughout the entire domain of propositional or declarative knowledge.

These impact upon how reliably deductive reasoning can be conducted, and beyond that to the kinds of results which can be achieved by deduction and the ways in which deduction can facilitate our knowledge of all matters in which deductive reason is possible.

Though the theoretical advances in logic which transform our understanding of logic are now well established, their fullest exploitation still remains for the future.
In the first instance the advances were of limited applicability because of the complexity of the formal derivations which they involved.

The pioneering work by Frege, and by Russell and Whitehead, showed that mathematics could be derived in appropriate fully formal logical systems.
But at the cost of great complexity, of which the benefits have seemed too slender to encourage those efforts to be continued.
The new discipline of ``Mathematical Logic'' or meta-mathematics flourished, but this was a new branch of mathematics primarily concerned with the theoretical studies of mathematical interest in relation to these new formal systems.
There was little interest among mathematicians in adopting the new standards of rigour exhibited by \emph{Principia Mathematica}\cite{russell10}.

\section{The Logic}

This synthetic epistemology is oriented toward the management and exploitation of a shared distributed body of propositional knowledge.

This consists in a description of the abstract structures involved, an account of the semantics of the propositions within it, and a set of rules which govern the deduction of new propositions, and of extension of the languages in which these propositions are presented.

The particulars of the logical system are to some degree arbitrary, there are many similar logical systems which are equivalent in many ways.
This is not an area in which this epistemology looks for innovation and the system around which the proposal is based is therefore a well established system which not only has very distinguished origins, bing substantially based on the work of Russell, Ramsey, leading to  Church's Simple Theory of Types.
This system has been further developed in the course of applied research, notably by Gordon and the Cambridge University Hardware Verification Group.
This system is well documented, the documentation including an account of the language and its semantics, a specification of the deductive system and arguments supporting its soundness relative to the given semantics.
In addition to an informal set theoretic account of these fundamentals there is a formal presentation in the language itself.

It is not therefore intended to present the details of this system here, but for philosophical purposes we will not entirely rest upon the impeccability of its academic credentials, for epistemology is expected to respond in some way to sceptical arguments to which non-philosophers would give no credence.

We will take such sceptical arguments seriously and sketch ways in which the credibility of these arguments may be reduced to minimal levels.
This applies to sceptical doubts, for example, about the definability of the semantics or the soundness of the proof system, which are the most crucial theoretical issues.

Some of the more speculative ideas presented here cannot be given similar levels of assurance, these include a suggestion of \emph{near universality} and the most important question of whether or not this synthetic epistemology is practical and optimal for progressing rigorously and productively the future of science and engineering in a world in which human endeavours are supplemented by artificial intelligence.

The epistemology proposed concerns primarily those kinds of knowledge about which it is possible to reason rationally, and is intended to ensure that such reasoning is correct, and hence that its results are true.

I will be using the term ``logical truth'' for that kind of true proposition which can be established by the epistemic methods described, and held in the manner described, which may be thought of either as a \emph{theory hierarchy} or a \emph{knowledge repository}.
This choice of terminology may be controversial, particularly among philosophers, but it is not my purpose to engage in terminological controversy, so I will not be arguing the merits of that choice.

\section{Structure}

The centrepiece of this exposition is the concept of logical truth, and the exposition therefore begins with the questions of how logical truth can be defined, how logical truths can be established and how to organise logical truths into a coherent hierarchies of theories.
This gives a position on the main scepticisms relevant to this topic, particularly, scepticism about giving meaning to language so as to make the truth of propositions definite, and about the possibility of conclusively demonstrating logical truths.

The structure presented here reflects my belief that near universal languages for logical truth exist, and that this is reflected in the existence of universal families of a kind similar to those which were thought to follow from the undefinability in arithmetic of arithmetic truth.
At their outer reaches both the semantics and the proof systems for logical truth become progressively more tenuous, but these outer reaches are tested only by the furthest reaches of set theory.
Within the more down to earth mathematics of empirical science and engineering, nether residual semantic ambiguity nor the incompleteness of deductive systems is practically significant.

Logical truth is propositional knowledge, i.e. knowledge expressed in language as propositions.
It therefore depends upon language and upon languages with unambiguous semantics (or truth conditions).
To define semantics we have to have some subject matter for these truths to be about, and we need some method for determining which sentences express true propositions under the given semantics.
Identifying the necessary ontology demands some discussion of metaphysics.

Epistemologically this is an account of a priori knowledge, which draws in some elementary considerations from, metaphysics, the philosophy of language and of logic, and ultimately yields a structure most relevant to cognitive engineering, a prescription for how a priori knowledge might be organised by some forthcoming artificial intelligence.

\section{Metaphysics}

This epistemological synthesis is ontologically conventionalist.

Whatever topic we care to discuss, however abstract, the aspects of that topic under discussion must ``exist'' in order for us to be able to talk about them.
For example, if we are responsible for deliveries, we need to be able to plan routes, and we will be concerned to know whether the is a route which will permit the goods in hand to be delivered within the constraints of time and fuel.
We therefore seek an answer to the question whether such a route exists, but we have no interest in any possible metaphysical question as to whether delivery routes exist.

If we are in the business of devising formal languages for addressing particular problem domains, we must when settling the semantics of our language, make the necessary ontological provisions for the existence problems in the problem domain to receive satisfactory answers.
Just as in all other aspects of the design of the language



\chapter{Appendices}

\section{The Evolution of Knowledge and Epistemology}

\subsection{Claude on the evolution of memory}

Here is a brief overview of the evolution of memory systems:

- Single-celled organisms like bacteria have simple memory in the form of biochemical pathways that allow them to adapt to stimuli and environments. This is a rudimentary form of memory.

- Invertebrates like Aplysia sea slugs exhibit simple forms of short-term memory and sensitization to stimuli. This allows basic learned behaviors.

- Arthropods like insects and crustaceans have more complex memory centers like mushroom bodies which allow more sophisticated learned behaviors and navigation. 

- In vertebrates, the hippocampus is the primary brain region associated with memory formation. Its structure evolved along different paths in amphibians, reptiles, birds and mammals.

- Long-term memory formation likely evolved in early bony fishes around 300-400 million years ago.

- The neocortex, critical for many higher-level cognitive functions like language and reasoning, evolved in mammals and reaches its most advanced form in humans.

- Long-term potentiation, the strengthening of synaptic connections, provides a key neuronal mechanism for memory storage in mammals.

- Episodic memory or autobiographical memory is thought to be uniquely advanced in humans compared to other species. 

So in summary, memory has evolved from basic biochemical mechanisms in single-celled lifeforms to increasingly complex systems across invertebrates, lower vertebrates, mammals and primates. More advanced memory systems have coevolved with higher intelligence and learning capabilities.

\subsection{Claude on the evolution of homo sapiens}

The genus Homo evolved around 2-3 million years ago during the Pliocene epoch. Here is a brief overview of the timeline of the evolution of the genus Homo:

- Around 7 million years ago, the evolutionary line that would lead to humans diverged from the ancestors of modern chimpanzees. 

- By around 4 million years ago, early hominins belonging to the Australopithecus genus had emerged in Africa. They exhibited bipedal walking.

- The earliest proposed member of the Homo genus is Homo habilis, which emerged approximately 2.8-2.3 million years ago. They were the first hominins to use stone tools.

- Homo erectus evolved almost 2 million years ago and were the first hominins to migrate out of Africa into Eurasia. They used more advanced stone tools and controlled fire.

- Archaic Homo sapiens evolved around 300,000 years ago and were the predecessors of modern humans, with more developed brains and skulls. 

- Anatomically modern humans, Homo sapiens sapiens, emerged around 300,000-200,000 years ago in Africa. 

- Homo sapiens developed complex language, art, and technology, allowing migration around the world beginning 60,000-70,000 years ago.

So in summary, the Homo evolutionary line emerged roughly 2-3 million years ago and has given rise to modern humans through a series of transitions and developments over millions of years.

\subsection{and on the evolution of the human brain}

The evolution of the brain in genus Homo progressed through several key stages:

- Homo habilis (~2.8-2.3 million years ago): The brain size of H. habilis averaged about 600-750 cc, larger than the 500 cc average for Australopithecines. This allowed more advanced tool use.

- Homo erectus (~2 million years ago): H. erectus brains averaged around 900 cc with expanded parietal and frontal lobes compared to H. habilis. This improved cognition for complex hunting and tool usage.

- Homo heidelbergensis (~600,000 years ago): Brain size increased to an average of 1,100-1,400 cc. The morphology was essentially modern. Improvements to linguistic capacity emerged.

- Archaic Homo sapiens (~300,000 years ago): Brain size plateaued around 1,200-1,750 cc with the cranial capacity very close to modern humans. Complex language likely evolved around this stage.

- Homo sapiens (~200,000 years ago): The modern human brain averages around 1,350 cc but overlaps with late archaic human ranges. Further neurological rewiring improved language, reasoning, and social cognition. 

- Behavioral modernity (~50,000 years ago): Rapid cultural and behavioral innovation indicates complex neural circuitry nearly identical to modern humans emerged around this time period.

So in summary, the Homo brain progressively increased in size and neurological complexity over several key stages, with rapid development of cognitive abilities related to language, social interaction, and abstract thinking in the later transitions leading to modern humans.

\subsection{Kinds of Knowledge}
For present purposes let us divide knowledge into three kinds:
\begin{itemize}
\item knowledge \emph{by acquaintance}
\item knowing \emph{how}
\item knowing \emph{that}
\end{itemize}

Knowledge of the first two kinds predates language and is difficult to share.
In the first instance we may consider the ability to survive in some suitable environmental niche as a form of knowledge which is secured by evolution, built into anatomy and its chemistry, and passed on by genes.

\subsection{Knowing How}

The evolution of life on earth can be viewed from many different perspectives.
For the purposes of this epistemological synthesis, it may be helpful to consider in the evolution of life the progression of ways in which living organisms or cooperating groups of organisms (not necessarily of tne same species) proliferate by the use of progressively more comprehensive and sophisticated modelling techniques, which progression this epistemology aspires to continue.

I begin only as multi-cellular organisms with some elementary nervous system have evolved, exhibiting behaviour in seeking nourishment and shelter, and avoiding predators and other perils.

For the purposes of this synthesis I consider knowledge as falling into three kinds, knowledge by acquaintance, knowing how, and knowing that.

The first two, I suggest, are kinds of knowledge which primarily reside in the structure of central nervous

\subsection{Stages in the Development of Memory}

The story here is mostly taken from \cite{murray2017evolution}.

\begin{itemize}
\item[early animals]
  
  \begin{itemize}
\item[]
\item[various]
  \end{itemize}

\item[early vertebrates]
  
  \begin{itemize}
\item[]
\item[Navigation memory]
  \end{itemize}
 
\item[early mammals]
  
  \begin{itemize}
\item[]
\item[Biased-competition memory]
  \end{itemize}
 
\item[early primates]
  
  \begin{itemize}
\item[]
\item[Manual-foraging memory]
  \end{itemize}
 
\item[anthropoids]
  
  \begin{itemize}
\item[]
\item[Feature memory]
\item[Goal memory]
  \end{itemize}
  
\item[hominims]
  
  \begin{itemize}
\item[]
\item[Social-subjective memory]
  \end{itemize}
  \end{itemize}

\subsection{Deduction}

It is only very recently in the evolution of life on earth that the possibility of exploiting knowledge by systematic and elaborate deductive inference has been known.

Some two and a half millennia ago the ancient Greek civilisation transformed mathematics into a theoretical discipline by instigating the systematic deductive exploration of geometry and arithmetic.
In doing so they demonstrated in these very particular domains, that deductive reasoning could be conducted with very high degrees of reliability, so that the truths thus obtained were universally accepted and have endured ever since.

By contrast, in the same period, the use of reason in other domains, such as the study of nature and the cosmos, proved unable to realise enduring consensus, and the use of \emph{reductio ad absurdum} facilitated the demonstration of absurdities.

Throughout subsequent history the stark contrast between what can be established with certainty by reasoning about abstract domains and all reasoning, deductive or otherwise, about the concrete world was frequently recognised.
As frequently, philosophers have sought the imprimatur of deductive rigour for their views and have claimed a broader scope for its conclusions than the historical record could support.

At the same time, the potential benefits of deductive reason have remained largely untapped.
This was in part due to the incomplete understanding of deductive logic which we inherited from the ancient Greeks.
When that defect was remedied, the complexity of detailed formal logical reasoning deterred its widespread adoption, and only in special circumstances where the extra costs involved were acceptable or in the context of academic research and with the aid of computer support, have the techniques been progressed and applied.


\section{HOL History and References}

\subsection{Some History}

The established logical system around which this epistemology is built is that of the ``Cambridge HOL'' Interactive Theorem Prover, the acronyn HOL standing for Higher Order Logic and used to refer to the theorem prover, the language and the deductive system.

This system descended from the logical system of \emph{Principia Mathematica} in the following stages:

\begin{enumerate}
\item Russell published in 1908 his ``Theory of Types'' \cite{russell1908,heijenoort67} which was to be used for the derivation of mathematics in Principia Mathematica \cite{russell1913}.
\item Frank Ramsey observed that the ramifications in Russell's Theory of Types were unnecessary, allowing the logical system to be simplified to the Simple Theory of Types.
  He wrote a manuscript on this in 1923, which was published posthumously in 1925 \cite{ramsey25,ramsey1931}
  Full details of this system were first published by Hilbert an Ackerman in their ``Grundlagen der Theoretichen Logic'' \cite{hilbert1928}
\item Alonzo Church, after failing to construct a consistent foundation for mathematics using the type-free lambda calculus re-cast the Simple Theory of Types as a typed lambda calculus \cite{churchSTT}.
\item In his work building theorem provers for software verification using Dana Scott's Logic for Computable Functions \cite{scott1993type}, Robin Milner adapted the type system in Church's STT by moving type variables from the metalanguage into the object language.
  This system later became known as the Hindley-Milner type system.
  During the development of the LCF theorem prover Mike Gordon joined Robin Milner at the University of Edinburgh, and would later be the lead figure in the continuation of work on LCF at Cambridge.\footnote{He later documented the history in \cite{gordon2000lcf}}

\item When switching from software verification to hardware verification at Cambridge University, Mike Gordon decided that Higher Order Logic would provide a better basis for that work, and adopted Church's Simple Theory of Types with a number of relatively uncontroversial adjustments for that work.
  The polymorphic version of the type system was one of them.
\end{enumerate}

\subsection{References}

  The following documents describe the logical system as it now stands.
  THe definitive description of the HOL logical system, including syntax, semantics, deductive system, rules for conservative extension, and the theory heirarchy was undertaken by Andrew Pitts and is available for download from the HOL repository on sourceforge \cite{pittsHOLlogic}.
  
  The following paper might by itself be sufficient, since it documents the most recent changes and cites previous literature \cite{arthan2016}.
  The ProofPower implementation of the HOL logic has an extensive formal treatment in HOL itself 
  \cite{arthanspc001,arthanspc002,arthanspc003,arthanspc004,arthanspc005} available from the {\tt lemmaone.com} website.
  Also presented at the VDM91\footnote{VDM ’91, Formal Software Development Methods} conference \cite{arthan91}.
  
\section{Positivist Metaphysics}

Positive philosophy is particularly noted for its rejection of Metaphysics.
This comes down as far as Rudolf Carnap who's anti-metaphysical stance remained was unremitting.

There are two principle conceptions of metaphysics which Carnap rejected.

The first can be captured in the phrase \emph{synthetic a priori}.
Metaphysics is thus conceived of as truths about the ultimate nature of reality which are established by rational means, rather than by observation.

A related critique of metaphysics flows from the empiricist desire to connect meaning with observation, if a proposition had no empirically verifiable consequences then it was either tautologous self-contradictory or meaningless.

The younger Carnap spoke of metaphysics as meaningless, the elder complained only of a lack of cognitive content.

At the same time Carnap broke with the Positivist tradition in a number of ways which effectively softened his stance on Metaphysics.
Firstly, he abjured nominalism.

To support a rich notion of logical truth we need to have things to talk about.
It doesn't matter what they are.
It does how many there are.

To get a ``universal'' conception of logical truth depends upon having arbitrarily many entities.
This is realised in set theory through ``large cardinal axioms'', which provide ever increasing lower bounds on the cardinality of the unverse of sets, in the context of a single axiomatic set theory.

In our logical system a similar method suffices.
The Simple Theory of Types has usually come with a single axiom of infinity sufficient for arithmentic, which asserts that there are infinitely many ``individuals''.
This enables the definition of the natural numbers and the operations upon them.
Given closure under the function space constructor, there will also be types suitable for representation of rational numbers, the more numerous real numbers and most applicable mathematics.
For the more exotic reaches of research in set theory, the same logical system can be extended with a stronger axiom of infinity or the required results can be rendered as conditional on the same ontological principles. 

  
\part{Essay Fragments}

One essay attempt per chapter.
This isn't a historical record, they are here to get mangled and recombined, or discarded.

\chapter{Synthetic Philosophy}

\section{Introduction}

This monograph presents a \emph{philosophical system} belonging to what has latterly been called the \emph{analytic} tradition.
Philosophy in that tradition, though predominantly engaged in varieties of supposedly rational analysis, have been, to a degree, understandably, anthropocentric.

David Hume's philosophical \emph{Magnum Opus} was ``A Treatise of Human Nature'' \cite{humeTHN} and his subsequent briefer presentation of its essence ``An Enquiry into Human Reason''\cite{humeECHU}.
A couple of centuries later the linguistic turn favoured logic and language over psychology, but resisted formalisation to retain its anthropocentric engrossment in the natural languages of \emph{homo sapiens}.

As I write we are firmly in the 21st Century.
``Artificial Intelligence'' is penetrating through the technology on which we now all depend, spawning on its way the new term ``General AI'' to denote what AI was once expected to be but has not yet become.

The colonisation of Mars has moved from science fiction into the plans of real world corporations contemplating the means to send a million tons to the red planet.
The rocket factories are in place and growing, while the design and testing of ever larger interplanetary transports continues to progress,

Meanwhile, the manufacture of android robots begins, destined to build on Mars the life support systems necessary for human habitation (and the fuel factories for return journeys).
A good measure of autonomous intelligence will be required.

These are the first steps in a process which may ultimately lead to the proliferation of intelligence across our Galaxy and into the Universe beyond.
That androids should be pioneers in colonising Mars, presages their prominence in any subsequent interstellar progression.
Ultimately we may find biological intelligence a crucial but small kernel of a sphere of human cognitive influence.

What then of epistemology?
The proliferation of intelligence across the Galaxy will be a technology intensive process, dependent on continued advances in science, technology and engineering.
What kind of epistemological theories will best support that process?

The central roles in the accumulation of knowledge played by the human intellect, sense organs, language and culture, and the imprint on these resulting from the mechanics of the evolutionary process, may not be ideal for inanimate intelligence in completely different and very varied circumstances.

Is it possible for \emph{homo sapiens philosophicus} to detach himself from his context and origins to produce philosophy relevant to diverse cognitive systems in very different circumstances?
Could philosophical foundations emerging from such enquiries bear fruit also for earthbound humanity?

[*****rework:{\it

I believe that such an enterprise may be worthwhile.
We can see in fundamental advances made in the last couple of centuries, an unwitting tendency in that direction.
The advancement of mathematics beyond its concern with numbers into more abstract and diverse subject matters, and the reduction of the whole to the simplest logical systems and ontologies, far removed from everyday language and commonplace clutter, ultimately to the benefit of science, engineering and human well-being and prosperity is just such a trend.
We have arguably yet to come properly to terms with the results of the combined effort of mathematicians and philosophers (and later computer scientists) in developing theoretical and formal approaches to language and logic, and may only do so when the complexity of detailed analysis is brought under control by the combined computational power and penetrating intelligence which is anticipated in our new cognitive assistants.

Even before these modern developments the simple structures of mathematics are among those subjects of human scholarship which can least be suspected of anthropocentrism.
Mathematics must surely begin with counting.
The details of numerals for recording how far a count has proceeded (and hence how many were counted) are arbitrary and human, but the structure of the natural numbers which the numerals designate seems uniquely determined by the very nature of counting.
It is impossible for us to imagine how any alien intelligence could not be acquainted with exactly the same number systems.

This tendency of mathematics to begin with the simplest structures and concepts was conspicuous when mathematics was brought to bear upon language and logic.
Whereas Aristotle's Organon%
\footnote{The collection of his writings related to logic.}%
and its scholastic elaborations were dressed in accidents of human cultural evolution, Frege's \emph{Begriffschrift} \cite{frege79}, %
\footnote{a seminal work in the early stages of the modern transformation of logic}%
  although undoubtedly an earthly notation, was much closer to exhibiting that compelling power and simplicity which marks the universal.

}]

\subsection{The Structure of Synthetic Philosophy}

In calling this \emph{synthetic} philosophy I suggest that it is constructed rather than discovered.
It is constructed in the context of a body of knowledge gathered and refined over the last several millennia and exploits some of the insights which that knowledge brings, not least on the nature of knowledge itself.
However, it is primarily pragmatic in its intent, not an addition to that body of knowledge but some ideas about how to move forward in its light.


These ideas are primarily epistemological, concerned with the ``theory of knowledge'' and in its implications for \emph{knowledge management}, how to gather, expand and exploit the knowledge which is essential to our future well-being and prosperity.

Having made a fuss about \emph{anthropocentrism} I will occasionally be commenting of the de-humanisation with results from its avoidance.
The first of those concerns how ``theory of knowledge'' is to be construed.

The word ``know'' belongs to the English language, which we can only know by observation, which in turn shows a variety of usage difficult to incorporate into a single coherent model.
If we sought to rigorously address the phenomenon spoken of by the word ``know'' our case would be hopeless.

Advances in science often depend on the introduction of new concepts, such as the the displacement for scientific purposes of the terms ``hot'' and ``cold'' by carefjlly defined temperature scales.
The word ``know'' is similar in its unsuitability for scientific purposes, and our synthetic philosophy is intended to share or exceed those characteristics of science which make it so.

So the first point of de-humanisation is to detatch our story from the word ``knowledge''.
This doesn't mean you won't be seeing the word again, it means it will not play a technical role in what follows, it belongs to the informal descriptions which are inteded to make the technical content intelligible.

So one of my first tasks will be to explain what the ``theory of knowledge'' becomes when we detach it from the English language.



Synthetic philosophy is primarily epistemologica;, it is concerned with the nature of knowledge, how it can be established, organised and applied.
But not with the meaning of the word ``know'', which is used informally and does not have a fundamental role in the philosophy.

In its concern for knowledge the distinction between knowing \emph{how}, which is to say, having some skill or capability, and knowing \emph{that}, being  is important

The central concept of synthetic philosophy is that of \emph{Logical Truth}.

\subsection{Positivist Metaphysics}




\chapter{Oracular AI (2)}

\section*{Preface}

It was in 1986 that I first had hands on an ``Interactive Theorem Prover'', the Cambridge HOL system,
engineered to support formal reasoning about digital electronics.
I had just began working on the applications of formal methods to the development of security and safety critical information systems.

One of my early reactions on acquaintance with this system was to perceive the broadness of its potential applications.
The theory hierarchy which HOL supported, enabling the structured development of abstract models and of  the underlying mathematics required to build such modesl was in effect a general purpose \emph{knowledge base} of a kind relevant to GOFAI \footnote{Good Old Fashioned AI} with some extra features not often mentioned in AI.

The ability of the system to ensure logical consistency, surely essential for large scale deductive reasoning to have any value, together with a strong cultural preference for staying within those bounds, and the general perception that applications in engineering require nothing more, provided a seed in my mind which I have nurtured (perhaps intermittently) ever since, without as yet succeeding in articulating its relevance.

Some years later, when my own practical involvement in the application of interactive theorem provers was drawing to a close, that seed found a new place in my more philosophical aspirations.
I have struggled ever since to articulate the ideas which grew from that seed, and in this essay I try once more, focusing as tightly as I can on the core insights.

These swim against the dominant tendency which is evident in AI of emulating the capabilities of human beings, hoping that once reached they will be readily surpassed.
They do so under the inspiration of ideas about how reasoning and the knowledge which can be derived from it can be made both more reliable and more broadly applicable.
The grounds for the belief that this is possible do not come from observation of the ways in which human beings commonly reason, but rather from advances in mathematics, theoretical computer science and philosophy which have transformed our understanding of deductive reasoning and its limits within the last couple of centuries (after a couple of millenia in which progress faltered).

Though these new logical methods were theoretically and philosophically fruitful, and hinged around the invention of potent strictly formal languages and deductive systems, the application of these formal systems (rather than their use in the development of new theoretical disciplines) was severely limited by the complexity of the detailed proofs which they required.
The use of these systems in real world applications, or even in the development of mathematical theories, was impractical until the advent of the digital stored program computer.
Even with this assistance, in the form of brute computational capability, the support remained short of what was needed to realise the full potential.

This became apparent as GOFAI came up against the problem of `combinatorial explosion' and came to understand that intelligent heuristics were essential to success in finding deductive proofs of non-trivial propositions.

\section{Introduction}

When stored program digital computers were first invented their applications primarily concerned doing large amounts of information processing or computation with almost perfect reliability and at superhuman speeds.
They were accurate and reliable.

As their computational power grew their applications were extended progressively, and this sometimes involved attempts to achieve ends which were much less clearly defined, involving more complex instructions which could less certainly be relied upon to achieve the intended purpose.

The kinds of brute computational power exhibited by these early computers might at first have been thought signs of intelligence, since human skill in computation had certainly been presumed a sign of intelligence.
But brute computational power soon came to be distinguished from intelligence.

As I write, generative AI and Large Language Models have momentarily set a new standard for the unreliability of Artificial Intelligence.
Not designed or trained to be reliable repositories if knowledge, or to be capable of any but the most eleentary reasoning, their exposure to vast quantities of human knowledge enables them to perform in many main-line subject matters in an apparently authoritative way, while morphing in more esoteric areas into fantasy, and failing under the most modest interrogation to demonstrate any but the most shallow comprehension in subjects whose generally accepted facts they can replay.

It may not be so hard to improve on this.
LLMs have proven capable of using tools effectively, and tools such as more reliable ways of saving accurately and reliably knowledge acquired, or reasoning about that knowledge may not be difficult to supply.
The discussion in this essay may be thought of as concerning the use by such an AI with a tool which is capable both of storing knowledge and of deductive reasoning in the context of that knowledge.
The effect alleged would be to enable Artificial Intelligence which is \emph{oracular} in relation to logical truth.

Oracles may be thought of as having great wisdom, possibly derived from divine connection, but here I use the term more narrowly.
For the purposes of this essay the term ``oracle'' is used for something which is always truthful in answering questions, but doesn't always answer.

The oracle of interest here can be asked whether a sentence in a formal language is a \emph{logical truth} a concept which I will try to characterise, but which ultimately cannot be made absolutely definite.

The term ``Logical Truth'' is philosophically controversial.
In my usage of that term I stand on a limb, for my use is very similar to that of Rudolf Carnap,and is synonymous with the term \emph{analytic}, a concept central to Quine's repudiation of the philosophy of Carnap in the mid 20th Century.

Its not my purpose here to argue about the terminology.
Some might insist that my conception of logical truth should more properly be spoken of as set theoretic truth, and I do not intend to argue against that opinion.
Carnap, who until 1952 used the terms ``logical truth'' and ``analytic truth'' synonymously %
\footnote{as is explicit in section 2 of ``Meaning and Necessity'' \cite{carnap56}}%
, eventually accepted defeat and began to use the term ``logical truth'' for a narrower concept%
\footnote{W.V. Quine's noted a defect in Carnap's definition of analyticity in \cite{carnap56}, which followed closely a defect first seen in Wittgenstein's ``Tractatus Logico-philosophicus''\cite{Wittgenstein1921}.
Carnap's response appeared in the paper ``Meaning Postulates''\cite{carnap52} in which for the first time he separates the concept of logical truth from that of analyticity.}.

The term ``Oracular AI'' as used here, refers to what AI might in principle be able to achieve if furnished with an oracle for logical truth.

One of the purposes of this essay is to discuss how thus notion of logical truth can be made precise, to consider the difficulties in implementing such a decision procedure and to talk about the value of approximations which fall short of logical omniscience.

\section{Some Historical Background}

The stark difference between the reliability of deduction in mathematics and ways of discovering truth in other domains has been plain since the ancient Greeks began the transformation of mathematics into a theoretical science.
Axiomatic geometry.
The results thus obtained, particularly in the axiomatic development of geometry, were reliable and were to be gradually accumulated and were ultimately gathered together as the Elements of Euclid.

By contrast, those same ancient Greeks, when attempting to reason about nature and the cosmos were unable to establish durable findings, and would find many ways in which deductive reasoning, via reductio, could establish absurd and contradictory conclusions.

The differential success of deductive reason in these distinct domains was to be reflected in the two worlds of Plato's philosophy, of which only that of Platonic ideals was susceptible to truw knowledge, to be reached by reason alone.
Aristotle sought to rescue the applicability of deduction to what we would now call empirical science through his conception of \emph{demonstrative science}, which relied for itscoherence on the special truth assuring characteristics of the fundamental principles of each science.

Millenia followed in which this situation remained largely stable.
The modern conception of science originating in the scientific revolution toward the end of the renaissance resulted in philosophy being split into two camps associated with a primary emphasis on reason and observation respectively as the source of knowledge.
Empiricism retained the idea of reasoning from scientific principles, but insisted on observation and empirical experiment for the discovery and verification of the principles.


\chapter{Oracular AI (1)}

\section*{Preface}

\section{Introduction}

Alongside the enthusiasm for the largest of the Large Language Models initiated by the release of \emph{chatGPT}, the ways in which these models fall short of the ideals of General Artificial Intelligence have come into focus, and the discussion of the associated risks has been more intense, if not more informative.

LLMs were not intended to solve the problem of AGI, the are ``generative AI'', not designed as accurate or reliable stores of knowledge, or to have competence in reasoning or mathematics.
The following discussion is about what might be achieved if the capabilities of LLM's adapted to address some of desiderata around reliable retention of knowledge and reasoning about that knowledge.

The term ``Oracular'' AI is intended to suggest a kind of AI which can be completely relied upon to speak the truth, and which has a completely reliable deductive capability which exhibits that same kind of scale relative to the ability of homo sapiens as a supercomputers ability to do arithmetic calculations exceeds that of humans.

\subsection{Contextual Speculation}

The following speculations are not the point of this essay, but may help to make intelligible the ideas which follow.

Public speculation about how LLM technology might progress has include the following two very general approaches:

\begin{itemize}
\item Giving Large Language Models (LLMs) direct access to tools.
\item Integrating LLMs with other approaches to the development of AGI
\end{itemize}

Most of the intelligent beings which now exist are slow and unreliable by comparison with algorithms running on digital computers in very many kinds of problem.
There is no reason to expect that Artificial Intelligence will change that situation.
Sometimes there is an efficient algorithm and that is the best way to solve the problem.
Once an ``AI'' has access to such tools, then it will use them in preference to its own machinations.

Most of the great accomplishment, perhaps all, of the greatest intellects in history are but one stage in the development of knowledge and skills by an entire culture over long periods of time.
Large engineering enterprises involve very many intelligent designers collaborating to create and refine clear statements of requirements, design, coding and details of the physical and logical structures necessary to meet the requirement.
Most of these contributors will have specialised in some aspect of this process and could not effectively contribute in areas which are too far removed from their speciality.

Nevertheless, AI marches forward toward a goal of complete generality.
That generality might be thought of as being capable of specialising to any area given appropriate training, but even that would exceed the kinds of intelligence which homo sapiens exhibits, for our gene pool is diverse and different individuals have the aptitude to perform well in distinct domains.

The direction of movement which we perceive in some of the most complex feats of engineering design is toward better support for collaboration between even larger distributed teams.
The most prominent manufacture of digital hardware intended to support AI, Nvidia, provides an environment called ``the Omniverse'' in which a complete factory can be designed and simulated, making the most of AI, and through which a widely distributed development team can work together to advance the design.
In such an environment the behaviour of each part of the manufacturing system can be simulated, and the simulated environment can be used to train the software for robots intended to participate in the running of the factory.

As AI becomes capable of contributing to such projects it may at first be directly used by human designers to help them accomplish their part of the work, but later we may expect AI agent to act on a par with human beings, seeking help as necessary, subject to review by the normal processes.

Alongside the Omniverse, Nvidia promotes the idea of ``digital twins'', which are abstract computational models of physical systems.
These models may serve various roles throughout the life cycle of the systems, appearing at first before the construction of the physical counterpart, enabling comprehensive testing verification and mutli-faceted evaluation prior to manufacture, and subsequently tracking the state of their physical instances to facilitate control and maintenance.

\subsection{A Safe Zone}

One aspect of making AI safe is simply making it more reliable in distinguishing truth from falsity.
This does nothing to prevent malfeasance, but a large proportion of pre-AI safety concern addressed the prevention of accidental rather than deliberate harms.
A safe car is not one which will refuse to mow down pedestrians, it is one that will not do so unless its driver intends it, or is incompetent or culpably negligent.

Even this kind of safety is hard to guarantee, and potentially becomes harder as AI permeates the workings of the world.

It may therefore be comforting to know of important domains in which conclusively establishing the truth is feasible (and customary) and in which even greater assurance of truth can be realised with the help of computers and in the context of artificial intelligence.

This is the domain of deductive inference, and logical truth.
It is the purpose of this essay to mention some of the well known ways in which these truths can conclusively be established and to add some ideas which may be less widely appreciated.

If deductive inference were to be automated so well that very lengthy deductions could be reliably conducted and subsequently depended upon, then the context in which those deductions are undertaken is important.
First it is necessary for the meanings of the language (or languages) involved must be clearly and precisely understood, for reasoning and comprehension is otherwise compromised.
Second, it is necessary that the context in which deduction is undertaken is coherent, for otherwise a contradiction may be proven and from that contradiction any conjecture, true or false, will be derivable and proof will confer no confidence.

The verification of engineering design is an important potential application for AI powered deductive inference.
But the reasoning involved must make use of a large body of mathematical and scientific knowledge, it must take place in the context of 
mathematical models of the designed artefact and the context in which it is required to operate.


\subsection{Some Observations on Intelligence}

We know well enough how intelligent beings go about extending our knowledge of the world and applying that knowledge to various practical ends.

Two pervasive features are:
\begin{itemize}
\item The gathering of knowledge is cumulative and cooperative.
\item The knowledge is a shared resource mostly held in media external to the intelligent agents involved.
\end{itemize}

I suggest that this is unlikely to change with the advent of AGI.
So it it likely that the AGIs will be cooperating with other intelligent agents in the continued accumulation of knowledge stored in shared distributed media a large part of which is freely accessible to all.

These are some features of intelligence which I expect to persist as AGI progresses.

Nevertheless there will be important and substantial changes.
In the past, it has been fairly rare for a new area to be automated by computer without significant changes in how it is done.
A simple example of this is the very different expectations we have of the reliability and scale with which arithmetic computation can be performed one digital computers were available.
In many more cases, it is not just the greater reliability and speed which matters, something which was hitherto infeasible may be rendered commonplace.

Another area in which computational machinery has outstripped prior capabilities is in memory.
Very large quantities of information can now be reliably stored, perfectly retrieved, swiftly searched and processed in some way.

In these terms, the current crop of LLM's are a regression.
They were not intended to perform well in any of those ways, they were intended to process languages, and having exhibitied surprising ``emergent capabilities'' are now sometimes judged as if they were intended as AGI.

Much of the effort invested in AI research aims to replicate the kinds of intellgence exhibited by humans, rather than aiming at superior performance in areas of practical importance.
Though early in the history of AI research the automation of formal logical theorem proving was not only an important domain of AI research, but was considered by some to be the route to a general intelligence, it later was outflanked by simulating the brain as neural nets and common sense reasoning became the aspiration.

In the following speculations I am concerned with how to enable GAI which is as reliable as possible in all domains, approaching this by focussing on the kind of reasoning which we can reasonably expect to be made wholly reliable.
This includes reasoning in mathematics, science and engineering.

In these domains, deductive reasoning has a very important role.
It may be said to be almost the whole of pure mathematics.
We may also speak of the nomologico-deductive model of science, in which the evauation of scientific hyotheses or the application of accepted theories is undertaken by deduction from those theories and the particulars of the case in hand.
Engineering is then merely a collection of application of science in which the process of verification of a design against some statement of desiderata is one of logical deduction.


My aim here is to sketch a tool for use by LLMs and other AI as a way of guaranteeing that the kinds of deductive reasoning which they might effectively deploy can be guaranteed sound.
The importance of this cannot be underestimated, due to the difficulty in containing infidelity.
Once a falsehood has been proven a contradiction is not far away, and armed with a contradiction all proofs whether of truths or of falsehoods become very short and completely worthless.

My description of the proposed logical tool will come in several stages, growing more complex as additional desiderata are introduced and address.

In the first instance the tool could simply be the logical kernel of en existing Interactive Theorem Prover, which for reasons which I hope to make clear should I suggest support formal deduction in the variant of the HOL logic devised at the University of Cambridge, originally for reasoning about digital hardware.


\subsection{Alternate Paradigms for System Development}

The approach to systems development exemplified by digital twinning and the Omniverse centres around computational simulation, accelerated and enhanced by AI.

This is an effective approach which we may expect to be progressed and advanced.
It retains the limitation which has always attended verification by testing, notwithstanding the scope for testing coverage improvement by AI.

It a very sophisticated for of informed guesswork.

The best way to ensure compliance of a design with a requirement is by logical analysis, which has been a distant ideal for decades.

\subsection{Language, Knowledge and Models}

Mathematics, science and engineering design and implementation involve a large body of knowledge presented in many distinct languages as well as the statements of requirements and descriptions of design and implementations, construction of one or more formal or abstract models of the

\section{What is an Oracle?}

The word has a variety of usage, so best to be clear about what is intended here.
In ancient Greece it was a mouthpiece for deity, and therefore had not only wisdom but any other characteristic one might attribute to someone with a direct line to God.

Skip, doubtless, many intervening subtleties, and arrive in the twentieth century not so long after mathematicians got into logic (a century is nothing in the scale of these things) and we find that the notion of oracle gets a place in recursion theory, i.e. the theory of effective computability.

In that theory, after the concept itself was precisely and convincingly defined, the next step (for which purpose the concept had been introduced) ws the demonstration that not all numerical functions were effectively computable.

This problem was addressed through the idea of a decision procedure for a set of natural numbers, and it was shown that some sets were not effectively decidable, and hence that their decision problem was ``unsolvable''.
It further transpired not all unsolvable sets were equally unsolvable, but that they could be classified according to their degree of unsolvability.
These degrees were defined through the concept of reduction, in which the decision problem for a set A is reducible to that of a set B if, given an oracle for set A the decision problem for B could be soled.
The notion of oracle for a set A is quite simple here, it is simply something which will tell you of any number whether or not it is in the set A.

There are no vague notions of wisdom in play here, calling something an Oracle for B just tells you that it has the answers to a particaular set of questions.
The idea of Oracular AI discussed in this paper is primarily that of an AI, which is, over some well defined class of problems, able to give the correct answer.

I have to make one retrenchment to that, in acknowledgement of the incompleteness result first discovered and proven by Kurt Godel.
There is also a related issue concerning semantics, which I suppose we might think connects back to Tarski, that the set which I will seek to characterise here as the domain in which Oracular AI might be an Oracle cannot be precisely defined.

With those two caveats the aim of this essay is to argue that corresponding to Hume's ``fork'' there is a domain within which we can reasonably expect AI's to be Oracle's.

\chapter{Formal Abstract Modelling for General Artificial Intelligence}

\section{Preface}

``General Artificial Intelligence'', as is now termed since the ``Artificial Intelligence'' lost its cachet, ought we hope, if not at first, to surpass humanity in the reliability of its memory and the certainty of its reasoning.
The latest wave of excellence fails miserably on both.

Large Language Models (LLMs) have certainly cracked language, which is, as some modern philosophers have pointed out, complicit in the incoherence of common discourse.
Can we now assume that when natural language discourse is required, an LLM technology will supply it, and seek deductive proficiency through simpler more precise notations, or relegate concrete syntax to intermediaries and approach deductive closure through exclusively abstract structures?

That is the possibility which this essay explores.

Despite their present ascendency, Large Language Models have not been the star achievers in recent AI R\&D.
The mantra that proficiency in AI is crucially dependent on the use of very large bodies of data for training neural nets has been disregarded in several of the most impressive achievements of Deep Mind the London AI startup now acquired by Google's parent Alphabet.

First with Chess, then Go and latterly alphafold, Deep Mind showed that in in simple worlds governed by a modest collection of unambiguous rules, neural nets relegated to heuristics could achieve proficiency as the system learns by playing itself, without needing role models to mimic.
Though their complexity is unbounded, the abstract structures of mathematics and their use in the more elaborate mathematical models created by science and the vastly more complex constructions of engineering (especially in digital semiconductors), are susceptible to the same methods, which place Monte-Carlo tree search in pole position with crucial subordinate roles for neural nets (which need no linguistic competence).

Thinking of deductive inference about abstract models as being a precise and simple game, it is merely a speculation on my part that it will be susceptible to similar methods as chess, go and protein folding, and the realisation of that aspiration is not something to which I can contribute, except by the attempt which I offer here to clarify the nature of the game.

There are many different ways in which a broadly universal game of deductive modelling can be framed, and what I offer here is just one of them.

I have been trying to tell this story for a long time, without success.
I don't know whether the story can be told without talking about the history of the ideas in which it appears, and, not being a historian I am likely to be scornful of my attempts to do that.
But i have attempted to tell the story without going into the history, and I cannot see how it can be understood.

The fact is that the ideas which I want to present under this heading are convincing to me only because of the history of ideas which has lead me to them, though my own particular perspective on them us substantially influenced by some of the most recent developments in them (by historical standards, not by the hectic standards of AI research) and by my own professional experience in trying to apply these ideas in the development of highly assured information processing systems in relation to security and safety concerns.

This essay is an attempted compromise, in which I try again to present the ideas in the context of a minimal presentation of those histotical precedent which seem most crucial in illuminating and motivating the approach which I am suggesting.

\section{Introduction}


Nearly half a century ago John McCarthy published a paper \cite{mccarthy1981} that divided the problem of artificial intelligence into two components or domains: an epistemological component, which concerned the representation of knowledge and the rules of reasoning, and a heuristic component that optimised search and pattern matching to permit the solution of  otherwise intractable problems.

In this essay I discuss a further division of the epistemological side of McCarthy's division.

McCarthy's ``epistemological part of AI'' can, I claim, profitably be advanced by separating our that part which concerns purely deductive reasoning.
In the established epistemological terminology, this is the problem of the \emph{a priori}.
The contention is that all deductive reasoning, even in relation to knowledge \emph{a posteriori} can be undertaken and validated in an abstract model with the standards of soundness and consistency which are realisable in that domain, and that an approach which separates out  abstract modelling as a fundamental shared component of all empirically applied AI is not only theoretically elegant but practically effective.

The separation of the whole of this domain yields a general facility for \emph{abstract modelling}, in which any aspect of empirical concern can be addressed by the use of an abstract model.
This is a particular way of thinking about and implementing the use of formalised mathematics and logic for modelling the phenomena in the real world, and thus a critical element of the problem of knowledge representation, and a means to very high levels of rigour and generality in the automation of deductive reasoning.

\section{A First Sketch of The Foundation}

The formalisation and mechanisation of logic and thence of mathematics was anticipated by Aristotle and Leibniz, but only became plausible after Frege devised his \emph{concept script} and attempted to demonstrate the reduction of mathematics to logic.
His own system was shown to be flawed by ``Russell's paradox'', but Russell then went on to devise and apply a type theory for that same purpose.

Frege's mantra was that:

\begin{center}
Mathematics = Logic + Definitions
\end{center}

Which has fallen foul of much philosophical disagreement about exactly logic is, but it was generally accepted that \emph{in practice}:

\begin{center}
Mathematics = Set-theory + Definitions
\end{center}

Where \emph{in practice} means that almost all of mathematics can be derived in the axiomatisation of set theory known as ZFC provided that definitions of the concepts of mathematics in terms of sets are accepted, and that such definitions can be supplied which render the resulting formal theory strictly the same as accepted but less formally conducted mathematics.
We have to say \emph{almost} all here because of the well known incompleteness results of G\"{o}del, but I have been unable to locate any incompleteness which is relevant to any but the most esoteric studies in logic or mathematics.

Henceforth any logical system (and there are many of them) which is capable of formalising logic and mathematics in a similar way I call a \emph{logical foundation system} (often eliding the `logical'), and I note here the importance of the possibility of proceeding in such a foundation system exclusively by the use of definitions of concepts rather than by the introduction of additional axioms or rules of inference as crucial to the integrity of the system, ensuring as it does, that if the foundation system itself is logically consistent the so will be the extensions for modelling purposes undertakem exclusively by definition.\footnote{
Though later we will admit a useful generalisation of the concept of definition as that of \emph{conservative extension}.}

Russell's \emph{Theory of Types} was a complex logical system due to ramifications thought necessary in the type system.
It was not long before Ramsey suggested its simplification into the \emph{Simple Theory of Types}%
\footnote{First published as a fully worked out formal system in 1928 by Hilbert and Ackermann "\cite{hilbert1928}.}%
, arguing that the ramifications were not in fact necessary.

In the 1930's logicians found it necessary to clarify the concept of \emph{effective method}, and in 1936 four simple but general ways of describing computational methods (algorithms) were published.
These were Stephen Kleene's ``recursive functions'', Alan Turings's machines, Post's production systems, and Church's lambda calculus.
\footnote{The relevant papers may all be found in ``The Undecidable'' \cite{davis65}}
These originally devised to demonstrate the existence of unsolvable problems, but Church's lambda calculus was to prove influential in the design of computer programming languages and as the basis of foundations for mathematics which through their simplicity and power would be embraced by those computer scientists and engineers who were interested in reasoning about computer programs and digital hardware.

Closely related to the lambda calculus, in which functions are defined using a variable to represent the argument, and an expression involving that variable for the value of the function at that point, an even simpler but similarly expressive notation was devised which made no use of these ``bound variables'', and there was much research in how these systems might form the basis for mathematical foundation systems.

A major figure in this research was Haskell Curry, whose names were to be enshrined in the terminolgy of functional programming languages (which are closely related to the lambda-calculus), but his work on mathematical foundation systems based on combinators was to prove less influential.
For our story it is the ideas of Church which are more significant, and after proposing a foundation system based on the simple (type-free) lambda-calculus which proved to be logically inconsistent, he added to the lambda-calculus a type system similar to that of the Simple Theory of Types resulting in a variant of that system which would prove influential in computer science.

Church's formulation of the Simple Theory of Types \cite{churchSTT} was to be taken up in research into the formal verification of computer programs and digital hardware.
The resulting elaboration of Church's system was the product of a line of development which began with a logical system devised by Dana Scott for reasoning about programs and programming languages
\footnote{First circulated in 1969 but not published until much later as \cite{scott1993type}}.
This became the logical basis for a line of software systems for computer assisted proof construction and checking intended for reasoning about programming languages and programmes.
The first system was called LCF for ``Logic for Computable Functions'' and after several steps, including migration from Stanford to Edinburgh and then Cambridge (UK) was re-oriented toward hardware verification and adapted to support a variant of Church's STT by Mike Gordon.%
\footnote{For a fuller account of this story see Gordon\cite{gordon2000lcf,plotkin2000proof}}
This logic and the software developed to support reasoning in the logic were both called HOL, or Cambridge HOL if the context left room for doubt.

As far as the Cambridge HOL logic was concerned the principle new feature was its inheritance from LCF of a polymorphic version of the type system.
In Church's exposition of his Simple Type Theory variables appear in the types, but these are syntactic variables belonging to the metatheory, not variables in the object language (STT).

The logical system put together from these two principle lines of influence, after some refinements settled with the following characteristics:

\begin{itemize}
\item From Church's STT:
  \begin{itemize}
  \item The abstract syntax, semantics and deductive system of STT is the base to which the following enhancements are applied.
  \end{itemize}
\item from Edinburgh LCF:
  \begin{itemize}
  \item Hindley-Milner polymorphism:
    
    The practical consequences of the strict type system in STT become onerous once the system is used beyond the confines of arithmetic, for the theorems governing the operation of structures like sequences would need to be proven again for every new kind of object from which lists need to be constructed.  Including type variables in the object language allows for a single theory of lists which can be instantiated to any type for which lists are needed.
  \end{itemize}
\item New in Cambridge HOL
  \begin{itemize}
  \item Definition of Type Constructors:
    
    In the development of mathematics, for example in the sequence of number systems leading up to (and beyond) the complex reals, each new kind of number is constructed from earlier kinds of number and has a new type.  The type is chosen for convenience in defining operators with the required properties, but is not a part of the abstract structure intended.  The definition of a new type which delivers the required structure while concealing the manner in which that result was obtained, is valuable in managing the complexity and intelligibility of layer upon layer of required abstractions.
  \item Definition of Constants:
    
    In many logical system \emph{definitions} are syntactic, permitting a concise expression to stand in stead of something more complex or extended.
    In Cambridge definitions extend the underlying logical system by adding new constant symbols into the language together with an axiom which constrains the interpretations of the constant.
    Simple definition in this form are \emph{conservative}, permitting no new theorems to be derived which do not mention the new constant, hence preserving the consistency of the logical system.
    In Cambridge HOL, for the sake of improved abstraction (or the avoidance of over-specification), the mechanisms which permit new constants to be introduced allow weak constraints.
  \item Sequent Calculus:

    The deduction system is reorganised as a sequent calculus rather than the more traditional Hilbert style inference system used in Church's STT, and for various other pragmatic reasons is equivalent to but distinct from the details of that system.

  \item Theory Hierarchy:

    It is essential for the tool supporting the logical system to keep a record of the definitions which have been entered, to ensure that consistency is not compromised by introducing the same constant more than once with conflicting definitions.
    In the implementation of Cambridge HOL this is done through a heirarchy of theorems which keep a record of the constant definitions which have a occurred in a way which controls the scope of the defintions and permits many different developments of logical theories to be saved in the same logical database.
    This may also be used to save theorems proven in the relevant context.
  \end{itemize}
\end{itemize}

There are many more original aspects of the HOL system which enabled the definition of constants and the construction and checking of formal proofs in this logical system, but these do not concern us here.
It is the abstract language, its semantics and its inference system which are here advocating for more general application, and not any particular software to support those applications.

The principle merits of this system so far as the role here proposed for it are as follows:

\begin{itemize}
\item It is a ``mathematical foundation system''
  \begin{itemize}
  \item It replicates Frege's prescription for the logical structure of mathematics as obtained by logic from the definitions of the mathematical concepts,  and the suitability for the formalisation of mathematics exhibited first by Russell's Theory of Types, in a neater and more practical way.
  \item It permits the derivation of mathematics and of applications of mathematics by conservative extension, subject only to a suitable axiom ensuring that the ontology is sufficiently rich.
    In Church this is simply an axiom of infinity asserting that there are infinitely many individuals, supporting the development of arithmetic and analysis, which for some of the more exotiv mathematical theories (e.g. parts of Set Theory) might not suffice, but which can readily be accommodated by the use of a stronger axiom of infinity, possibly one asserting that the cardinality of the individuals is inaccessible.
    \item only the abstract syntax and semantics of the system is adopted, and this abstract syntax because of its simple and higher-orer structure is ideal to replicate the abstract syntax of arbitrary language using constructors whose definition captures the semantics of the language, hence 
    \end{itemize}
\end{itemize}

\cite{gordon1989mechanizing,birtwistle2012current}

\cite{cohnPIHV}
\cite{mccarthy2022artificial}

\section{A History of Modelling}

\subsection{What is a Model?}

The aim of this document is to talk about formal abstract models, since it seems possible that they will be the next important development in the evolution of models and their use.

In order to give a sense of the importance of such models I aim to sketch the history behind them.
This might involve a bit of stretching of the term, but it already serves a great variety of purposes so I'm sure it will cope.

It may be useful to characterise the sort of model which are of interest here, which are those which consist in something representing information about some other thing of interest.
This encompasses linguistic descriptions as well as information strucures which long pre-date the evolution of language.

Since the beginning of life on earth, even the most simple forms of life have found it necessary to sense their environment, and to adapt their behaviour according to what those senses reveal.
In the simplest cases, the senses will serve to detect potential foods and suspected predators, in order to secure the former and avoid the latter.

This involves some kind of representation of the environment being created by the sense organs, to be transformed into that kind of representation of necessary effect which will secure the best interests of the organism.
In those organisms which possess a nervous system the representation may be as patterns of firing in neurons, and the transformation will be undertaken by a neural net before some other pattern of firings initiates appropriate motor functions.
In yet more primitive organisms, the necessary presentations and processes are likely to be chemical, and the representations less likely to be thought of as models even though serving similar purposes.

\subsection{}

My aim here is to describe a game, which I believe to be susceptible to automation, but which because of its algorithmic complexity depends upon the use of heuristics best supplied by intensively trained neural nets.

The game is the prediction of real world events by the use deduction in the context of abstract models of the relevant real world phenomena.
It also includes the elaboration of the pre-requisite logical and mathematical theories which provide a basis for the construction of the relevant scientific and engineering models.
The whole provides potentially a substrate for the automation of engineering design.

A bare presentation of the game as proposed would I expect be unconvincing and opaque, so I propose to come to the details via some discussion of the history of abstract modelling, or of those kinds of process which have lead the particular conception of abstract modelleing which is presented here.

This leads to a minimal presentation, followed by the consideration of a certain number of desirable elaborations.
Throughout, an understanding will be supported by the presentation of certain philosophical positions, which are offered primarily as useful in supporting this kind of enterprise (i.e. on a pragmatic basis) not as demonstrably superior to the many alternatives.

\section{Some History of Modelling}

\subsection{Pre-History}

Throughout the evolution of life on earth it has been necessary for living organisms to take cognisance of their surroundings in order to take those actions which will secure nourishment, avoid predators, and effect reproduction.
When an organism takes in information about its surroundings and uses it as a basis for its actions in the near future, it is not wholly unreasonable to consider that it has thereby acquired some sort of useful model of its environment, and that is will use that model to predict which courses of action will best serve its need to grow and replicate.
For organisms with a nervoud system we might call these neural models, for single celled organisms perhaps chemical ``models'' (stretching the point beyond its limits perhaps).

As the nervous systems of organisms evolve to greater complexity we may imagine that the complexity of the models grows with that of the nervous systems.

Though these models serve a vital purpose for the organisms, we cannot reasonably say at this point that the models are used deductively.

Deduction can only be considered in the context of \emph{propositional language}, which is that kind of language in which claims can be expressed which will be either true or false according to how things are, and whose assertion may thereby communicate knowledge (about how things are) from one to another.
Such language is probably coeval with homo sapiens, who appears at the end of a million years in which the size of the brain grew quite rapidly.
Any faculty which demands special mental faculties must evolve alongside both the physical and mental adaptations necessary to support it.
Those regions of the brain which are adapted to support language could not have appeared without the languages faculties they support.

With propositional language comes deduction, for even the most elementary language must embrace taxonomic classifications.
If your language has a word for horse and one for animal, then you cannot be said to understand the language unless you know that horses are a kind of animal and are able to deduce from the knowledge that something is a horse the ``conclusion'' that it is also an animal.
Admittedly this kind of deduction is neither formal nor self-conscious.

The advent of language enabled oral culture which would ultimately empower mankind in the transformation of the world.
The evolution of culture must at first have been quite slow, since the first evidence of animal husbandry and then agriculture date back only 13,000 years, 200-300 thousand years after the evolution of homo sapiens.

The first stage in accelerating cultural evolution was the invention of the written word, which, in the form of clay tablets, was probably of modest impact, but the technology of writing and communication has itself evolved and many important cultural advances have been facilitated by new media for storing, and sharing knowledge.

Papyrus was invented by the Egyptians around 3000 BC, and was a medium for the preservation of Egyptian culture, including geometry and arithmetic.
It does not appear to have been adopted in ancient Greece until about the 8th century BC, but then catalysed the flourishing of Greek culture which then shaped the development of the Western and ultimately the whole world.
This began with literary works such as the Odyssey, but of greater interest to us here is the transformation of Mathematics which began around 600BC.

\subsection{Greek Mathematics and Logic}

The period between 600 and 300BC was one in which major advances were made in sophistication with which reason could be applied to modelling the world.

It is this period that mathematics is first systematised as a deductive science, and in which we first see deduction talked and theorised about.
The systematisation of Geometry now known as Euclidean Geometry set a new standard of rigour allowing more elaborate deductive reasoning to be undertaken with great reliability.
This standard was not to be improved upon for the next two millenia, and was to be the envy many philosophers who wished for the emprimateur of logical certainty to be attached to their conclusions.

\subsection{The Organon and Demonstrative Science}

It is in Aristotle's ``Organon'' \cite{aristotleL325,aristotleL391} that the applicability of logic to the understanding of reality was first to be made explicit, and it was in Aristotle that we see the first attempts at formally codifying the rules of sound deductive reasoning.

In his Organon, six books forming his contribution to logic, Aristotle presents the idea of Demonstrative Science in which necessary conlcusions about the world are deductively derived from the fundamental principles of each science.
In these volumes the nature of deductive reason is analysed using the idea of the syllogism.
This is the first known metatheory concerning deduction, and it was to remain dominant until the end of the 19th century.

\subsection{Bacon's Novum Organum and Empiricist Science}

The next major development in the methodology of science, the ways in which we build models of the world, came in the Renaissance and is presented in writings of Francis Bacon, notably his \emph{Novum Organum} \cite{bacon2017novum}.

Whereas Aristotle sought to adapt the axiomatic methods of mathematics to Science, looking in greater depth at the process of deriving conclusions from the principles than that of establishing the principles, Francis Bacon was more concerned with how the principles could be established, and in this the differences between mathematics and science wer more marked.

He described a four stage process as follows:

\begin{itemize}
\item Collect data about the phenomema of interest.
\item Look for patterns in the data.
\item Formulate a hypothesis.
\item Test the hypothesis by observation and experiment.
\end{itemize}

This is the new nomologico-deductive method, in which the novelty is primarily in the establishment of the laws rather than their application (though testing hypotheses is similar to application, as involving deductive inference from the hypotheses).

Almost contemporary with Bacon, and with more substantial credentials as a scientist and engineer (as well as a philosopher), Galileo Galilei (1564–1642) spent most of his life trying to give structure to the new science, in which Aristotelian science was largely cast aside.
Gone was the teleological emphasis on ``final cause'', and replace the five fold ontology of ether, fire, air, water and earth with a system built exclusively on matter, introducing a mechanical system which came to dominate modern science.

Modern philosophy from this point divides into two camps, empiricist like Bacon who emphasised observation and experiment as the source of worldly knowledge, and those who sought knowledge through reason.

\subsection{Leibniz and The Mechanisation of Science}

\subsection{Hume's Forks and The Synthetic A Priori}

\chapter{Evolution and Logic}

% Evolution is the mechanism which turns chaos into order,
% not so much by design as by the accident that chance will eventually create complex structures which thrive.
% It has many forms, some very different to what we call Darwinian evolution, and by consideration of how those forms of evolution have themselves evolved we can speculate about the future of evolution and of intelligence.
%My own speculations on this topic say something about epistemology and logic.

\section*{Preface}
\phantomsection

\addcontentsline{toc}{section}{Preface}

This preface is really immaterial, but there might be some point in saying \emph{why} its immaterial.
Here's Bertrand Russell more than a century ago on a theme that remains current.

\begin{quote}
  \emph{That Man is the product of causes which had no prevision of the end they were achieving; that his origin, his growth, his hopes and fears, his loves and his beliefs, are but the outcome of accidental collocations of atoms; that no fire, no heroism, no intensity of thought and feeling, can preserve an individual life beyond the grave; that all the labours of the ages, all the devotion, all the inspiration, all the noonday brightness of human genius, are destined to extinction in the vast death of the solar system, and that the whole temple of Man's achievement must inevitably be buried beneath the débris of a universe in ruins — all these things, if not quite beyond dispute, are yet so nearly certain, that no philosophy which rejects them can hope to stand.}

Bertrand Russell, Mysticism and Logic \cite{russell17}
\end{quote}

Much has changed, in the world and in philosophy, since Russell penned those words, but majority opinion is probably still with Russell's sentiment (if not his way of putting it), and its not even controversial, as he says (in other words) resistance is futile.

I was reminded of Russell's words just as I resolved on this essay, and the contrast between that point of view and the perspective intended for the essay seemed a good way to highlight that perspective.

Russell talks of the death of the solar system, and of a universe in ruins.
The former would be a consequence of the Sun progressing through the observed life cycle of similar stars, which I don't doubt.
It will not be happening soon, not even in evolutionary timescales, so it doesn't seem wholly irrational to hope that humanity or its progeny will already have ventured to havens beyond our solar system before it happens.
I have something to say about how that might happen.

The idea of a universe in ruins has no support which I know of other than the second law of thermodynamics.
I have no problem with the practical applications of thermodynamics, which as far as I am aware mainly talks about entropy changes in closed systems.

I have struggled to understand the concept in the context of the second law, and struggled to comprehend the evidence offered for the second law, and I have neither understood nor believed.
So this essay is written as a speculation about how evolution has and will continue to progress, by someone who has no expectation that this will ever come to an end (and is agnostic about whether it ever had a beginning).

I might add a more comprehensive scepticism about the plausibility about the science of the very greatest extremes.
I doubt that scientists will ever realise perfect models of the microscopic structure of the universe, or about the gross structure of the universe, or about what happens close to the supposed singularities in relativistic models.
However far we have been able to peer with our best instruments, there may yet be something beyond which defies our expectations.

As to the significance of these beliefs to this essay, it is small and mainly psychological.
The timescales in which the second law might be expected to yield heat death are beyond those in which my speculations are credible even to me.
So far as I know, if I did believe the second law of thermodynamics, I would still think it ``academic'' and write the same essay.
Nevertheless I reject Russell's perspective.
Russell has made important contributions in some of his other works to the ideas presented here, and has been for me and many others over the last century, an inspirational figure.
But on this I demur.

\section{Introduction}

There is at the centre of this essay a very simple idea which is not mine.
I believe its importance is much greater than has been recognised, and this essay is intended argue the case for that belief.
More important than the belief itself is its consequences for the way in which artificial intelligence (and synthetic ecology) is approached and progressed.

Having said that, the ideas in the essay are not intended to be a direct contribution to those fields, in which I have no expertise.

The central idea is very simple, and apart perhaps from the details of presentation and the conception of significance, is not original.
A naked presentation could not convey the importance which I attach to it, and this document is primarily intended to provide a backdrop against which it's importance can more clearly be perceived.

That backdrop is evolutionary.
It is a story of the many kinds of evolution which have brought us to this moment in time at this point in space, and a speculation about some of the evolutionary processes which will lead us forward and the product of those very different evolutions.

My own reading of certain aspects of evolution provides a basis for the speculations to follow about how evolution will itself evolve in the future and how that will shape the growing sphere of influence which homo sapiens has in the universe.
From those speculations, some conclusions are drawn about the representation of knowledge and the automation of reasoning which lead to a a sketch of how those requirements might best be satisfied which form the kernel of this essay.

\subsection{Evolutionary Themes}

It is evolution which has shaped life on earth.
But no single kind of evolution can explain it all, and the purpose of this essay is best served by a varied diet.

If we are to consider the future trajectory of evolution, it will be necessary to identify some of the characteristic which those different kinds of evolution have in common, as features most likely to be preserved into the future, as well as to consider those aspects of contemporary evolutionary processes which are most closely aligned to context which will not be preserved into our futures.

Homo sapiens is soon to become and interplanetary species, perhaps then interstellar.
This is such a profound transformation in context that it must surely have impact not only on what evolves, but on how evolution works, both in the minutiae of the reproductive processes, the ways in which variation occurs and the selective pressures which guide the process.

So, what is evolution?
Some kinds of evolutionary thinking go back as far as the ancient Greeks, but modern conceptions of evolution date back to Darwin, who offered evolution as an explanation of the origin of species.
As science has progressed the dominant conception of evolution has also advanced, the development most conspicuously punctuated by the ``modern synthesis'' of which the most important aspect, though by no means the whole, was the incorporation of Mendelian genetics.

For our purposes its best to start with a simple model, since we seek an idea of evolution which will have broad scope.

Darwinian evolution may be characterised in three points:

\begin{itemize}
\item It concerns the evolution of species, which are populations of interbreeding organisms, and it is the species which are said to evolve.
\item As the population of the species reproduce, the offspring share many of the characteristics of the parent or parents, but they are not identical.
  Variation is thereby introduced.
\item Different variations may impact on reproductive success.
  This also happens in selective breeding by farmers, but in nature no deliberate selection is involved, and the differential success in reproduction is attributed to ``natural selection''.
\end{itemize}

It is generally emphasised that the variation which occurs is random and that the selection process is ``natural''.
These emphases are necessary because the intention is to assert that evolution, even of the most complex organisms, can work without any intelligent intervention.
Neither in creating appropriate variations, nor in weeding out those variations which are not advantageous.

Notwithstanding that point, even though intelligent intervention may be inessential, it may still be reasonable to consider a progress evolutionary even if some intelligence is involved in selection (as is arguably the case in sexually reproducing species) or even in variation (which genetic engineering makes possible).
For the sake of talking about a broad swath of different evolutionary processes, I therefore offer a broader conception of evolution which encompasses Darwinian evolution.

One of the challenges which I address in this reformulation is to obtain a conception of evolution which includes the processes which took place in the ``primordial soup'' leading to the first living organisms, and also to include the continuation of evolution when we have artificial intelligence incorporated into possibly inorganic systems which are capable as a whole of self-reproduction, but capable of undertaking design modifications in the process to optimise effectiveness.
Such systems will still exhibit progress which reflects the differential success of the different design decisions, and we can expect the best design decisions to become dominant among the resulting populations.

The single shared feature which I can see here is change to a population arising from differential replication of its members, which process is unlikely to progress indefinitely unless the replication is not invariably perfect.

\section{How We Got Here}

\section{Some Places We Are Going}

\section{Knowledge Representation and Deductive Closure}


\chapter{Synthetic Epistemology for Oracular AI}

% A philosophical perspective on evolution, and an evolutionary approach to philosophy.

\section{Introduction}

Homo sapiens is, as yet, ignorant of any intelligence in the universe but his own.\footnote{With apologies to theists.}
Our concept of knowledge, our epistemology, and our understanding of adjacent subjects such as language and logic, is shaped by the history of our origins.
Many now believe that intelligent artefacts will soon be created, and to a great extent those involved in their development may conceive their task as that of, first mimicking, and then surpassing, human intelligence.
But intelligence, living or synthetic, might have come to us from another part of the universe, and might then have originated in quite different ways.
When we design intelligent artefacts, perhaps intended primarily for service in progressing the exploration of the solar system and the galaxy beyond, very many aspects of the environment which nurtured human intelligence will be irrelevant.

In early approaches to the design of intelligent artefacts, it may be advantageous to rethink the foundations of epistemology which inform our thinking about knowledge and will shape the architecture.
In doing so, we have a freedom which we may not feel when engaged in the philosophical analysis of what we humans call ``knowledge''.
It is the freedom to consider what purposes knowledge might serve and how it might best serve them, and to build, from its foundations a synthetic epistemology with those purposes in mind.

That is the enterprise which I am exploring in this essay.
Though that perspective on the project is fresh, it is an enterprise which has occupied my mind for many decades while I have sought a way forward.
I hope that the essay I write here will seem too simple to have gestated for a lifetime.

\section{Oracular AI}

In the headlong rush toward what we used to call Artificial Intelligence, but now call General Artificial Intelligence, various capabilities in which computers are greatly superior to human beings seem to have passed us by.

The central place which neural nets now take, in which structured knowledge representation is abjured in favour of the digital analogue of synaptic connections and weights results in intelligence which is better at \emph{knowing how} than \emph{knowing what}, i.e. procedural rather than declarative or propositional knowledge.

Procedural knowledge predates declarative knowledge in the evolution of humanity, and as culture began to evolve this was a limiting factor.
In default of declarative language, the transmission of knowledge can only take place between individuals in close proximity.
With verbal language the dissemination can at least encompass a group of people not quite so close by, and with written language knowledge can be transmitted across large distances and over substantial periods with greater reliability.
In addition, a written record permits the gathering or application of knowledge to become an extended collaborative enterprise.

The ability of human beings acting in concert to accumulate a shared body of declarative knowledge, gradually extending into science, technology, and engineering and extending the control we exert over our environment to the benefit of humanity is something we should expect an AI to contribute to.

As the scientific knowledge of humanity has progressed, the languages which we use for science become more refined, precise and expressive and our understanding of how to reason reliability and soundly in the application of that knowledge advances.






Oracular AI is that kind of Artificial Intelligence which is oriented 

In making a connection between AI and synthetic epistemology, I have been been motivated by more than one kind of foundational thinking.
Foundational thinking is for me the quintessence of philosophy, and the synthetic epistemology I discuss here may be thought of as at a polar extreme to the idea of `epistemology naturalised'.

This philosophical synthesis is to be built from a base which is neither human nor even living, but we must begin with some sense of purpose, in default of which no criterion for success could be mooted and no sense of direction would emerge.

I therefore begin thinking of with a diverse collection of intelligent agents which both collaborate and compete in seeking to replicate themselves or some larger system of which they are a part across the cosmos.
Their success in so doing is contingent on their ability to gather knowledge and use that knowledge to plan and optimise their continuing proliferation.



There have been many substantial changes in the dominant paradigms for Artificial Intelligence over the last three quarters of a century.
At one stage, which is sometimes now referred to as GOFAI%
\footnote{Good Old Fashioned AI}, %
epistemological concerns were more prominent than they now are.
Typical epistemological concerns from that period included the representation of propositional knowledge, the ways in which one can use deduction to infer new propositional knowledge from old, and how such deductive techniques could be used for solving a broad range of problems.

Though these methods were capable in principle of solving the broad range of problems which were then thought characteristic of intelligence, they ultimately failed once large problems were tackled due to the combinatorial intractability of the relevant search spaces, and it was recognised that intelligent levels of performance on these problems depended on the ability of intelligent people to noarrow down intuitively the search space sufficiently to have a reasonable success rate in finding a solution.

Alongside the impasse created by the complexity of search spaces, problems whose solution was essential for some some applications of AI, but which were perhaps not previously thought of as requiring ``intelligence'' began to come to tne fore.
And example was the problem not of understanding the work but simply \emph{seeing} the world and being able to identify its principle features, the problem of computer vision.
This was not a problem for deduction, the things we perceive in our surroundings are not deductively inferred from the detailed sensory inputs reaching the brain from the sense organs.
This is indeed an epistemological problem, but of an entirely different kind.


When stored program digital computers were first invented their applications primarily concerned doing large amounts of information processing or computation with almost perfect reliability and at superhuman speeds.
They were accurate and reliable.

As their computational power grew their applications were extended progressively, and this sometimes involved attempts to achieve ends which were much less clearly defined, involving more complex instructions which could less certainly be relied upon to achieve the intended purpose.

The kinds of brute computational power exhibited by these early computers might at first have been thought signs of intelligence, since human skill in computation had certainly been presumed a sign of intelligence.
But brute computational power soon came to be distinguished from intelligence.

As I write, generative AI and Large Language Models have momentarily set a new standard for the unreliability of Artificial Intelligence.
Not designed or trained to be reliable repositories of knowledge, or to be capable of any but the most eleentary reasoning, their exposure to vast quantities of human knowledge enables them to perform in many main-line subject matters in an apparently authoritative way, while morphing in more esoteric areas into fantasy, and failing under even gentle interrogation to demonstrate any but the most shallow comprehension in subjects whose generally accepted facts they can replay.

It may not be so hard to improve on this.
LLMs have proven capable of using tools effectively, and tools such as more reliable ways of saving accurately and reliably knowledge acquired, or reasoning about that knowledge may not be difficult to supply.
The discussion in this essay may be thought of as concerning the use by such an AI with a tool which is capable both of storing knowledge and of deductive reasoning in the context of that knowledge.
The effect alleged would be to enable Artificial Intelligence which is \emph{oracular} in relation to logical truth.

Oracles may be thought of as having great wisdom, possibly derived from divine connection, but here I use the term more narrowly.
For the purposes of this essay the term ``oracle'' is used for something which is always truthful in answering questions, but doesn't always answer.

The oracle of interest here can be asked whether a sentence in a formal language is a \emph{logical truth} a concept which I will try to characterise, but which ultimately cannot be made absolutely definite.

The term ``Logical Truth'' is philosophically controversial.
In my usage of that term I stand on a limb, for my use is very similar to that of Rudolf Carnap, and is synonymous with the term \emph{analytic}, a concept central to Quine's repudiation of the philosophy of Carnap in the mid 20th Century.

Its not my purpose here to argue about the terminology.
Some might insist that my conception of logical truth should more properly be spoken of as set theoretic truth, and I do not intend to argue against that opinion.
Carnap, who until 1952 used the terms ``logical truth'' and ``analytic truth'' synonymously %
\footnote{as is explicit in section 2 of ``Meaning and Necessity'' \cite{carnap56}}%
, eventually accepted defeat and began to use the term ``logical truth'' for a narrower concept%
\footnote{W.V. Quine's noted a defect in Carnap's definition of analyticity in \cite{carnap56}, which followed closely a defect first seen in Wittgenstein's ``Tractatus Logico-philosophicus''\cite{Wittgenstein1921}.
Carnap's response appeared in the paper ``Meaning Postulates''\cite{carnap52} in which for the first time he separates the concept of logical truth from that of analyticity.}.

The term ``Oracular AI'' as used here, refers to what AI might in principle be able to achieve if furnished with an oracle for logical truth.

One of the purposes of this essay is to discuss how thus notion of logical truth can be made precise, to consider the difficulties in implementing such a decision procedure and to talk about the value of approximations which fall short of logical omniscience.

\section{Synthetic Epistemology}

We are concerned here with what propositions can be expressed and how the truth of those propositions can be established.

The general context here is the use of models of reality to facilitate successful planning of actions which may lead to some desirable outcome, the acquisition of food, the avoidance of peril, the construction of a skyscraper, the design of a GPU, an expedition to Mars.

These kinds of knowledge are almost invariably approximate, and the propositions involved, if judged in a black and white way are likely to be false.
As far as empirical claims in science and engineering are concerned, it is therefore preferable to find more subtle and informative ways of characterising propositions.
It is helpful in doing so to separate out an abstract model from the phenomena in question, for one may then engage in computations and reasonings about the abstract model which are exact or sound as appropriate, and subsequently and separately say something about the expected levels of correspondence between the abstract model and the concrete world.

There is, as noted by many philosophers and others, a very great divide between the levels of confidence with which we can come to conclusions in a formal mathematical context by contrast with the reliabilty with which we can measure and predict the behaviour of the real world.
In separating out these domains, we maximise the sphere in which the highest standards of confidence can be realised, and also in which the scope for automation and the application of artificial intelligence is safely (rather than speculative;y) maximised.

Beyond this very sketchy suggestion of how empirical knowledge might be addressed, similar considerations may also apply to matters in the sphere of ethics.
Moral reasoning can equally well be conducted reliably, if there is first some agreement on moral principles, provided only that moral truths are indeed systematisable in some such way.
Greater difficulties arise if moral judgements are not thought to be rational in that way, if for example, subtle judgements are needed which depend upon both a wise head and a life's experience.

Later. more discussion of empirical knowledge will be necessary, but I have sketched here a case for the necessary machinery for building abstract models to be treated as foundational.
It is more tractible than any other area, and it provides the machinery for building precise knowledge in all other areas.

There are some interesting foundational problems within that domain, which I will refer to as that of ``Logical Truth''.

\section{Logical Truth}

\part{Historical Threads}

\chapter{Terminological Notes}

\section{On the notion of Logical Truth}

The concept of logical truth has a long history in which modern controversy plays a role.
It is not my aim here to argue a case for the particular usage of the concept which I have adopted for this essay, but rather to make some observations about how that usage relates to some milestones in the history of logic which seem important in the present context.
Most discussions about the concept proceed as though there is an objective truth about the meaning of the concept, and argue for a particular explanation of what that meaning is.
That is not what is going on here.
I do not claim to know what that concept really means, I aim only to explain the usage which I have adopted and mention some ways in which this usage connects with the ideas of some other philosophers.

Let me first mention the four philosophers who seem to me to have come closest to articulating the same concept, mostly in quite different terms: Plato, Hume, Frege and Carnap.


\subsection{Plato}

Plato lived at a time when systematic deduction had first shown its value in the development of mathematics, and had also been shown capable of proving any nonsense you like in metaphysics or cosmology.
This contrast was exhibited in the conflict between the philosophies of Parmenides (who believed that nothing changes) and Heraclitus (who saw a perpetual flux).

These two philosophies were reconciled through Plato's two worlds, that of platonic ideals, and the world of appearances.
Plato thus made the distinction between logical and empirical truth which is the basis for the conception of logical truth addressed in this essay.

\subsection{Empiricism}

Plato's pupil Aristotle was to make enormous contributions to logic, but possibly not material advancement of this particular distinction.
He was concerned by the difficulty of understanding in the context of Plato's philosophy how it was possible to reason about the concrete world, dismissed by Plato as the shadowy world of appearances of which true knowledge was not possible.
The characterisation of reason as effective only in the realm of ideas and forms unfortunately excluded it from relevance to that shadowy realm of impressions which could yield no true knowledge, but which vitally concerns us all.

In articulating the concept of \emph{demonstrative science} Aristotle gave a good account of how one can reason about the concrete world, not just the etherial world of ideas, but in doing so the line he drew was between what was scientifically necessary as determined by the logical consequences of fundamental scientific principles and the accidents of how things happen to be.
The line he thus drew between necessary and contingent truths, was that between \emph{physical} (rather than \emph{logical}) necessity and his concept of contingency was confined to the accidental rather than embracing scientific laws.

Aristotle's conception of logic, and his conception of necessity was to be dominant for thousands of years, and perhaps held back further refinement of Plato's distinction.

The debates which ultimately lead to its further refinement may be thought to have begun with the division of early modern philosophers into \emph{rationaists} (Descartes, Spinoza and Leibniz) and \emph{empiricists} (Bacon, Gallileo, Locke, Berkeley and Hume).

\pagebreak
\phantomsection
\addcontentsline{toc}{section}{Bibliography}
\bibliographystyle{rbjfmu}
\bibliography{rbj2,fmu}

%\addcontentsline{toc}{section}{Index}\label{index}
%{\twocolumn[]
%{\small\printindex}}

%\vfill

\tiny{
Started 2023/07/28

\href{http://www.rbjones.com/rbjpub/www/papers/p034.pdf}{http://www.rbjones.com/rbjpub/www/papers/p035.pdf}

}%tiny

\end{document}

% LocalWords:
