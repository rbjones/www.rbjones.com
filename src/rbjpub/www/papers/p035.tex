% $Id: p035.tex $ﬁ
% bibref{rbjp035} pdfname{p035}
\documentclass[10pt,titlepage]{book}
\usepackage{makeidx}
\usepackage{turnstile,amssymb}
\newcommand{\ignore}[1]{}
\usepackage{graphicx}
\usepackage[unicode]{hyperref}
\pagestyle{plain}
\usepackage[paperwidth=5.25in,paperheight=8in,hmargin={0.75in,0.5in},vmargin={0.5in,0.5in},includehead,includefoot]{geometry}
\hypersetup{pdfauthor={Roger Bishop Jones}}
\hypersetup{pdftitle={Positive Philosophy}}
\hypersetup{colorlinks=true, urlcolor=red, citecolor=blue, filecolor=blue, linkcolor=blue}
%\usepackage{html}
\usepackage{paralist}
\usepackage{relsize}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{longtable}
\usepackage{url}
\newcommand{\hreg}[2]{\href{#1}{#2}\footnote{\url{#1}}}
\makeindex

\title{Positive Philosophy}
\author{Roger~Bishop~Jones}
\date{\small 2023-08-08}


\begin{document}
%\frontmatter

%\begin{abstract}
% This is now a portmanteau of sketches of writings in the crossover between philosophy and artificail intelligence, in general more specifically relating to the philosophical areas of epitemology and logic, and knowledge management and automated deduction.
% It is being massaged in the direction of a book on "Synthetic Philosophy", via, in the first instance, God willing, an essay on Synthetic Epistemology for Artificial Intelligence.
%\end{abstract}
                               
\begin{titlepage}
\maketitle

%\vfill

%\begin{centering}

%{\footnotesize
%copyright\ Roger~Bishop~Jones;
%}%footnotesize

%\end{centering}

\end{titlepage}

\ \

\ignore{
\begin{centering}
{}
\end{centering}
}%ignore

\setcounter{tocdepth}{2}
{\parskip-0pt\tableofcontents}

%\listoffigures

%\mainmatter

\section{Preface}

This is a portmanteau document in which I have collected together many feeble attempts at philosophy into one place with the intent of massaging the materials until they can be bullied into one decent monograph articulating the main elements of my thinking.
It remains to be seen whether it serves that purpose.

I have used the \LaTeX{} book format so that my current principle effort can be cleanly separated out as the first part, and my abortive prior essays can each fit into a single chapter in some other part.

\part{A Synthetic Epistemology}

\chapter{Introduction}

Epistemology is the branch of philosophy which is concerned with the theory of knowledge.
Knowledge is also studied in a variety of empirical sciences, including biology, psychology and cognitive science.
Understanding human cognition is primarily the province of empirical science, but already computing machinery and other technologies have substantially transformed the way knowledge is managed, and cognitive machinery is expected to take on a larger and broader role.

As human culture has advanced, so have the ways in which we gather, preserve, disseminate and exploit knowledge.
One tendency in those changes is toward greater abstraction from the particulars of human evolution which are the origins of knowledge as we know it.
These tendencies are particularly marked in pure mathematics and the recent discipline of mathematical or symbolic logic, in which completely abstract languages and modes of reasoning are adopted.

At the same time, the cognitive abilities which are essential to the advancement and exploitation of declarative knowledge are no longer exclusively seen in humans.
Though we may still have an edge in some important aspects, cognitive machines clearly excel humans in some areas, and in an evolutionary blink may soon dwarf human cognition.

Our development and exploitation of knowledge in science and engineering is a collaborative activity, and the resulting knowledge is a shared resource of which a large part is declarative knowledge held in non-biological media.
As we advance towards a society in which this collaborative enterprise becomes increasingly dominated by cognitive machinery it may be appropriate to think hard about the structure of that shared repository, and the ways in which it is expanded and exploited.

Synthetic epistemology, as it is here construed and considered, is devoted to that task, and offers some constructive suggestions about the logical structure of such shared repositories and the ways in which those structures support rationally underpinned reasearch, development and deployment.

Though the term ``synthetic epistemology'' has been used before, its use here is distinct as far as I am aware from that previous usage.
Possibly the most distinctive feature is that the present work is non-anthropocentric.
It is not concerned with analysis of natural language, or with the study of human cognition.
It is abstract and logical in character and is primarily concerned with ways to represent, organise and apply declarative or propositional knowledge so as to facilitate the fullest exploitation of deductive reason in all those domains in which such reasoning is possible.

Centralising epistemology around logical truth facilitates the evaluation and exploitation of empirical knowledge about the world around us and the laws which govern the behaviour of physical systems.
The use of formal rather than natural language makes precision of language possible and provides a context in which deductive reasoning on a very large scale can be trusted to preserve truth.

The discussion takes the concept of logical truth in two directions.
The first is foundational, not only defining logical truth and and identifying sound and reliable proof methods, but also provding a pragmatic response to the most important forms of scepticism about such foundations.
How do we make the concept of logical truth definite and precise?
How do we give meaning to the languages in which logical truths can be expressed?
How do we establish rules which reliably establish which inferences are sound and which propositions are true?

\begin{itemize}
\item Theoretical Foundations
  \begin{itemize}
    \item Truth conditional semantics
  \item Set theoretic truth
  \item Proof in set theory
  \end{itemize}
\item Practical Foundations
  \begin{itemize}
  \item Type theoretic truth
  \item Type theoretic proof
  \end{itemize}
\item Universality and Pluralism
  \end{itemize}

\chapter{Logical Truth}

In describing ``logical truth'' I will be defining (substnatially by reference to prior literature) certain languages.
These are foundational languages and there is therefore a problem of regress in the definition to be considered.
Either we have some language which itself is not defined or we must have a language which is directly or indirectly defined usingthat same language.
We will discuss this further later, but for present purposes I will assume that the kind of language used to describe logical systems in the literature of mathematical logic is sufficiently clear.

\section{Declarative Language}

The concept of ``logical truth'' can only be given a precise definition in a suitable context.
A logical truth is a sentence in a certain kind of language, a declarative or propositional language.
This is the kind of language in which some subject matter is spoken of using sentences which make a claim about that subject which may be either truth or false, depending on whether the subject matter conforms to conditions which are understood to be expressed by the sentence.

These conditions are called ``truth conditions'' and for present purposes the truth conditions may be considered the proposition expressed by a sentence (though in other contexts a very different notion of proposition is called for).

\subsection{Some Metaphysics}

In defining a declarative language various entities are involved.
It is in the essence of such languages that there are things about which propositions are expressed (belonging to the \emph{domain of discourse}), and there are other things which are understood to express the propositions (\emph{syntactic} entities often called sentences).
Expressing a proposition about some subject matter, or domain of discourse, usually will involve referring to some (or all) of the things which exist in that domain, and will refer to them using syntactic entities (which are often called ``terms'').

Though my positive philosophical stance is ontologically conventionalist, this account of logical truth is ontologically neutral.


\subsection{Truth Conditional Semantics}


\section{Set Theoretic Truth}

In this chapter I give a definition of the concept of ``logical truth'' as it is used in this exposition.
This involves defining a class of logical systems each consisting of an abstract syntax, a truth conditional semantics and a semi-decidable deductive system which is sound relative to the semantics.
The sentences of these languages which are ``valid'' under the semantics are the logical truths expressible in the language in question.

I then defend the following claims:

\begin{itemize}

\item That there are languages which are \emph{practically universal} for logical truth, by virtue of the reducibility of logical truth in all other languages to logical truth in those languages.
\item That there are deductive system for these universal languages which are \emph{practically universal}, and which may therefore serve to reliably establish practically all logical truths.
\end{itemize}

In defending these claims I will offer arguments responding to the following contrary sceptical claims:

\begin{itemize}
\item Arguments from regress against the possibility of defining the semantics of languages, and against the possibility of conclusive deductive proof.
\end{itemize}

This is a \emph{foundationalist} enterprise, but one which affirms the utility of foundational thinking and foundational technologies, rather than promulgating absolutist claims about foundations.
It is not my position that it is possible to give absolutely precise or unambiguous definitions of semantics, or that we can ever demonstrate truth with absolute certainty, but rather that these things can be done to a degree of precision and certainty which greatly exceeds any practical need.

Foundations in the real world are solid bases on which important constructions can be raised.
Though they themselves must be sufficiently solid, they do not have to be absolutely solid, and their solidity does not depend upon them resting upon some other equally or more solid substrate.
They are often devised because their structure enables a solid foundation to be built on a more tenuous substrate, by techniques such as the use of rigid rafts or deeply penetrating piles.

Similarly, the advancement of precision engineering is possible because a machine tool used suitably can build another tool which has superior precision.
It is possible in semantics, to defined a language which has precise and unambiguous meaning using a language which is rather more unruly.
This is what has happened throughout the progress of science.
Mathematics has been a source of more precise language for science, and in recent times, mathematical logic has advanced the precision beyond mathematics beyond its former confines.

There are two logical systems which I nominate here as my preferred exemplars of the kinds of practical universality in my claims.
These are both well established systems of impeccable repute, but they are not unique in exhibiting the characteristics which I applaud.
In these basic systems I offer no novelty, though later in the discussion I will talk about some ways in which further elaboration might be beneficial.

That there are two, arises from my belief that the requirements for addressing the philosophical scepticisms which may be raised differ from those which are convenient in the application of the logical systems.
The more practical system is the logic implemented in the Cambridge HOL system, which was devised at the hardware verification group at the University of Cambridge for the purpose of reasoning about he properties of digital electronics, but which has subsequently been applied more broadly.
The other system which is convenient for the purpose of foundational discussions, and in which the semantics of Cambridge HOL has been rendered, is the first order set theory ZFC\footnote{Zermelo Fraenkel with Choice}.
These systems are have both been carefully defined and extensively studied and applied, the former being defined in terms which may be interpreted in the latter.
These descriptions will not be reproduced here except in those aspects which prove essential to the philosophical discussions which are my focus, though detailed references are provided below.
  


\chapter{Empirical and Other Truths}


\chapter{Attik2}

\section{Introductory material}

It is divorced from humanity in just those ways in which the work of mathematicians seeking solidity to their methods detached mathematics from the vernacular and made reasoning into the calculus which Leibniz had sought before him.
That calculus was a great achievement, but not a practical way to do mathematics, or any other kind of reasoning, for mere mortals.
Pushing through Frege's



The idea of \emph{logical truth} is therefore central to this presentation, and is used in a broad sense now widely deprecated.
The meaning of the word ``know'', and hence the difference between knowledge and belief is not something I consider, but I do address some of the classical concerns of epistemology, including sceptical arguments about meaning and truth.

The centrepiece of the ideas presented is the idea that ``logical truth'' can be given a broad and practically universal characterisation.
To give a sense for the use here of the term ``practically universal'' consider the significance of Godel's celebrated demonstration of the incompleteness of arithmetic.
There is no controversy about this theorem, insofar as it is generally accepted as true, but its practical consequences are not conspicuous.
We have available to us a heirarchy of theories of increasing strength in which to demonstrate recalcitrant conjectures, and though there are enture mathematical theories developed to exhibit the limitations of the erstwhile standard foundation for mathematics, they all fall under the sway of the stronger foundations invigorated by a suitable ``large cardinal axiom''.
Meanwhile, in the more prosaic realms of real world applications of mathematics, such extremes are not called for.


It has been used for epistemology conducted as a kind of empirical philosophy, concerned therefore with reaching synthetic rather than analytic results in the sense give to those terms by Kant.
By contrast, the usage here is closer to the more ancient usage (but still more broadly understood than Kant's wrinkle) in which analysis, breaking down, is contrasted with synthesis, putting together.

Hence, we are concerned here to synthesise epistemology in the sense of putting together a theory of knowledge, which is done with the intent of contributing not only to our understanding of what knowledge \emph{might be}, but also to inform the construction of more effective ways of gathering, evaluating and applying knowledge.



Its concern with knowledge is not a concern with any material phenomenon but a concern with the structure of explicit (propositional, declarative) knowledge as it may be represented in shared resources whose form is designed rather than evolved.
It is this concern which legitimises the idea an epistemology as a synthesis.
This kind of theoretical approach to epistemology should be seen as analogous to the elaboration of mathematical theories or the design of notations or languages, which it does indeed involve.

I am therefore here concerned with propositional or declarative knowledge, that kind of knowledge which consists in knowing that some sentence or proposition it true and which is therefore held expressed or manipulated in some language.
However, it not the knowing which concerns us, it is that which is known, for which we retain the word, for discursive purposes, while stepping back from some of its everyday connotations and associations, or even its everyday meaning.

The ways in which this kind of knowledge works have been transformed over the last few millenia as a result of advances in our knowledge, and this synthesis is intended to continue that progression.
A first step was the development of axiomatic geometry by the ancient Greeks, which showed that by draconian restriction of language and a focus on abstractions which can readily be given a concise (axiomatic) characterisation, deductive reason can be conducted with very high levels of accuracy and reliability.
Sharp language enables rigorous reasoning.

In the same time it was amply demonstrated that ostensibly the same rational methods failed to deliver similar results in a broader linguistic context.
The solid results obtained by the axiomatic method in reasoning about mathematics could not be reproduced in the applications of reason to metaphysics and cosmology.

Aristotle's pioneering investigations into logic were to prove influential for the next two thousand years, but failed to broaden the scope over which deductive reasoning would yield good results.
So impotent were they that Aristotelian Syllogistic was to be entirely dismissed by such important figures in the rise of modern science as Bacon and DesCartes.

Leibniz, on the other hand, was thoroughly convinced of the potential for application of Aristotelian logic, and saw in it a route to the mechanisation of ``demonstrative science'', which would have constituted a transformation in our conception of scientific knowledge if it had been realised.
His confidence in Aristotle's logic was misplaced, but his sense of direction was sound.
He was active in progressing scientific method in more realistic ways, including the establishment of scientific institutions and journals, a universal language.
These advances shifted science toward a globally cooperative enterprise managing and extending a shared corpus of knowledge.
It was but a few hundred years before technical advances made progression of his ideas feasible.

It was in the 19th and 20th Centuries that mathematicians, after two millenia of refinement and elaboration of mathematical notation turned their attention to logic, and hence to a conception of formal logic which was potentially applicable in any body of declarative or propositional language.
During that time, the computational machinery became of age, and rapidly progressed in its computational power, making feasible computation on a scale previously unimaginable.

There is in this sketch a progression whereby more is achieved by less.
To make reasoning more reliable, much simpler language is chosen and the domain of discourse is limited to a space in which meanings are precise because governed by fiat rather than being determined by prior discourse with its ambiguities and inconsistencies.

One last point in this progression flows from the progress which was achieved in addressing the precise description of languages both ibn their syntactic form and in their semantic content.
To achieve precision and clarity in this enterprise, mathematical languages are used, and this effects a factorisation in which languages speaking of the material world are rendered completely in terms of abstract entities and then connected to the material world by describing the relationship between material entities and the abstract entities which are their surrogates in the abstract semantics. 

The ``synthetic epistemology'' which is presented here provides a prescription for a distributed knowledge repository which is collaboratively developed, extended and applied in a manner not dissimilar to the share resource which is the internet, or the World Wide Web, and would indeed most likely be accessible from the world wide web (though that will not be further discussed here).

At the core of this synthesis is an abstract universal representation of \emph{logical truth}.
The definition of this notation includes semantics and detailed inference rules.
The system is not new, and much of the detail, which is not significant is identified by reference.
There are three main directions in which the discussion here progresses and which are undertaken in the following order:

\begin{itemize}
\item Foundational responses to sceptical doubts, particularly in relation to the problem of regress in the definition of the semantics and the soundness of the deductive system (and hence the soundness of the deductive closure).
  
\item A presentation of the scope of applicability of models constructed in the system.

\end{itemize}

\section{Logical Truth}

The system I describe here is an abstract language and deductive system for the broad conception of logical truth, which is often called ``set theoretic truth''.

I take a language to be a system of representatives.
These are tokens for entities, some of which are truth values.





\chapter{Attic}

\section{Why Synthesise?}

The advent of synthetic biology marks possibly the largest ever transformation in the way in which the ecosystem of planet earth and humanity evolves.
The transformation from an evolution primarily engendered the blind forces of chance mutation and natural selection to one in which \emph{intelligent design} plays an increasing role.

That we have reached this point is due to evolution itself, which has shaped not only our physical nature but our culture, the ways in which we seek and apply knowledge, and the nature of knowledge itself.

Just as the study of biology is moving from observation to design, the study of knowledge may also graduate, from analysis and observation to synthesis.
This may be the moment for epistemology naturalised to yield ground to epistemology synthesised.

Synthetic biology is not the most trenchant scientific advance to suggest this approach to epistemology, it is surely eclipsed by cognitive science and engineering.
Though much of the engineering of artificial intelligence is focused on replicating human capabilities, the expectation of going beyond is high, and the invitation to re-think the nature of knowledge and the role of intelligence in its genesis and exploitation is surely compelling.

Beyond the relatively conservative scientific academia epistemology has has been more radically transformed by Michel Foucault and ``post-modern'' philosophy.
From his perspective epistemology is always culturally relative and is a sel-interested power-play by the dominant groups in each society, a perspective which serves to justify activist groups in moulding meaning and truth to their agenda, and making truth and freedom a casualty of ideological zeal.
This involves the denial that knowledge and the ways in which it is established and applied are instrumental in advancing the prosperity of humanity, and invites an epistemological synthesis which supports a robust challenge to Foucauldian nihilism.

The rationale for synthesis faces both ways.
It is a defence against the kind of progressivism which would abandon the instrumental advantages of objective truth for the sake of seizing political power.
On the other hand it peers forward into a future in which the instrumental advantages of knowledge are leveraged not only using the intellectual power of \emph{homo sapiens} but also by an increasingly diverse array of intelligent machinery.

\section{The Approach}

Postmodern scepticism may have begun as a critique of ``hegemonic'' epistemes which characterised them as power-seeking or power-preserving rather than being rooted in instrumental approaches to objective truth, but it graduated to the more radical scepticisms associated with the denial of objective truth consequent on the denial of meaning to language.
It may be noted that similarly sceptical arguments about meaning appear in mid 20th Century analytic philosophy and miraculously served to discredit the scientifically oriented logical positivism without impinging on less scientifically oriented approaches to analysis.

This epistemology will therefore be responding first of all to scepticism about the meaning of language and the possibility of sound deduction.

************


Epistemology one may therefore argue, should not be seen as the theory of a fixed phenomenon of which we seek to advance our understanding, but rather of a developing phenomenon in relation to which we may play a role not merely in understanding the present but also in shaping the future.

The term ``synthetic epistemology'' is adopted here for an epistemology which is designed rather than hypothesised.
In undertaking such a synthesis one must surely have some criteria in mind which will guide the choices which must be made and any subsequent evaluation of the merits of the resulting epistemology.

The nature of knowledge has been changing throughout the evolution of life on earth.
Epistemology, the \emph{theory} of knowledge, is recent innovation, and has itself evolved over the two millennia since the writings of Plato and Aristotle.

The use of deductive reason in its simplest forms is probably coeval with \emph{homo sapiens}, since it depends on language.%
\footnote{It is not universally agreed that non-linguistic primates are \emph{by definition} unable to undertake deductive inference, but no definition of deduction seems possible which does not explicitly or implicitly refer to language.  To be deductive, and inference must be \emph{logically sound}, and must therefore be expressed in a language which has a definite semantics.}
It's explicit, systematic and elaborate use is much more recent, beginning with the transformation of mathematics into a theoretical discipline by the ancient Greek philosophers, most famously in the development of Euclidean geometry.

Over the next two thousand years the development of mathematics underpinned advances in science and engineering which enabled the industrial revolution and transformed the prosperity and well being of humanity.
During the last two hundred years, the continuing advances of mathematics included a renewed attention to the foundations of mathematical analysis which ultimately addressed the logical foundations of the discipline.
These advances lead to a new sub-discipline of mathematical logic, which had an epistemological significance not confined to the foundations of mathematics, but, as will be argued here, thoughout the entire domain of propositional or declarative knowledge.

These impact upon how reliably deductive reasoning can be conducted, and beyond that to the kinds of results which can be achieved by deduction and the ways in which deduction can facilitate our knowledge of all matters in which deductive reason is possible.

Though the theoretical advances in logic which transform our understanding of logic are now well established, their fullest exploitation still remains for the future.
In the first instance the advances were of limited applicability because of the complexity of the formal derivations which they involved.

The pioneering work by Frege, and by Russell and Whitehead, showed that mathematics could be derived in appropriate fully formal logical systems.
But at the cost of great complexity, of which the benefits have seemed too slender to encourage those efforts to be continued.
The new discipline of ``Mathematical Logic'' or meta-mathematics flourished, but this was a new branch of mathematics primarily concerned with the theoretical studies of mathematical interest in relation to these new formal systems.
There was little interest among mathematicians in adopting the new standards of rigour exhibited by \emph{Principia Mathematica}\cite{russell1910}.

\section{The Logic}

This synthetic epistemology is oriented toward the management and exploitation of a shared distributed body of propositional knowledge.

This consists in a description of the abstract structures involved, an account of the semantics of the propositions within it, and a set of rules which govern the deduction of new propositions, and of extension of the languages in which these propositions are presented.

The particulars of the logical system are to some degree arbitrary, there are many similar logical systems which are equivalent in many ways.
This is not an area in which this epistemology looks for innovation and the system around which the proposal is based is therefore a well established system which not only has very distinguished origins, bing substantially based on the work of Russell, Ramsey, leading to  Church's Simple Theory of Types.
This system has been further developed in the course of applied research, notably by Gordon and the Cambridge University Hardware Verification Group.
This system is well documented, the documentation including an account of the language and its semantics, a specification of the deductive system and arguments supporting its soundness relative to the given semantics.
In addition to an informal set theoretic account of these fundamentals there is a formal presentation in the language itself.

It is not therefore intended to present the details of this system here, but for philosophical purposes we will not entirely rest upon the impeccability of its academic credentials, for epistemology is expected to respond in some way to sceptical arguments to which non-philosophers would give no credence.

We will take such sceptical arguments seriously and sketch ways in which the credibility of these arguments may be reduced to minimal levels.
This applies to sceptical doubts, for example, about the definability of the semantics or the soundness of the proof system, which are the most crucial theoretical issues.

Some of the more speculative ideas presented here cannot be given similar levels of assurance, these include a suggestion of \emph{near universality} and the most important question of whether or not this synthetic epistemology is practical and optimal for progressing rigorously and productively the future of science and engineering in a world in which human endeavours are supplemented by artificial intelligence.

The epistemology proposed concerns primarily those kinds of knowledge about which it is possible to reason rationally, and is intended to ensure that such reasoning is correct, and hence that its results are true.

I will be using the term ``logical truth'' for that kind of true proposition which can be established by the epistemic methods described, and held in the manner described, which may be thought of either as a \emph{theory hierarchy} or a \emph{knowledge repository}.
This choice of terminology may be controversial, particularly among philosophers, but it is not my purpose to engage in terminological controversy, so I will not be arguing the merits of that choice.

\section{Structure}

The centrepiece of this exposition is the concept of logical truth, and the exposition therefore begins with the questions of how logical truth can be defined, how logical truths can be established and how to organise logical truths into a coherent hierarchies of theories.
This gives a position on the main scepticisms relevant to this topic, particularly, scepticism about giving meaning to language so as to make the truth of propositions definite, and about the possibility of conclusively demonstrating logical truths.

The structure presented here reflects my belief that near universal languages for logical truth exist, and that this is reflected in the existence of universal families of a kind similar to those which were thought to follow from the undefinability in arithmetic of arithmetic truth.
At their outer reaches both the semantics and the proof systems for logical truth become progressively more tenuous, but these outer reaches are tested only by the furthest reaches of set theory.
Within the more down to earth mathematics of empirical science and engineering, nether residual semantic ambiguity nor the incompleteness of deductive systems is practically significant.

Logical truth is propositional knowledge, i.e. knowledge expressed in language as propositions.
It therefore depends upon language and upon languages with unambiguous semantics (or truth conditions).
To define semantics we have to have some subject matter for these truths to be about, and we need some method for determining which sentences express true propositions under the given semantics.
Identifying the necessary ontology demands some discussion of metaphysics.

Epistemologically this is an account of a priori knowledge, which draws in some elementary considerations from, metaphysics, the philosophy of language and of logic, and ultimately yields a structure most relevant to cognitive engineering, a prescription for how a priori knowledge might be organised by some forthcoming artificial intelligence.

\section{Metaphysics}

This epistemological synthesis is ontologically conventionalist.

Whatever topic we care to discuss, however abstract, the aspects of that topic under discussion must ``exist'' in order for us to be able to talk about them.
For example, if we are responsible for deliveries, we need to be able to plan routes, and we will be concerned to know whether the is a route which will permit the goods in hand to be delivered within the constraints of time and fuel.
We therefore seek an answer to the question whether such a route exists, but we have no interest in any possible metaphysical question as to whether delivery routes exist.

If we are in the business of devising formal languages for addressing particular problem domains, we must when settling the semantics of our language, make the necessary ontological provisions for the existence problems in the problem domain to receive satisfactory answers.
Just as in all other aspects of the design of the language



\chapter{Appendices}

\section{The Evolution of Knowledge and Epistemology}

\subsection{Claude on the evolution of memory}

Here is a brief overview of the evolution of memory systems:

- Single-celled organisms like bacteria have simple memory in the form of biochemical pathways that allow them to adapt to stimuli and environments. This is a rudimentary form of memory.

- Invertebrates like Aplysia sea slugs exhibit simple forms of short-term memory and sensitization to stimuli. This allows basic learned behaviors.

- Arthropods like insects and crustaceans have more complex memory centers like mushroom bodies which allow more sophisticated learned behaviors and navigation. 

- In vertebrates, the hippocampus is the primary brain region associated with memory formation. Its structure evolved along different paths in amphibians, reptiles, birds and mammals.

- Long-term memory formation likely evolved in early bony fishes around 300-400 million years ago.

- The neocortex, critical for many higher-level cognitive functions like language and reasoning, evolved in mammals and reaches its most advanced form in humans.

- Long-term potentiation, the strengthening of synaptic connections, provides a key neuronal mechanism for memory storage in mammals.

- Episodic memory or autobiographical memory is thought to be uniquely advanced in humans compared to other species. 

So in summary, memory has evolved from basic biochemical mechanisms in single-celled lifeforms to increasingly complex systems across invertebrates, lower vertebrates, mammals and primates. More advanced memory systems have coevolved with higher intelligence and learning capabilities.

\subsection{Claude on the evolution of homo sapiens}

The genus Homo evolved around 2-3 million years ago during the Pliocene epoch. Here is a brief overview of the timeline of the evolution of the genus Homo:

- Around 7 million years ago, the evolutionary line that would lead to humans diverged from the ancestors of modern chimpanzees. 

- By around 4 million years ago, early hominins belonging to the Australopithecus genus had emerged in Africa. They exhibited bipedal walking.

- The earliest proposed member of the Homo genus is Homo habilis, which emerged approximately 2.8-2.3 million years ago. They were the first hominins to use stone tools.

- Homo erectus evolved almost 2 million years ago and were the first hominins to migrate out of Africa into Eurasia. They used more advanced stone tools and controlled fire.

- Archaic Homo sapiens evolved around 300,000 years ago and were the predecessors of modern humans, with more developed brains and skulls. 

- Anatomically modern humans, Homo sapiens sapiens, emerged around 300,000-200,000 years ago in Africa. 

- Homo sapiens developed complex language, art, and technology, allowing migration around the world beginning 60,000-70,000 years ago.

So in summary, the Homo evolutionary line emerged roughly 2-3 million years ago and has given rise to modern humans through a series of transitions and developments over millions of years.

\subsection{and on the evolution of the human brain}

The evolution of the brain in genus Homo progressed through several key stages:

- Homo habilis (~2.8-2.3 million years ago): The brain size of H. habilis averaged about 600-750 cc, larger than the 500 cc average for Australopithecines. This allowed more advanced tool use.

- Homo erectus (~2 million years ago): H. erectus brains averaged around 900 cc with expanded parietal and frontal lobes compared to H. habilis. This improved cognition for complex hunting and tool usage.

- Homo heidelbergensis (~600,000 years ago): Brain size increased to an average of 1,100-1,400 cc. The morphology was essentially modern. Improvements to linguistic capacity emerged.

- Archaic Homo sapiens (~300,000 years ago): Brain size plateaued around 1,200-1,750 cc with the cranial capacity very close to modern humans. Complex language likely evolved around this stage.

- Homo sapiens (~200,000 years ago): The modern human brain averages around 1,350 cc but overlaps with late archaic human ranges. Further neurological rewiring improved language, reasoning, and social cognition. 

- Behavioral modernity (~50,000 years ago): Rapid cultural and behavioral innovation indicates complex neural circuitry nearly identical to modern humans emerged around this time period.

So in summary, the Homo brain progressively increased in size and neurological complexity over several key stages, with rapid development of cognitive abilities related to language, social interaction, and abstract thinking in the later transitions leading to modern humans.

\subsection{Kinds of Knowledge}
For present purposes let us divide knowledge into three kinds:
\begin{itemize}
\item knowledge \emph{by acquaintance}
\item knowing \emph{how}
\item knowing \emph{that}
\end{itemize}

Knowledge of the first two kinds predates language and is difficult to share.
In the first instance we may consider the ability to survive in some suitable environmental niche as a form of knowledge which is secured by evolution, built into anatomy and its chemistry, and passed on by genes.

\subsection{Knowing How}

The evolution of life on earth can be viewed from many different perspectives.
For the purposes of this epistemological synthesis, it may be helpful to consider in the evolution of life the progression of ways in which living organisms or cooperating groups of organisms (not necessarily of tne same species) proliferate by the use of progressively more comprehensive and sophisticated modelling techniques, which progression this epistemology aspires to continue.

I begin only as multi-cellular organisms with some elementary nervous system have evolved, exhibiting behaviour in seeking nourishment and shelter, and avoiding predators and other perils.

For the purposes of this synthesis I consider knowledge as falling into three kinds, knowledge by acquaintance, knowing how, and knowing that.

The first two, I suggest, are kinds of knowledge which primarily reside in the structure of central nervous

\subsection{Stages in the Development of Memory}

The story here is mostly taken from \cite{murray2017evolution}.

\begin{itemize}
\item[early animals]
  
  \begin{itemize}
\item[]
\item[various]
  \end{itemize}

\item[early vertebrates]
  
  \begin{itemize}
\item[]
\item[Navigation memory]
  \end{itemize}
 
\item[early mammals]
  
  \begin{itemize}
\item[]
\item[Biased-competition memory]
  \end{itemize}
 
\item[early primates]
  
  \begin{itemize}
\item[]
\item[Manual-foraging memory]
  \end{itemize}
 
\item[anthropoids]
  
  \begin{itemize}
\item[]
\item[Feature memory]
\item[Goal memory]
  \end{itemize}
  
\item[hominims]
  
  \begin{itemize}
\item[]
\item[Social-subjective memory]
  \end{itemize}
  \end{itemize}

\subsection{Deduction}

It is only very recently in the evolution of life on earth that the possibility of exploiting knowledge by systematic and elaborate deductive inference has been known.

Some two and a half millennia ago the ancient Greek civilisation transformed mathematics into a theoretical discipline by instigating the systematic deductive exploration of geometry and arithmetic.
In doing so they demonstrated in these very particular domains, that deductive reasoning could be conducted with very high degrees of reliability, so that the truths thus obtained were universally accepted and have endured ever since.

By contrast, in the same period, the use of reason in other domains, such as the study of nature and the cosmos, proved unable to realise enduring consensus, and the use of \emph{reductio ad absurdum} facilitated the demonstration of absurdities.

Throughout subsequent history the stark contrast between what can be established with certainty by reasoning about abstract domains and all reasoning, deductive or otherwise, about the concrete world was frequently recognised.
As frequently, philosophers have sought the imprimatur of deductive rigour for their views and have claimed a broader scope for its conclusions than the historical record could support.

At the same time, the potential benefits of deductive reason have remained largely untapped.
This was in part due to the incomplete understanding of deductive logic which we inherited from the ancient Greeks.
When that defect was remedied, the complexity of detailed formal logical reasoning deterred its widespread adoption, and only in special circumstances where the extra costs involved were acceptable or in the context of academic research and with the aid of computer support, have the techniques been progressed and applied.


\section{HOL History and References}

\subsection{Some History}

The established logical system around which this epistemology is built is that of the ``Cambridge HOL'' Interactive Theorem Prover, the acronyn HOL standing for Higher Order Logic and used to refer to the theorem prover, the language and the deductive system.

This system descended from the logical system of \emph{Principia Mathematica} in the following stages:

\begin{enumerate}
\item Russell published in 1908 his ``Theory of Types'' \cite{russell1908,heijenoort67} which was to be used for the derivation of mathematics in Principia Mathematica \cite{russell1913}.
\item Frank Ramsey observed that the ramifications in Russell's Theory of Types were unnecessary, allowing the logical system to be simplified to the Simple Theory of Types.
  He wrote a manuscript on this in 1923, which was published posthumously in 1925 \cite{ramsey25,ramsey1931}
  Full details of this system were first published by Hilbert an Ackerman in their ``Grundlagen der Theoretichen Logic'' \cite{hilbert1928}
\item Alonzo Church, after failing to construct a consistent foundation for mathematics using the type-free lambda calculus re-cast the Simple Theory of Types as a typed lambda calculus \cite{churchSTT}.
\item In his work building theorem provers for software verification using Dana Scott's Logic for Computable Functions \cite{scott1993type}, Robin Milner adapted the type system in Church's STT by moving type variables from the metalanguage into the object language.
  This system later became known as the Hindley-Milner type system.
  During the development of the LCF theorem prover Mike Gordon joined Robin Milner at the University of Edinburgh, and would later be the lead figure in the continuation of work on LCF at Cambridge.\footnote{He later documented the history in \cite{gordon2000lcf}}

\item When switching from software verification to hardware verification at Cambridge University, Mike Gordon decided that Higher Order Logic would provide a better basis for that work, and adopted Church's Simple Theory of Types with a number of relatively uncontroversial adjustments for that work.
  The polymorphic version of the type system was one of them.
\end{enumerate}

\subsection{References}

  The following documents describe the logical system as it now stands.
  THe definitive description of the HOL logical system, including syntax, semantics, deductive system, rules for conservative extension, and the theory heirarchy was undertaken by Andrew Pitts and is available for download from the HOL repository on sourceforge \cite{pittsHOLlogic}.
  
  The following paper might by itself be sufficient, since it documents the most recent changes and cites previous literature \cite{arthan2016}.
  The ProofPower implementation of the HOL logic has an extensive formal treatment in HOL itself 
  \cite{arthanspc001,arthanspc002,arthanspc003,arthanspc004,arthanspc005} available from the {\tt lemmaone.com} website.
  Also presented at the VDM91\footnote{VDM ’91, Formal Software Development Methods} conference \cite{arthan91}.
  
\section{Positivist Metaphysics}

Positive philosophy is particularly noted for its rejection of Metaphysics.
This comes down as far as Rudolf Carnap who's anti-metaphysical stance remained was unremitting.

There are two principle conceptions of metaphysics which Carnap rejected.

The first can be captured in the phrase \emph{synthetic a priori}.
Metaphysics is thus conceived of as truths about the ultimate nature of reality which are established by rational means, rather than by observation.

A related critique of metaphysics flows from the empiricist desire to connect meaning with observation, if a proposition had no empirically verifiable consequences then it was either tautologous self-contradictory or meaningless.

The younger Carnap spoke of metaphysics as meaningless, the elder complained only of a lack of cognitive content.

At the same time Carnap broke with the Positivist tradition in a number of ways which effectively softened his stance on Metaphysics.
Firstly, he abjured nominalism.

To support a rich notion of logical truth we need to have things to talk about.
It doesn't matter what they are.
It does how many there are.

To get a ``universal'' conception of logical truth depends upon having arbitrarily many entities.
This is realised in set theory through ``large cardinal axioms'', which provide ever increasing lower bounds on the cardinality of the unverse of sets, in the context of a single axiomatic set theory.

In our logical system a similar method suffices.
The Simple Theory of Types has usually come with a single axiom of infinity sufficient for arithmentic, which asserts that there are infinitely many ``individuals''.
This enables the definition of the natural numbers and the operations upon them.
Given closure under the function space constructor, there will also be types suitable for representation of rational numbers, the more numerous real numbers and most applicable mathematics.
For the more exotic reaches of research in set theory, the same logical system can be extended with a stronger axiom of infinity or the required results can be rendered as conditional on the same ontological principles. 

  
\part{Essay Fragments}

One essay attempt per chapter.
This isn't a historical record, they are here to get mangled and recombined, or discarded.

\chapter{Synthetic Philosophy}

\section{Introduction}

This monograph presents a \emph{philosophical system} belonging to what has latterly been called the \emph{analytic} tradition.
Philosophy in that tradition, though predominantly engaged in varieties of supposedly rational analysis, have been, to a degree, understandably, anthropocentric.

David Hume's philosophical \emph{Magnum Opus} was ``A Treatise of Human Nature'' \cite{humeTHN} and his subsequent briefer presentation of its essence ``An Enquiry into Human Reason''\cite{humeECHU}.
A couple of centuries later the linguistic turn favoured logic and language over psychology, but resisted formalisation to retain its anthropocentric engrossment in the natural languages of \emph{homo sapiens}.

As I write we are firmly in the 21st Century.
``Artificial Intelligence'' is penetrating through the technology on which we now all depend, spawning on its way the new term ``General AI'' to denote what AI was once expected to be but has not yet become.

The colonisation of Mars has moved from science fiction into the plans of real world corporations contemplating the means to send a million tons to the red planet.
The rocket factories are in place and growing, while the design and testing of ever larger interplanetary transports continues to progress,

Meanwhile, the manufacture of android robots begins, destined to build on Mars the life support systems necessary for human habitation (and the fuel factories for return journeys).
A good measure of autonomous intelligence will be required.

These are the first steps in a process which may ultimately lead to the proliferation of intelligence across our Galaxy and into the Universe beyond.
That androids should be pioneers in colonising Mars, presages their prominence in any subsequent interstellar progression.
Ultimately we may find biological intelligence a crucial but small kernel of a sphere of human cognitive influence.

What then of epistemology?
The proliferation of intelligence across the Galaxy will be a technology intensive process, dependent on continued advances in science, technology and engineering.
What kind of epistemological theories will best support that process?

The central roles in the accumulation of knowledge played by the human intellect, sense organs, language and culture, and the imprint on these resulting from the mechanics of the evolutionary process, may not be ideal for inanimate intelligence in completely different and very varied circumstances.

Is it possible for \emph{homo sapiens philosophicus} to detach himself from his context and origins to produce philosophy relevant to diverse cognitive systems in very different circumstances?
Could philosophical foundations emerging from such enquiries bear fruit also for earthbound humanity?

[*****rework:{\it

I believe that such an enterprise may be worthwhile.
We can see in fundamental advances made in the last couple of centuries, an unwitting tendency in that direction.
The advancement of mathematics beyond its concern with numbers into more abstract and diverse subject matters, and the reduction of the whole to the simplest logical systems and ontologies, far removed from everyday language and commonplace clutter, ultimately to the benefit of science, engineering and human well-being and prosperity is just such a trend.
We have arguably yet to come properly to terms with the results of the combined effort of mathematicians and philosophers (and later computer scientists) in developing theoretical and formal approaches to language and logic, and may only do so when the complexity of detailed analysis is brought under control by the combined computational power and penetrating intelligence which is anticipated in our new cognitive assistants.

Even before these modern developments the simple structures of mathematics are among those subjects of human scholarship which can least be suspected of anthropocentrism.
Mathematics must surely begin with counting.
The details of numerals for recording how far a count has proceeded (and hence how many were counted) are arbitrary and human, but the structure of the natural numbers which the numerals designate seems uniquely determined by the very nature of counting.
It is impossible for us to imagine how any alien intelligence could not be acquainted with exactly the same number systems.

This tendency of mathematics to begin with the simplest structures and concepts was conspicuous when mathematics was brought to bear upon language and logic.
Whereas Aristotle's Organon%
\footnote{The collection of his writings related to logic.}%
and its scholastic elaborations were dressed in accidents of human cultural evolution, Frege's \emph{Begriffschrift} \cite{frege79}, %
\footnote{a seminal work in the early stages of the modern transformation of logic}%
  although undoubtedly an earthly notation, was much closer to exhibiting that compelling power and simplicity which marks the universal.

}]

\subsection{The Structure of Synthetic Philosophy}

In calling this \emph{synthetic} philosophy I suggest that it is constructed rather than discovered.
It is constructed in the context of a body of knowledge gathered and refined over the last several millennia and exploits some of the insights which that knowledge brings, not least on the nature of knowledge itself.
However, it is primarily pragmatic in its intent, not an addition to that body of knowledge but some ideas about how to move forward in its light.


These ideas are primarily epistemological, concerned with the ``theory of knowledge'' and in its implications for \emph{knowledge management}, how to gather, expand and exploit the knowledge which is essential to our future well-being and prosperity.

Having made a fuss about \emph{anthropocentrism} I will occasionally be commenting of the de-humanisation with results from its avoidance.
The first of those concerns how ``theory of knowledge'' is to be construed.

The word ``know'' belongs to the English language, which we can only know by observation, which in turn shows a variety of usage difficult to incorporate into a single coherent model.
If we sought to rigorously address the phenomenon spoken of by the word ``know'' our case would be hopeless.

Advances in science often depend on the introduction of new concepts, such as the the displacement for scientific purposes of the terms ``hot'' and ``cold'' by carefjlly defined temperature scales.
The word ``know'' is similar in its unsuitability for scientific purposes, and our synthetic philosophy is intended to share or exceed those characteristics of science which make it so.

So the first point of de-humanisation is to detatch our story from the word ``knowledge''.
This doesn't mean you won't be seeing the word again, it means it will not play a technical role in what follows, it belongs to the informal descriptions which are inteded to make the technical content intelligible.

So one of my first tasks will be to explain what the ``theory of knowledge'' becomes when we detach it from the English language.



Synthetic philosophy is primarily epistemologica;, it is concerned with the nature of knowledge, how it can be established, organised and applied.
But not with the meaning of the word ``know'', which is used informally and does not have a fundamental role in the philosophy.

In its concern for knowledge the distinction between knowing \emph{how}, which is to say, having some skill or capability, and knowing \emph{that}, being  is important

The central concept of synthetic philosophy is that of \emph{Logical Truth}.

\subsection{Positivist Metaphysics}




\chapter{Oracular AI (2)}

\section*{Preface}

It was in 1986 that I first had hands on an ``Interactive Theorem Prover'', the Cambridge HOL system,
engineered to support formal reasoning about digital electronics.
I had just began working on the applications of formal methods to the development of security and safety critical information systems.

One of my early reactions on acquaintance with this system was to perceive the broadness of its potential applications.
The theory hierarchy which HOL supported, enabling the structured development of abstract models and of  the underlying mathematics required to build such modesl was in effect a general purpose \emph{knowledge base} of a kind relevant to GOFAI \footnote{Good Old Fashioned AI} with some extra features not often mentioned in AI.

The ability of the system to ensure logical consistency, surely essential for large scale deductive reasoning to have any value, together with a strong cultural preference for staying within those bounds, and the general perception that applications in engineering require nothing more, provided a seed in my mind which I have nurtured (perhaps intermittently) ever since, without as yet succeeding in articulating its relevance.

Some years later, when my own practical involvement in the application of interactive theorem provers was drawing to a close, that seed found a new place in my more philosophical aspirations.
I have struggled ever since to articulate the ideas which grew from that seed, and in this essay I try once more, focusing as tightly as I can on the core insights.

These swim against the dominant tendency which is evident in AI of emulating the capabilities of human beings, hoping that once reached they will be readily surpassed.
They do so under the inspiration of ideas about how reasoning and the knowledge which can be derived from it can be made both more reliable and more broadly applicable.
The grounds for the belief that this is possible do not come from observation of the ways in which human beings commonly reason, but rather from advances in mathematics, theoretical computer science and philosophy which have transformed our understanding of deductive reasoning and its limits within the last couple of centuries (after a couple of millenia in which progress faltered).

Though these new logical methods were theoretically and philosophically fruitful, and hinged around the invention of potent strictly formal languages and deductive systems, the application of these formal systems (rather than their use in the development of new theoretical disciplines) was severely limited by the complexity of the detailed proofs which they required.
The use of these systems in real world applications, or even in the development of mathematical theories, was impractical until the advent of the digital stored program computer.
Even with this assistance, in the form of brute computational capability, the support remained short of what was needed to realise the full potential.

This became apparent as GOFAI came up against the problem of `combinatorial explosion' and came to understand that intelligent heuristics were essential to success in finding deductive proofs of non-trivial propositions.

\section{Introduction}

When stored program digital computers were first invented their applications primarily concerned doing large amounts of information processing or computation with almost perfect reliability and at superhuman speeds.
They were accurate and reliable.

As their computational power grew their applications were extended progressively, and this sometimes involved attempts to achieve ends which were much less clearly defined, involving more complex instructions which could less certainly be relied upon to achieve the intended purpose.

The kinds of brute computational power exhibited by these early computers might at first have been thought signs of intelligence, since human skill in computation had certainly been presumed a sign of intelligence.
But brute computational power soon came to be distinguished from intelligence.

As I write, generative AI and Large Language Models have momentarily set a new standard for the unreliability of Artificial Intelligence.
Not designed or trained to be reliable repositories if knowledge, or to be capable of any but the most eleentary reasoning, their exposure to vast quantities of human knowledge enables them to perform in many main-line subject matters in an apparently authoritative way, while morphing in more esoteric areas into fantasy, and failing under the most modest interrogation to demonstrate any but the most shallow comprehension in subjects whose generally accepted facts they can replay.

It may not be so hard to improve on this.
LLMs have proven capable of using tools effectively, and tools such as more reliable ways of saving accurately and reliably knowledge acquired, or reasoning about that knowledge may not be difficult to supply.
The discussion in this essay may be thought of as concerning the use by such an AI with a tool which is capable both of storing knowledge and of deductive reasoning in the context of that knowledge.
The effect alleged would be to enable Artificial Intelligence which is \emph{oracular} in relation to logical truth.

Oracles may be thought of as having great wisdom, possibly derived from divine connection, but here I use the term more narrowly.
For the purposes of this essay the term ``oracle'' is used for something which is always truthful in answering questions, but doesn't always answer.

The oracle of interest here can be asked whether a sentence in a formal language is a \emph{logical truth} a concept which I will try to characterise, but which ultimately cannot be made absolutely definite.

The term ``Logical Truth'' is philosophically controversial.
In my usage of that term I stand on a limb, for my use is very similar to that of Rudolf Carnap,and is synonymous with the term \emph{analytic}, a concept central to Quine's repudiation of the philosophy of Carnap in the mid 20th Century.

Its not my purpose here to argue about the terminology.
Some might insist that my conception of logical truth should more properly be spoken of as set theoretic truth, and I do not intend to argue against that opinion.
Carnap, who until 1952 used the terms ``logical truth'' and ``analytic truth'' synonymously %
\footnote{as is explicit in section 2 of ``Meaning and Necessity'' \cite{carnap56}}%
, eventually accepted defeat and began to use the term ``logical truth'' for a narrower concept%
\footnote{W.V. Quine's noted a defect in Carnap's definition of analyticity in \cite{carnap56}, which followed closely a defect first seen in Wittgenstein's ``Tractatus Logico-philosophicus''\cite{Wittgenstein1921}.
Carnap's response appeared in the paper ``Meaning Postulates''\cite{carnap52} in which for the first time he separates the concept of logical truth from that of analyticity.}.

The term ``Oracular AI'' as used here, refers to what AI might in principle be able to achieve if furnished with an oracle for logical truth.

One of the purposes of this essay is to discuss how thus notion of logical truth can be made precise, to consider the difficulties in implementing such a decision procedure and to talk about the value of approximations which fall short of logical omniscience.

\section{Some Historical Background}

The stark difference between the reliability of deduction in mathematics and ways of discovering truth in other domains has been plain since the ancient Greeks began the transformation of mathematics into a theoretical science.
Axiomatic geometry.
The results thus obtained, particularly in the axiomatic development of geometry, were reliable and were to be gradually accumulated and were ultimately gathered together as the Elements of Euclid.

By contrast, those same ancient Greeks, when attempting to reason about nature and the cosmos were unable to establish durable findings, and would find many ways in which deductive reasoning, via reductio, could establish absurd and contradictory conclusions.

The differential success of deductive reason in these distinct domains was to be reflected in the two worlds of Plato's philosophy, of which only that of Platonic ideals was susceptible to truw knowledge, to be reached by reason alone.
Aristotle sought to rescue the applicability of deduction to what we would now call empirical science through his conception of \emph{demonstrative science}, which relied for itscoherence on the special truth assuring characteristics of the fundamental principles of each science.

Millenia followed in which this situation remained largely stable.
The modern conception of science originating in the scientific revolution toward the end of the renaissance resulted in philosophy being split into two camps associated with a primary emphasis on reason and observation respectively as the source of knowledge.
Empiricism retained the idea of reasoning from scientific principles, but insisted on observation and empirical experiment for the discovery and verification of the principles.


\chapter{Oracular AI (1)}

\section*{Preface}

\section{Introduction}

Alongside the enthusiasm for the largest of the Large Language Models initiated by the release of \emph{chatGPT}, the ways in which these models fall short of the ideals of General Artificial Intelligence have come into focus, and the discussion of the associated risks has been more intense, if not more informative.

LLMs were not intended to solve the problem of AGI, the are ``generative AI'', not designed as accurate or reliable stores of knowledge, or to have competence in reasoning or mathematics.
The following discussion is about what might be achieved if the capabilities of LLM's adapted to address some of desiderata around reliable retention of knowledge and reasoning about that knowledge.

The term ``Oracular'' AI is intended to suggest a kind of AI which can be completely relied upon to speak the truth, and which has a completely reliable deductive capability which exhibits that same kind of scale relative to the ability of homo sapiens as a supercomputers ability to do arithmetic calculations exceeds that of humans.

\subsection{Contextual Speculation}

The following speculations are not the point of this essay, but may help to make intelligible the ideas which follow.

Public speculation about how LLM technology might progress has include the following two very general approaches:

\begin{itemize}
\item Giving Large Language Models (LLMs) direct access to tools.
\item Integrating LLMs with other approaches to the development of AGI
\end{itemize}

Most of the intelligent beings which now exist are slow and unreliable by comparison with algorithms running on digital computers in very many kinds of problem.
There is no reason to expect that Artificial Intelligence will change that situation.
Sometimes there is an efficient algorithm and that is the best way to solve the problem.
Once an ``AI'' has access to such tools, then it will use them in preference to its own machinations.

Most of the great accomplishment, perhaps all, of the greatest intellects in history are but one stage in the development of knowledge and skills by an entire culture over long periods of time.
Large engineering enterprises involve very many intelligent designers collaborating to create and refine clear statements of requirements, design, coding and details of the physical and logical structures necessary to meet the requirement.
Most of these contributors will have specialised in some aspect of this process and could not effectively contribute in areas which are too far removed from their speciality.

Nevertheless, AI marches forward toward a goal of complete generality.
That generality might be thought of as being capable of specialising to any area given appropriate training, but even that would exceed the kinds of intelligence which homo sapiens exhibits, for our gene pool is diverse and different individuals have the aptitude to perform well in distinct domains.

The direction of movement which we perceive in some of the most complex feats of engineering design is toward better support for collaboration between even larger distributed teams.
The most prominent manufacture of digital hardware intended to support AI, Nvidia, provides an environment called ``the Omniverse'' in which a complete factory can be designed and simulated, making the most of AI, and through which a widely distributed development team can work together to advance the design.
In such an environment the behaviour of each part of the manufacturing system can be simulated, and the simulated environment can be used to train the software for robots intended to participate in the running of the factory.

As AI becomes capable of contributing to such projects it may at first be directly used by human designers to help them accomplish their part of the work, but later we may expect AI agent to act on a par with human beings, seeking help as necessary, subject to review by the normal processes.

Alongside the Omniverse, Nvidia promotes the idea of ``digital twins'', which are abstract computational models of physical systems.
These models may serve various roles throughout the life cycle of the systems, appearing at first before the construction of the physical counterpart, enabling comprehensive testing verification and mutli-faceted evaluation prior to manufacture, and subsequently tracking the state of their physical instances to facilitate control and maintenance.

\subsection{A Safe Zone}

One aspect of making AI safe is simply making it more reliable in distinguishing truth from falsity.
This does nothing to prevent malfeasance, but a large proportion of pre-AI safety concern addressed the prevention of accidental rather than deliberate harms.
A safe car is not one which will refuse to mow down pedestrians, it is one that will not do so unless its driver intends it, or is incompetent or culpably negligent.

Even this kind of safety is hard to guarantee, and potentially becomes harder as AI permeates the workings of the world.

It may therefore be comforting to know of important domains in which conclusively establishing the truth is feasible (and customary) and in which even greater assurance of truth can be realised with the help of computers and in the context of artificial intelligence.

This is the domain of deductive inference, and logical truth.
It is the purpose of this essay to mention some of the well known ways in which these truths can conclusively be established and to add some ideas which may be less widely appreciated.

If deductive inference were to be automated so well that very lengthy deductions could be reliably conducted and subsequently depended upon, then the context in which those deductions are undertaken is important.
First it is necessary for the meanings of the language (or languages) involved must be clearly and precisely understood, for reasoning and comprehension is otherwise compromised.
Second, it is necessary that the context in which deduction is undertaken is coherent, for otherwise a contradiction may be proven and from that contradiction any conjecture, true or false, will be derivable and proof will confer no confidence.

The verification of engineering design is an important potential application for AI powered deductive inference.
But the reasoning involved must make use of a large body of mathematical and scientific knowledge, it must take place in the context of 
mathematical models of the designed artefact and the context in which it is required to operate.


\subsection{Some Observations on Intelligence}

We know well enough how intelligent beings go about extending our knowledge of the world and applying that knowledge to various practical ends.

Two pervasive features are:
\begin{itemize}
\item The gathering of knowledge is cumulative and cooperative.
\item The knowledge is a shared resource mostly held in media external to the intelligent agents involved.
\end{itemize}

I suggest that this is unlikely to change with the advent of AGI.
So it it likely that the AGIs will be cooperating with other intelligent agents in the continued accumulation of knowledge stored in shared distributed media a large part of which is freely accessible to all.

These are some features of intelligence which I expect to persist as AGI progresses.

Nevertheless there will be important and substantial changes.
In the past, it has been fairly rare for a new area to be automated by computer without significant changes in how it is done.
A simple example of this is the very different expectations we have of the reliability and scale with which arithmetic computation can be performed one digital computers were available.
In many more cases, it is not just the greater reliability and speed which matters, something which was hitherto infeasible may be rendered commonplace.

Another area in which computational machinery has outstripped prior capabilities is in memory.
Very large quantities of information can now be reliably stored, perfectly retrieved, swiftly searched and processed in some way.

In these terms, the current crop of LLM's are a regression.
They were not intended to perform well in any of those ways, they were intended to process languages, and having exhibitied surprising ``emergent capabilities'' are now sometimes judged as if they were intended as AGI.

Much of the effort invested in AI research aims to replicate the kinds of intellgence exhibited by humans, rather than aiming at superior performance in areas of practical importance.
Though early in the history of AI research the automation of formal logical theorem proving was not only an important domain of AI research, but was considered by some to be the route to a general intelligence, it later was outflanked by simulating the brain as neural nets and common sense reasoning became the aspiration.

In the following speculations I am concerned with how to enable GAI which is as reliable as possible in all domains, approaching this by focussing on the kind of reasoning which we can reasonably expect to be made wholly reliable.
This includes reasoning in mathematics, science and engineering.

In these domains, deductive reasoning has a very important role.
It may be said to be almost the whole of pure mathematics.
We may also speak of the nomologico-deductive model of science, in which the evauation of scientific hyotheses or the application of accepted theories is undertaken by deduction from those theories and the particulars of the case in hand.
Engineering is then merely a collection of application of science in which the process of verification of a design against some statement of desiderata is one of logical deduction.


My aim here is to sketch a tool for use by LLMs and other AI as a way of guaranteeing that the kinds of deductive reasoning which they might effectively deploy can be guaranteed sound.
The importance of this cannot be underestimated, due to the difficulty in containing infidelity.
Once a falsehood has been proven a contradiction is not far away, and armed with a contradiction all proofs whether of truths or of falsehoods become very short and completely worthless.

My description of the proposed logical tool will come in several stages, growing more complex as additional desiderata are introduced and address.

In the first instance the tool could simply be the logical kernel of en existing Interactive Theorem Prover, which for reasons which I hope to make clear should I suggest support formal deduction in the variant of the HOL logic devised at the University of Cambridge, originally for reasoning about digital hardware.


\subsection{Alternate Paradigms for System Development}

The approach to systems development exemplified by digital twinning and the Omniverse centres around computational simulation, accelerated and enhanced by AI.

This is an effective approach which we may expect to be progressed and advanced.
It retains the limitation which has always attended verification by testing, notwithstanding the scope for testing coverage improvement by AI.

It a very sophisticated for of informed guesswork.

The best way to ensure compliance of a design with a requirement is by logical analysis, which has been a distant ideal for decades.

\subsection{Language, Knowledge and Models}

Mathematics, science and engineering design and implementation involve a large body of knowledge presented in many distinct languages as well as the statements of requirements and descriptions of design and implementations, construction of one or more formal or abstract models of the

\section{What is an Oracle?}

The word has a variety of usage, so best to be clear about what is intended here.
In ancient Greece it was a mouthpiece for deity, and therefore had not only wisdom but any other characteristic one might attribute to someone with a direct line to God.

Skip, doubtless, many intervening subtleties, and arrive in the twentieth century not so long after mathematicians got into logic (a century is nothing in the scale of these things) and we find that the notion of oracle gets a place in recursion theory, i.e. the theory of effective computability.

In that theory, after the concept itself was precisely and convincingly defined, the next step (for which purpose the concept had been introduced) ws the demonstration that not all numerical functions were effectively computable.

This problem was addressed through the idea of a decision procedure for a set of natural numbers, and it was shown that some sets were not effectively decidable, and hence that their decision problem was ``unsolvable''.
It further transpired not all unsolvable sets were equally unsolvable, but that they could be classified according to their degree of unsolvability.
These degrees were defined through the concept of reduction, in which the decision problem for a set A is reducible to that of a set B if, given an oracle for set A the decision problem for B could be soled.
The notion of oracle for a set A is quite simple here, it is simply something which will tell you of any number whether or not it is in the set A.

There are no vague notions of wisdom in play here, calling something an Oracle for B just tells you that it has the answers to a particaular set of questions.
The idea of Oracular AI discussed in this paper is primarily that of an AI, which is, over some well defined class of problems, able to give the correct answer.

I have to make one retrenchment to that, in acknowledgement of the incompleteness result first discovered and proven by Kurt Godel.
There is also a related issue concerning semantics, which I suppose we might think connects back to Tarski, that the set which I will seek to characterise here as the domain in which Oracular AI might be an Oracle cannot be precisely defined.

With those two caveats the aim of this essay is to argue that corresponding to Hume's ``fork'' there is a domain within which we can reasonably expect AI's to be Oracle's.

\chapter{Formal Abstract Modelling for General Artificial Intelligence}

\section{Preface}

``General Artificial Intelligence'', as is now termed since the ``Artificial Intelligence'' lost its cachet, ought we hope, if not at first, to surpass humanity in the reliability of its memory and the certainty of its reasoning.
The latest wave of excellence fails miserably on both.

Large Language Models (LLMs) have certainly cracked language, which is, as some modern philosophers have pointed out, complicit in the incoherence of common discourse.
Can we now assume that when natural language discourse is required, an LLM technology will supply it, and seek deductive proficiency through simpler more precise notations, or relegate concrete syntax to intermediaries and approach deductive closure through exclusively abstract structures?

That is the possibility which this essay explores.

Despite their present ascendency, Large Language Models have not been the star achievers in recent AI R\&D.
The mantra that proficiency in AI is crucially dependent on the use of very large bodies of data for training neural nets has been disregarded in several of the most impressive achievements of Deep Mind the London AI startup now acquired by Google's parent Alphabet.

First with Chess, then Go and latterly alphafold, Deep Mind showed that in in simple worlds governed by a modest collection of unambiguous rules, neural nets relegated to heuristics could achieve proficiency as the system learns by playing itself, without needing role models to mimic.
Though their complexity is unbounded, the abstract structures of mathematics and their use in the more elaborate mathematical models created by science and the vastly more complex constructions of engineering (especially in digital semiconductors), are susceptible to the same methods, which place Monte-Carlo tree search in pole position with crucial subordinate roles for neural nets (which need no linguistic competence).

Thinking of deductive inference about abstract models as being a precise and simple game, it is merely a speculation on my part that it will be susceptible to similar methods as chess, go and protein folding, and the realisation of that aspiration is not something to which I can contribute, except by the attempt which I offer here to clarify the nature of the game.

There are many different ways in which a broadly universal game of deductive modelling can be framed, and what I offer here is just one of them.

I have been trying to tell this story for a long time, without success.
I don't know whether the story can be told without talking about the history of the ideas in which it appears, and, not being a historian I am likely to be scornful of my attempts to do that.
But i have attempted to tell the story without going into the history, and I cannot see how it can be understood.

The fact is that the ideas which I want to present under this heading are convincing to me only because of the history of ideas which has lead me to them, though my own particular perspective on them us substantially influenced by some of the most recent developments in them (by historical standards, not by the hectic standards of AI research) and by my own professional experience in trying to apply these ideas in the development of highly assured information processing systems in relation to security and safety concerns.

This essay is an attempted compromise, in which I try again to present the ideas in the context of a minimal presentation of those histotical precedent which seem most crucial in illuminating and motivating the approach which I am suggesting.

\section{Introduction}


Nearly half a century ago John McCarthy published a paper \cite{mccarthy1981} that divided the problem of artificial intelligence into two components or domains: an epistemological component, which concerned the representation of knowledge and the rules of reasoning, and a heuristic component that optimised search and pattern matching to permit the solution of  otherwise intractable problems.

In this essay I discuss a further division of the epistemological side of McCarthy's division.

McCarthy's ``epistemological part of AI'' can, I claim, profitably be advanced by separating our that part which concerns purely deductive reasoning.
In the established epistemological terminology, this is the problem of the \emph{a priori}.
The contention is that all deductive reasoning, even in relation to knowledge \emph{a posteriori} can be undertaken and validated in an abstract model with the standards of soundness and consistency which are realisable in that domain, and that an approach which separates out  abstract modelling as a fundamental shared component of all empirically applied AI is not only theoretically elegant but practically effective.

The separation of the whole of this domain yields a general facility for \emph{abstract modelling}, in which any aspect of empirical concern can be addressed by the use of an abstract model.
This is a particular way of thinking about and implementing the use of formalised mathematics and logic for modelling the phenomena in the real world, and thus a critical element of the problem of knowledge representation, and a means to very high levels of rigour and generality in the automation of deductive reasoning.

\section{A First Sketch of The Foundation}

The formalisation and mechanisation of logic and thence of mathematics was anticipated by Aristotle and Leibniz, but only became plausible after Frege devised his \emph{concept script} and attempted to demonstrate the reduction of mathematics to logic.
His own system was shown to be flawed by ``Russell's paradox'', but Russell then went on to devise and apply a type theory for that same purpose.

Frege's mantra was that:

\begin{center}
Mathematics = Logic + Definitions
\end{center}

Which has fallen foul of much philosophical disagreement about exactly logic is, but it was generally accepted that \emph{in practice}:

\begin{center}
Mathematics = Set-theory + Definitions
\end{center}

Where \emph{in practice} means that almost all of mathematics can be derived in the axiomatisation of set theory known as ZFC provided that definitions of the concepts of mathematics in terms of sets are accepted, and that such definitions can be supplied which render the resulting formal theory strictly the same as accepted but less formally conducted mathematics.
We have to say \emph{almost} all here because of the well known incompleteness results of G\"{o}del, but I have been unable to locate any incompleteness which is relevant to any but the most esoteric studies in logic or mathematics.

Henceforth any logical system (and there are many of them) which is capable of formalising logic and mathematics in a similar way I call a \emph{logical foundation system} (often eliding the `logical'), and I note here the importance of the possibility of proceeding in such a foundation system exclusively by the use of definitions of concepts rather than by the introduction of additional axioms or rules of inference as crucial to the integrity of the system, ensuring as it does, that if the foundation system itself is logically consistent the so will be the extensions for modelling purposes undertakem exclusively by definition.\footnote{
Though later we will admit a useful generalisation of the concept of definition as that of \emph{conservative extension}.}

Russell's \emph{Theory of Types} was a complex logical system due to ramifications thought necessary in the type system.
It was not long before Ramsey suggested its simplification into the \emph{Simple Theory of Types}%
\footnote{First published as a fully worked out formal system in 1928 by Hilbert and Ackermann "\cite{hilbert1928}.}%
, arguing that the ramifications were not in fact necessary.

In the 1930's logicians found it necessary to clarify the concept of \emph{effective method}, and in 1936 four simple but general ways of describing computational methods (algorithms) were published.
These were Stephen Kleene's ``recursive functions'', Alan Turings's machines, Post's production systems, and Church's lambda calculus.
\footnote{The relevant papers may all be found in ``The Undecidable'' \cite{davis65}}
These originally devised to demonstrate the existence of unsolvable problems, but Church's lambda calculus was to prove influential in the design of computer programming languages and as the basis of foundations for mathematics which through their simplicity and power would be embraced by those computer scientists and engineers who were interested in reasoning about computer programs and digital hardware.

Closely related to the lambda calculus, in which functions are defined using a variable to represent the argument, and an expression involving that variable for the value of the function at that point, an even simpler but similarly expressive notation was devised which made no use of these ``bound variables'', and there was much research in how these systems might form the basis for mathematical foundation systems.

A major figure in this research was Haskell Curry, whose names were to be enshrined in the terminolgy of functional programming languages (which are closely related to the lambda-calculus), but his work on mathematical foundation systems based on combinators was to prove less influential.
For our story it is the ideas of Church which are more significant, and after proposing a foundation system based on the simple (type-free) lambda-calculus which proved to be logically inconsistent, he added to the lambda-calculus a type system similar to that of the Simple Theory of Types resulting in a variant of that system which would prove influential in computer science.

Church's formulation of the Simple Theory of Types \cite{churchSTT} was to be taken up in research into the formal verification of computer programs and digital hardware.
The resulting elaboration of Church's system was the product of a line of development which began with a logical system devised by Dana Scott for reasoning about programs and programming languages
\footnote{First circulated in 1969 but not published until much later as \cite{scott1993type}}.
This became the logical basis for a line of software systems for computer assisted proof construction and checking intended for reasoning about programming languages and programmes.
The first system was called LCF for ``Logic for Computable Functions'' and after several steps, including migration from Stanford to Edinburgh and then Cambridge (UK) was re-oriented toward hardware verification and adapted to support a variant of Church's STT by Mike Gordon.%
\footnote{For a fuller account of this story see Gordon\cite{gordon2000lcf,plotkin2000proof}}
This logic and the software developed to support reasoning in the logic were both called HOL, or Cambridge HOL if the context left room for doubt.

As far as the Cambridge HOL logic was concerned the principle new feature was its inheritance from LCF of a polymorphic version of the type system.
In Church's exposition of his Simple Type Theory variables appear in the types, but these are syntactic variables belonging to the metatheory, not variables in the object language (STT).

The logical system put together from these two principle lines of influence, after some refinements settled with the following characteristics:

\begin{itemize}
\item From Church's STT:
  \begin{itemize}
  \item The abstract syntax, semantics and deductive system of STT is the base to which the following enhancements are applied.
  \end{itemize}
\item from Edinburgh LCF:
  \begin{itemize}
  \item Hindley-Milner polymorphism:
    
    The practical consequences of the strict type system in STT become onerous once the system is used beyond the confines of arithmetic, for the theorems governing the operation of structures like sequences would need to be proven again for every new kind of object from which lists need to be constructed.  Including type variables in the object language allows for a single theory of lists which can be instantiated to any type for which lists are needed.
  \end{itemize}
\item New in Cambridge HOL
  \begin{itemize}
  \item Definition of Type Constructors:
    
    In the development of mathematics, for example in the sequence of number systems leading up to (and beyond) the complex reals, each new kind of number is constructed from earlier kinds of number and has a new type.  The type is chosen for convenience in defining operators with the required properties, but is not a part of the abstract structure intended.  The definition of a new type which delivers the required structure while concealing the manner in which that result was obtained, is valuable in managing the complexity and intelligibility of layer upon layer of required abstractions.
  \item Definition of Constants:
    
    In many logical system \emph{definitions} are syntactic, permitting a concise expression to stand in stead of something more complex or extended.
    In Cambridge definitions extend the underlying logical system by adding new constant symbols into the language together with an axiom which constrains the interpretations of the constant.
    Simple definition in this form are \emph{conservative}, permitting no new theorems to be derived which do not mention the new constant, hence preserving the consistency of the logical system.
    In Cambridge HOL, for the sake of improved abstraction (or the avoidance of over-specification), the mechanisms which permit new constants to be introduced allow weak constraints.
  \item Sequent Calculus:

    The deduction system is reorganised as a sequent calculus rather than the more traditional Hilbert style inference system used in Church's STT, and for various other pragmatic reasons is equivalent to but distinct from the details of that system.

  \item Theory Hierarchy:

    It is essential for the tool supporting the logical system to keep a record of the definitions which have been entered, to ensure that consistency is not compromised by introducing the same constant more than once with conflicting definitions.
    In the implementation of Cambridge HOL this is done through a heirarchy of theorems which keep a record of the constant definitions which have a occurred in a way which controls the scope of the defintions and permits many different developments of logical theories to be saved in the same logical database.
    This may also be used to save theorems proven in the relevant context.
  \end{itemize}
\end{itemize}

There are many more original aspects of the HOL system which enabled the definition of constants and the construction and checking of formal proofs in this logical system, but these do not concern us here.
It is the abstract language, its semantics and its inference system which are here advocating for more general application, and not any particular software to support those applications.

The principle merits of this system so far as the role here proposed for it are as follows:

\begin{itemize}
\item It is a ``mathematical foundation system''
  \begin{itemize}
  \item It replicates Frege's prescription for the logical structure of mathematics as obtained by logic from the definitions of the mathematical concepts,  and the suitability for the formalisation of mathematics exhibited first by Russell's Theory of Types, in a neater and more practical way.
  \item It permits the derivation of mathematics and of applications of mathematics by conservative extension, subject only to a suitable axiom ensuring that the ontology is sufficiently rich.
    In Church this is simply an axiom of infinity asserting that there are infinitely many individuals, supporting the development of arithmetic and analysis, which for some of the more exotiv mathematical theories (e.g. parts of Set Theory) might not suffice, but which can readily be accommodated by the use of a stronger axiom of infinity, possibly one asserting that the cardinality of the individuals is inaccessible.
    \item only the abstract syntax and semantics of the system is adopted, and this abstract syntax because of its simple and higher-orer structure is ideal to replicate the abstract syntax of arbitrary language using constructors whose definition captures the semantics of the language, hence 
    \end{itemize}
\end{itemize}

\cite{gordon1989mechanizing,birtwistle2012current}

\cite{cohnPIHV}
\cite{mccarthy2022artificial}

\section{A History of Modelling}

\subsection{What is a Model?}

The aim of this document is to talk about formal abstract models, since it seems possible that they will be the next important development in the evolution of models and their use.

In order to give a sense of the importance of such models I aim to sketch the history behind them.
This might involve a bit of stretching of the term, but it already serves a great variety of purposes so I'm sure it will cope.

It may be useful to characterise the sort of model which are of interest here, which are those which consist in something representing information about some other thing of interest.
This encompasses linguistic descriptions as well as information strucures which long pre-date the evolution of language.

Since the beginning of life on earth, even the most simple forms of life have found it necessary to sense their environment, and to adapt their behaviour according to what those senses reveal.
In the simplest cases, the senses will serve to detect potential foods and suspected predators, in order to secure the former and avoid the latter.

This involves some kind of representation of the environment being created by the sense organs, to be transformed into that kind of representation of necessary effect which will secure the best interests of the organism.
In those organisms which possess a nervous system the representation may be as patterns of firing in neurons, and the transformation will be undertaken by a neural net before some other pattern of firings initiates appropriate motor functions.
In yet more primitive organisms, the necessary presentations and processes are likely to be chemical, and the representations less likely to be thought of as models even though serving similar purposes.

\subsection{}

My aim here is to describe a game, which I believe to be susceptible to automation, but which because of its algorithmic complexity depends upon the use of heuristics best supplied by intensively trained neural nets.

The game is the prediction of real world events by the use deduction in the context of abstract models of the relevant real world phenomena.
It also includes the elaboration of the pre-requisite logical and mathematical theories which provide a basis for the construction of the relevant scientific and engineering models.
The whole provides potentially a substrate for the automation of engineering design.

A bare presentation of the game as proposed would I expect be unconvincing and opaque, so I propose to come to the details via some discussion of the history of abstract modelling, or of those kinds of process which have lead the particular conception of abstract modelleing which is presented here.

This leads to a minimal presentation, followed by the consideration of a certain number of desirable elaborations.
Throughout, an understanding will be supported by the presentation of certain philosophical positions, which are offered primarily as useful in supporting this kind of enterprise (i.e. on a pragmatic basis) not as demonstrably superior to the many alternatives.

\section{Some History of Modelling}

\subsection{Pre-History}

Throughout the evolution of life on earth it has been necessary for living organisms to take cognisance of their surroundings in order to take those actions which will secure nourishment, avoid predators, and effect reproduction.
When an organism takes in information about its surroundings and uses it as a basis for its actions in the near future, it is not wholly unreasonable to consider that it has thereby acquired some sort of useful model of its environment, and that is will use that model to predict which courses of action will best serve its need to grow and replicate.
For organisms with a nervoud system we might call these neural models, for single celled organisms perhaps chemical ``models'' (stretching the point beyond its limits perhaps).

As the nervous systems of organisms evolve to greater complexity we may imagine that the complexity of the models grows with that of the nervous systems.

Though these models serve a vital purpose for the organisms, we cannot reasonably say at this point that the models are used deductively.

Deduction can only be considered in the context of \emph{propositional language}, which is that kind of language in which claims can be expressed which will be either true or false according to how things are, and whose assertion may thereby communicate knowledge (about how things are) from one to another.
Such language is probably coeval with homo sapiens, who appears at the end of a million years in which the size of the brain grew quite rapidly.
Any faculty which demands special mental faculties must evolve alongside both the physical and mental adaptations necessary to support it.
Those regions of the brain which are adapted to support language could not have appeared without the languages faculties they support.

With propositional language comes deduction, for even the most elementary language must embrace taxonomic classifications.
If your language has a word for horse and one for animal, then you cannot be said to understand the language unless you know that horses are a kind of animal and are able to deduce from the knowledge that something is a horse the ``conclusion'' that it is also an animal.
Admittedly this kind of deduction is neither formal nor self-conscious.

The advent of language enabled oral culture which would ultimately empower mankind in the transformation of the world.
The evolution of culture must at first have been quite slow, since the first evidence of animal husbandry and then agriculture date back only 13,000 years, 200-300 thousand years after the evolution of homo sapiens.

The first stage in accelerating cultural evolution was the invention of the written word, which, in the form of clay tablets, was probably of modest impact, but the technology of writing and communication has itself evolved and many important cultural advances have been facilitated by new media for storing, and sharing knowledge.

Papyrus was invented by the Egyptians around 3000 BC, and was a medium for the preservation of Egyptian culture, including geometry and arithmetic.
It does not appear to have been adopted in ancient Greece until about the 8th century BC, but then catalysed the flourishing of Greek culture which then shaped the development of the Western and ultimately the whole world.
This began with literary works such as the Odyssey, but of greater interest to us here is the transformation of Mathematics which began around 600BC.

\subsection{Greek Mathematics and Logic}

The period between 600 and 300BC was one in which major advances were made in sophistication with which reason could be applied to modelling the world.

It is this period that mathematics is first systematised as a deductive science, and in which we first see deduction talked and theorised about.
The systematisation of Geometry now known as Euclidean Geometry set a new standard of rigour allowing more elaborate deductive reasoning to be undertaken with great reliability.
This standard was not to be improved upon for the next two millenia, and was to be the envy many philosophers who wished for the emprimateur of logical certainty to be attached to their conclusions.

\subsection{The Organon and Demonstrative Science}

It is in Aristotle's ``Organon'' \cite{aristotleL325,aristotleL391} that the applicability of logic to the understanding of reality was first to be made explicit, and it was in Aristotle that we see the first attempts at formally codifying the rules of sound deductive reasoning.

In his Organon, six books forming his contribution to logic, Aristotle presents the idea of Demonstrative Science in which necessary conlcusions about the world are deductively derived from the fundamental principles of each science.
In these volumes the nature of deductive reason is analysed using the idea of the syllogism.
This is the first known metatheory concerning deduction, and it was to remain dominant until the end of the 19th century.

\subsection{Bacon's Novum Organum and Empiricist Science}

The next major development in the methodology of science, the ways in which we build models of the world, came in the Renaissance and is presented in writings of Francis Bacon, notably his \emph{Novum Organum} \cite{bacon2017novum}.

Whereas Aristotle sought to adapt the axiomatic methods of mathematics to Science, looking in greater depth at the process of deriving conclusions from the principles than that of establishing the principles, Francis Bacon was more concerned with how the principles could be established, and in this the differences between mathematics and science wer more marked.

He described a four stage process as follows:

\begin{itemize}
\item Collect data about the phenomema of interest.
\item Look for patterns in the data.
\item Formulate a hypothesis.
\item Test the hypothesis by observation and experiment.
\end{itemize}

This is the new nomologico-deductive method, in which the novelty is primarily in the establishment of the laws rather than their application (though testing hypotheses is similar to application, as involving deductive inference from the hypotheses).

Almost contemporary with Bacon, and with more substantial credentials as a scientist and engineer (as well as a philosopher), Galileo Galilei (1564–1642) spent most of his life trying to give structure to the new science, in which Aristotelian science was largely cast aside.
Gone was the teleological emphasis on ``final cause'', and replace the five fold ontology of ether, fire, air, water and earth with a system built exclusively on matter, introducing a mechanical system which came to dominate modern science.

Modern philosophy from this point divides into two camps, empiricist like Bacon who emphasised observation and experiment as the source of worldly knowledge, and those who sought knowledge through reason.

\subsection{Leibniz and The Mechanisation of Science}

\subsection{Hume's Forks and The Synthetic A Priori}

\chapter{Evolution and Logic}

% Evolution is the mechanism which turns chaos into order,
% not so much by design as by the accident that chance will eventually create complex structures which thrive.
% It has many forms, some very different to what we call Darwinian evolution, and by consideration of how those forms of evolution have themselves evolved we can speculate about the future of evolution and of intelligence.
%My own speculations on this topic say something about epistemology and logic.

\section*{Preface}
\phantomsection

\addcontentsline{toc}{section}{Preface}

This preface is really immaterial, but there might be some point in saying \emph{why} its immaterial.
Here's Bertrand Russell more than a century ago on a theme that remains current.

\begin{quote}
  \emph{That Man is the product of causes which had no prevision of the end they were achieving; that his origin, his growth, his hopes and fears, his loves and his beliefs, are but the outcome of accidental collocations of atoms; that no fire, no heroism, no intensity of thought and feeling, can preserve an individual life beyond the grave; that all the labours of the ages, all the devotion, all the inspiration, all the noonday brightness of human genius, are destined to extinction in the vast death of the solar system, and that the whole temple of Man's achievement must inevitably be buried beneath the débris of a universe in ruins — all these things, if not quite beyond dispute, are yet so nearly certain, that no philosophy which rejects them can hope to stand.}

Bertrand Russell, Mysticism and Logic \cite{russell17}
\end{quote}

Much has changed, in the world and in philosophy, since Russell penned those words, but majority opinion is probably still with Russell's sentiment (if not his way of putting it), and its not even controversial, as he says (in other words) resistance is futile.

I was reminded of Russell's words just as I resolved on this essay, and the contrast between that point of view and the perspective intended for the essay seemed a good way to highlight that perspective.

Russell talks of the death of the solar system, and of a universe in ruins.
The former would be a consequence of the Sun progressing through the observed life cycle of similar stars, which I don't doubt.
It will not be happening soon, not even in evolutionary timescales, so it doesn't seem wholly irrational to hope that humanity or its progeny will already have ventured to havens beyond our solar system before it happens.
I have something to say about how that might happen.

The idea of a universe in ruins has no support which I know of other than the second law of thermodynamics.
I have no problem with the practical applications of thermodynamics, which as far as I am aware mainly talks about entropy changes in closed systems.

I have struggled to understand the concept in the context of the second law, and struggled to comprehend the evidence offered for the second law, and I have neither understood nor believed.
So this essay is written as a speculation about how evolution has and will continue to progress, by someone who has no expectation that this will ever come to an end (and is agnostic about whether it ever had a beginning).

I might add a more comprehensive scepticism about the plausibility about the science of the very greatest extremes.
I doubt that scientists will ever realise perfect models of the microscopic structure of the universe, or about the gross structure of the universe, or about what happens close to the supposed singularities in relativistic models.
However far we have been able to peer with our best instruments, there may yet be something beyond which defies our expectations.

As to the significance of these beliefs to this essay, it is small and mainly psychological.
The timescales in which the second law might be expected to yield heat death are beyond those in which my speculations are credible even to me.
So far as I know, if I did believe the second law of thermodynamics, I would still think it ``academic'' and write the same essay.
Nevertheless I reject Russell's perspective.
Russell has made important contributions in some of his other works to the ideas presented here, and has been for me and many others over the last century, an inspirational figure.
But on this I demur.

\section{Introduction}

There is at the centre of this essay a very simple idea which is not mine.
I believe its importance is much greater than has been recognised, and this essay is intended argue the case for that belief.
More important than the belief itself is its consequences for the way in which artificial intelligence (and synthetic ecology) is approached and progressed.

Having said that, the ideas in the essay are not intended to be a direct contribution to those fields, in which I have no expertise.

The central idea is very simple, and apart perhaps from the details of presentation and the conception of significance, is not original.
A naked presentation could not convey the importance which I attach to it, and this document is primarily intended to provide a backdrop against which it's importance can more clearly be perceived.

That backdrop is evolutionary.
It is a story of the many kinds of evolution which have brought us to this moment in time at this point in space, and a speculation about some of the evolutionary processes which will lead us forward and the product of those very different evolutions.

My own reading of certain aspects of evolution provides a basis for the speculations to follow about how evolution will itself evolve in the future and how that will shape the growing sphere of influence which homo sapiens has in the universe.
From those speculations, some conclusions are drawn about the representation of knowledge and the automation of reasoning which lead to a a sketch of how those requirements might best be satisfied which form the kernel of this essay.

\subsection{Evolutionary Themes}

It is evolution which has shaped life on earth.
But no single kind of evolution can explain it all, and the purpose of this essay is best served by a varied diet.

If we are to consider the future trajectory of evolution, it will be necessary to identify some of the characteristic which those different kinds of evolution have in common, as features most likely to be preserved into the future, as well as to consider those aspects of contemporary evolutionary processes which are most closely aligned to context which will not be preserved into our futures.

Homo sapiens is soon to become and interplanetary species, perhaps then interstellar.
This is such a profound transformation in context that it must surely have impact not only on what evolves, but on how evolution works, both in the minutiae of the reproductive processes, the ways in which variation occurs and the selective pressures which guide the process.

So, what is evolution?
Some kinds of evolutionary thinking go back as far as the ancient Greeks, but modern conceptions of evolution date back to Darwin, who offered evolution as an explanation of the origin of species.
As science has progressed the dominant conception of evolution has also advanced, the development most conspicuously punctuated by the ``modern synthesis'' of which the most important aspect, though by no means the whole, was the incorporation of Mendelian genetics.

For our purposes its best to start with a simple model, since we seek an idea of evolution which will have broad scope.

Darwinian evolution may be characterised in three points:

\begin{itemize}
\item It concerns the evolution of species, which are populations of interbreeding organisms, and it is the species which are said to evolve.
\item As the population of the species reproduce, the offspring share many of the characteristics of the parent or parents, but they are not identical.
  Variation is thereby introduced.
\item Different variations may impact on reproductive success.
  This also happens in selective breeding by farmers, but in nature no deliberate selection is involved, and the differential success in reproduction is attributed to ``natural selection''.
\end{itemize}

It is generally emphasised that the variation which occurs is random and that the selection process is ``natural''.
These emphases are necessary because the intention is to assert that evolution, even of the most complex organisms, can work without any intelligent intervention.
Neither in creating appropriate variations, nor in weeding out those variations which are not advantageous.

Notwithstanding that point, even though intelligent intervention may be inessential, it may still be reasonable to consider a progress evolutionary even if some intelligence is involved in selection (as is arguably the case in sexually reproducing species) or even in variation (which genetic engineering makes possible).
For the sake of talking about a broad swath of different evolutionary processes, I therefore offer a broader conception of evolution which encompasses Darwinian evolution.

One of the challenges which I address in this reformulation is to obtain a conception of evolution which includes the processes which took place in the ``primordial soup'' leading to the first living organisms, and also to include the continuation of evolution when we have artificial intelligence incorporated into possibly inorganic systems which are capable as a whole of self-reproduction, but capable of undertaking design modifications in the process to optimise effectiveness.
Such systems will still exhibit progress which reflects the differential success of the different design decisions, and we can expect the best design decisions to become dominant among the resulting populations.

The single shared feature which I can see here is change to a population arising from differential replication of its members, which process is unlikely to progress indefinitely unless the replication is not invariably perfect.

\section{How We Got Here}

\section{Some Places We Are Going}

\section{Knowledge Representation and Deductive Closure}


\chapter{Synthetic Epistemology for Oracular AI}

% A philosophical perspective on evolution, and an evolutionary approach to philosophy.

\section{Introduction}

Homo sapiens is, as yet, ignorant of any intelligence in the universe but his own.\footnote{With apologies to theists.}
Our concept of knowledge, our epistemology, and our understanding of adjacent subjects such as language and logic, is shaped by the history of our origins.
Many now believe that intelligent artefacts will soon be created, and to a great extent those involved in their development may conceive their task as that of, first mimicking, and then surpassing, human intelligence.
But intelligence, living or synthetic, might have come to us from another part of the universe, and might then have originated in quite different ways.
When we design intelligent artefacts, perhaps intended primarily for service in progressing the exploration of the solar system and the galaxy beyond, very many aspects of the environment which nurtured human intelligence will be irrelevant.

In early approaches to the design of intelligent artefacts, it may be advantageous to rethink the foundations of epistemology which inform our thinking about knowledge and will shape the architecture.
In doing so, we have a freedom which we may not feel when engaged in the philosophical analysis of what we humans call ``knowledge''.
It is the freedom to consider what purposes knowledge might serve and how it might best serve them, and to build, from its foundations a synthetic epistemology with those purposes in mind.

That is the enterprise which I am exploring in this essay.
Though that perspective on the project is fresh, it is an enterprise which has occupied my mind for many decades while I have sought a way forward.
I hope that the essay I write here will seem too simple to have gestated for a lifetime.

\section{Oracular AI}

In the headlong rush toward what we used to call Artificial Intelligence, but now call General Artificial Intelligence, various capabilities in which computers are greatly superior to human beings seem to have passed us by.

The central place which neural nets now take, in which structured knowledge representation is abjured in favour of the digital analogue of synaptic connections and weights results in intelligence which is better at \emph{knowing how} than \emph{knowing what}, i.e. procedural rather than declarative or propositional knowledge.

Procedural knowledge predates declarative knowledge in the evolution of humanity, and as culture began to evolve this was a limiting factor.
In default of declarative language, the transmission of knowledge can only take place between individuals in close proximity.
With verbal language the dissemination can at least encompass a group of people not quite so close by, and with written language knowledge can be transmitted across large distances and over substantial periods with greater reliability.
In addition, a written record permits the gathering or application of knowledge to become an extended collaborative enterprise.

The ability of human beings acting in concert to accumulate a shared body of declarative knowledge, gradually extending into science, technology, and engineering and extending the control we exert over our environment to the benefit of humanity is something we should expect an AI to contribute to.

As the scientific knowledge of humanity has progressed, the languages which we use for science become more refined, precise and expressive and our understanding of how to reason reliability and soundly in the application of that knowledge advances.






Oracular AI is that kind of Artificial Intelligence which is oriented 

In making a connection between AI and synthetic epistemology, I have been been motivated by more than one kind of foundational thinking.
Foundational thinking is for me the quintessence of philosophy, and the synthetic epistemology I discuss here may be thought of as at a polar extreme to the idea of `epistemology naturalised'.

This philosophical synthesis is to be built from a base which is neither human nor even living, but we must begin with some sense of purpose, in default of which no criterion for success could be mooted and no sense of direction would emerge.

I therefore begin thinking of with a diverse collection of intelligent agents which both collaborate and compete in seeking to replicate themselves or some larger system of which they are a part across the cosmos.
Their success in so doing is contingent on their ability to gather knowledge and use that knowledge to plan and optimise their continuing proliferation.



There have been many substantial changes in the dominant paradigms for Artificial Intelligence over the last three quarters of a century.
At one stage, which is sometimes now referred to as GOFAI%
\footnote{Good Old Fashioned AI}, %
epistemological concerns were more prominent than they now are.
Typical epistemological concerns from that period included the representation of propositional knowledge, the ways in which one can use deduction to infer new propositional knowledge from old, and how such deductive techniques could be used for solving a broad range of problems.

Though these methods were capable in principle of solving the broad range of problems which were then thought characteristic of intelligence, they ultimately failed once large problems were tackled due to the combinatorial intractability of the relevant search spaces, and it was recognised that intelligent levels of performance on these problems depended on the ability of intelligent people to noarrow down intuitively the search space sufficiently to have a reasonable success rate in finding a solution.

Alongside the impasse created by the complexity of search spaces, problems whose solution was essential for some some applications of AI, but which were perhaps not previously thought of as requiring ``intelligence'' began to come to tne fore.
And example was the problem not of understanding the work but simply \emph{seeing} the world and being able to identify its principle features, the problem of computer vision.
This was not a problem for deduction, the things we perceive in our surroundings are not deductively inferred from the detailed sensory inputs reaching the brain from the sense organs.
This is indeed an epistemological problem, but of an entirely different kind.


When stored program digital computers were first invented their applications primarily concerned doing large amounts of information processing or computation with almost perfect reliability and at superhuman speeds.
They were accurate and reliable.

As their computational power grew their applications were extended progressively, and this sometimes involved attempts to achieve ends which were much less clearly defined, involving more complex instructions which could less certainly be relied upon to achieve the intended purpose.

The kinds of brute computational power exhibited by these early computers might at first have been thought signs of intelligence, since human skill in computation had certainly been presumed a sign of intelligence.
But brute computational power soon came to be distinguished from intelligence.

As I write, generative AI and Large Language Models have momentarily set a new standard for the unreliability of Artificial Intelligence.
Not designed or trained to be reliable repositories of knowledge, or to be capable of any but the most eleentary reasoning, their exposure to vast quantities of human knowledge enables them to perform in many main-line subject matters in an apparently authoritative way, while morphing in more esoteric areas into fantasy, and failing under even gentle interrogation to demonstrate any but the most shallow comprehension in subjects whose generally accepted facts they can replay.

It may not be so hard to improve on this.
LLMs have proven capable of using tools effectively, and tools such as more reliable ways of saving accurately and reliably knowledge acquired, or reasoning about that knowledge may not be difficult to supply.
The discussion in this essay may be thought of as concerning the use by such an AI with a tool which is capable both of storing knowledge and of deductive reasoning in the context of that knowledge.
The effect alleged would be to enable Artificial Intelligence which is \emph{oracular} in relation to logical truth.

Oracles may be thought of as having great wisdom, possibly derived from divine connection, but here I use the term more narrowly.
For the purposes of this essay the term ``oracle'' is used for something which is always truthful in answering questions, but doesn't always answer.

The oracle of interest here can be asked whether a sentence in a formal language is a \emph{logical truth} a concept which I will try to characterise, but which ultimately cannot be made absolutely definite.

The term ``Logical Truth'' is philosophically controversial.
In my usage of that term I stand on a limb, for my use is very similar to that of Rudolf Carnap, and is synonymous with the term \emph{analytic}, a concept central to Quine's repudiation of the philosophy of Carnap in the mid 20th Century.

Its not my purpose here to argue about the terminology.
Some might insist that my conception of logical truth should more properly be spoken of as set theoretic truth, and I do not intend to argue against that opinion.
Carnap, who until 1952 used the terms ``logical truth'' and ``analytic truth'' synonymously %
\footnote{as is explicit in section 2 of ``Meaning and Necessity'' \cite{carnap56}}%
, eventually accepted defeat and began to use the term ``logical truth'' for a narrower concept%
\footnote{W.V. Quine's noted a defect in Carnap's definition of analyticity in \cite{carnap56}, which followed closely a defect first seen in Wittgenstein's ``Tractatus Logico-philosophicus''\cite{Wittgenstein1921}.
Carnap's response appeared in the paper ``Meaning Postulates''\cite{carnap52} in which for the first time he separates the concept of logical truth from that of analyticity.}.

The term ``Oracular AI'' as used here, refers to what AI might in principle be able to achieve if furnished with an oracle for logical truth.

One of the purposes of this essay is to discuss how thus notion of logical truth can be made precise, to consider the difficulties in implementing such a decision procedure and to talk about the value of approximations which fall short of logical omniscience.

\section{Synthetic Epistemology}

We are concerned here with what propositions can be expressed and how the truth of those propositions can be established.

The general context here is the use of models of reality to facilitate successful planning of actions which may lead to some desirable outcome, the acquisition of food, the avoidance of peril, the construction of a skyscraper, the design of a GPU, an expedition to Mars.

These kinds of knowledge are almost invariably approximate, and the propositions involved, if judged in a black and white way are likely to be false.
As far as empirical claims in science and engineering are concerned, it is therefore preferable to find more subtle and informative ways of characterising propositions.
It is helpful in doing so to separate out an abstract model from the phenomena in question, for one may then engage in computations and reasonings about the abstract model which are exact or sound as appropriate, and subsequently and separately say something about the expected levels of correspondence between the abstract model and the concrete world.

There is, as noted by many philosophers and others, a very great divide between the levels of confidence with which we can come to conclusions in a formal mathematical context by contrast with the reliabilty with which we can measure and predict the behaviour of the real world.
In separating out these domains, we maximise the sphere in which the highest standards of confidence can be realised, and also in which the scope for automation and the application of artificial intelligence is safely (rather than speculative;y) maximised.

Beyond this very sketchy suggestion of how empirical knowledge might be addressed, similar considerations may also apply to matters in the sphere of ethics.
Moral reasoning can equally well be conducted reliably, if there is first some agreement on moral principles, provided only that moral truths are indeed systematisable in some such way.
Greater difficulties arise if moral judgements are not thought to be rational in that way, if for example, subtle judgements are needed which depend upon both a wise head and a life's experience.

Later. more discussion of empirical knowledge will be necessary, but I have sketched here a case for the necessary machinery for building abstract models to be treated as foundational.
It is more tractible than any other area, and it provides the machinery for building precise knowledge in all other areas.

There are some interesting foundational problems within that domain, which I will refer to as that of ``Logical Truth''.

\section{Logical Truth}

\part{Historical Threads}

\chapter{Terminological Notes}

\section{On the notion of Logical Truth}

The concept of logical truth has a long history in which modern controversy plays a role.
It is not my aim here to argue a case for the particular usage of the concept which I have adopted for this essay, but rather to make some observations about how that usage relates to some milestones in the history of logic which seem important in the present context.
Most discussions about the concept proceed as though there is an objective truth about the meaning of the concept, and argue for a particular explanation of what that meaning is.
That is not what is going on here.
I do not claim to know what that concept really means, I aim only to explain the usage which I have adopted and mention some ways in which this usage connects with the ideas of some other philosophers.

Let me first mention the four philosophers who seem to me to have come closest to articulating the same concept, mostly in quite different terms: Plato, Hume, Frege and Carnap.


\subsection{Plato}

Plato lived at a time when systematic deduction had first shown its value in the development of mathematics, and had also been shown capable of proving any nonsense you like in metaphysics or cosmology.
This contrast was exhibited in the conflict between the philosophies of Parmenides (who believed that nothing changes) and Heraclitus (who saw a perpetual flux).

These two philosophies were reconciled through Plato's two worlds, that of platonic ideals, and the world of appearances.
Plato thus made the distinction between logical and empirical truth which is the basis for the conception of logical truth addressed in this essay.

\subsection{Empiricism}

Plato's pupil Aristotle was to make enormous contributions to logic, but possibly not material advancement of this particular distinction.
He was concerned by the difficulty of understanding in the context of Plato's philosophy how it was possible to reason about the concrete world, dismissed by Plato as the shadowy world of appearances of which true knowledge was not possible.
The characterisation of reason as effective only in the realm of ideas and forms unfortunately excluded it from relevance to that shadowy realm of impressions which could yield no true knowledge, but which vitally concerns us all.

In articulating the concept of \emph{demonstrative science} Aristotle gave a good account of how one can reason about the concrete world, not just the etherial world of ideas, but in doing so the line he drew was between what was scientifically necessary as determined by the logical consequences of fundamental scientific principles and the accidents of how things happen to be.
The line he thus drew between necessary and contingent truths, was that between \emph{physical} (rather than \emph{logical}) necessity and his concept of contingency was confined to the accidental rather than embracing scientific laws.

Aristotle's conception of logic, and his conception of necessity was to be dominant for thousands of years, and perhaps held back further refinement of Plato's distinction.

The debates which ultimately lead to its further refinement may be thought to have begun with the division of early modern philosophers into \emph{rationaists} (Descartes, Spinoza and Leibniz) and \emph{empiricists} (Bacon, Gallileo, Locke, Berkeley and Hume).

\pagebreak
\phantomsection
\addcontentsline{toc}{section}{Bibliography}
\bibliographystyle{rbjfmu}
\bibliography{rbj2,fmu}

%\addcontentsline{toc}{section}{Index}\label{index}
%{\twocolumn[]
%{\small\printindex}}

%\vfill

\tiny{
Started 2023/07/28

\href{http://www.rbjones.com/rbjpub/www/papers/p034.pdf}{http://www.rbjones.com/rbjpub/www/papers/p035.pdf}

}%tiny

\end{document}

% LocalWords:
