% $Id: p035.tex $ﬁ
% bibref{rbjp035} pdfname{p035}
\documentclass[10pt,titlepage]{book}
\usepackage{makeidx}
\usepackage{turnstile,amssymb}
\newcommand{\ignore}[1]{}
\usepackage{graphicx}
\usepackage[unicode]{hyperref}
\pagestyle{plain}
\usepackage[paperwidth=5.25in,paperheight=8in,hmargin={0.75in,0.5in},vmargin={0.5in,0.5in},includehead,includefoot]{geometry}
\hypersetup{pdfauthor={Roger Bishop Jones}}
\hypersetup{pdftitle={Synthetic Philosophy}}
\hypersetup{colorlinks=true, urlcolor=red, citecolor=blue, filecolor=blue, linkcolor=blue}
%\usepackage{html}
\usepackage{paralist}
\usepackage{relsize}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{longtable}
\usepackage{url}
\newcommand{\hreg}[2]{\href{#1}{#2}\footnote{\url{#1}}}
\makeindex

\title{Synthetic Philosophy}
\author{Roger~Bishop~Jones}
\date{\small 2023-08-08}


\begin{document}
%\frontmatter

%\begin{abstract}
% This is now a portmanteau of sketches of writings in the crossover between philosophy and artificail intelligence, in general more specifically relating to the philosophical areas of epitemology and logic, and knowledge management and automated deduction.
% It is being massaged in the direction of a book on "Synthetic Philosophy", via, in the first instance, God willing, an essay on Synthetic Epistemology for Artificial Intelligence.
%\end{abstract}
                               
\begin{titlepage}
\maketitle

%\vfill

%\begin{centering}

%{\footnotesize
%copyright\ Roger~Bishop~Jones;
%}%footnotesize

%\end{centering}

\end{titlepage}

\ \

\ignore{
\begin{centering}
{}
\end{centering}
}%ignore

\setcounter{tocdepth}{2}
{\parskip-0pt\tableofcontents}

%\listoffigures

%\mainmatter

\section{Preface}

This is a portmanteau document in which I have collected together many feeble attempts at philosophy into one place with the intent of massaging the materials until they can be bullied into one decent monograph articulating the main elements of my thinking.
It remains to be seen whether it serves that purpose.

I have used the \LaTeX{} book format so that my current principle effort can be cleanly separated out as the first part, and my abortive prior essays can each fit into a single chapter in some other part.

\part{Synthetic Philosophy}

\chapter{Introduction}

This monograph presents a \emph{philosophical system} belonging to what has latterly been called the \emph{analytic} tradition.
Philosophy in that tradition, though predominantly engaged in varieties of supposedly rational analysis, have been, to a degree, understandably, anthropocentric.

David Hume's philosophical \emph{Magnum Opus} was ``A Treatise of Human Nature'' \cite{humeTHN} and his subsequent briefer presentation of its essence ``An Enquiry into Human Reason''\cite{humeECHU}.
A couple of centuries later the linguistic turn favoured logic and language over psychology, but resisted formalisation to retain its anthropocentric engrossment in the natural languages of \emph{homo sapiens}.

As I write we are firmly in the 21st Century.
``Artificial Intelligence'' is penetrating through the technology on which we now all depend, spawning on its way the new term ``General AI'' to denote what AI was once expected to be but has not yet become.

The colonisation of Mars has moved from science fiction into the plans of real world corporations contemplating the means to send a million tons to the red planet.
The rocket factories are in place and growing, while the design and testing of ever larger interplanetary transports continues to progress,

Meanwhile, the manufacture of android robots begins, destined to build on Mars the life support systems necessary for human habitation (and the fuel factories for return journeys).
A good measure of autonomous intelligence will be required.

These are the first steps in a process which may ultimately lead to the proliferation of intelligence across our Galaxy and into the Universe beyond.
That androids should be pioneers in colonising Mars, presages their prominence in any subsequent interstellar progression.
Ultimately we may find biological intelligence a crucial but small kernel of a sphere of human cognitive influence.

What then of epistemology?
The proliferation of intelligence across the Galaxy will be a technology intensive process, dependent on continued advances in science, technology and engineering.
What kind of epistemological theories will best support that process?

The central roles in the accumulation of knowledge played by the human intellect, sense organs, language and culture, and the imprint on these resulting from the mechanics of the evolutionary process, may not be ideal for inanimate intelligence in completely different and very varied circumstances.

Is it possible for \emph{homo sapiens philosophicus} to detach himself from his context and origins to produce philosophy relevant to diverse cognitive systems in very different circumstances?
Could philosophical foundations emerging from such enquiries bear fruit also for earthbound humanity?

[*****rework:{\it

I believe that such an enterprise may be worthwhile.
We can see in fundamental advances made in the last couple of centuries, an unwitting tendency in that direction.
The advancement of mathematics beyond its concern with numbers into more abstract and diverse subject matters, and the reduction of the whole to the simplest logical systems and ontologies, far removed from everyday language and commonplace clutter, ultimately to the benefit of science, engineering and human well-being and prosperity is just such a trend.
We have arguably yet to come properly to terms with the results of the combined effort of mathematicians and philosophers (and later computer scientists) in developing theoretical and formal approaches to language and logic, and may only do so when the complexity of detailed analysis is brought under control by the combined computational power and penetrating intelligence which is anticipated in our new cognitive assistants.

Even before these modern developments the simple structures of mathematics are among those subjects of human scholarship which can least be suspected of anthropocentrism.
Mathematics must surely begin with counting.
The details of numerals for recording how far a count has proceeded (and hence how many were counted) are arbitrary and human, but the structure of the natural numbers which the numerals designate seems uniquely determined by the very nature of counting.
It is impossible for us to imagine how any alien intelligence could not be acquainted with exactly the same number systems.

This tendency of mathematics to begin with the simplest structures and concepts was conspicuous when mathematics was brought to bear upon language and logic.
Whereas Aristotle's Organon%
\footnote{The collection of his writings related to logic.}%
and its scholastic elaborations were dressed in accidents of human cultural evolution, Frege's \emph{Begriffschrift} \cite{frege79}, %
\footnote{a seminal work in the early stages of the modern transformation of logic}%
  although undoubtedly an earthly notation, was much closer to exhibiting that compelling power and simplicity which marks the universal.

}]

\section{The Structure of Synthetic Philosophy}

In calling this \emph{synthetic} philosophy I suggest that it is constructed rather than discovered.
It is constructed in the context of a body of knowledge gathered and refined over the last several millennia and exploits some of the insights which that knowledge brings, not least on the nature of knowledge itself.
However, it is primarily pragmatic in its intent, not an addition to that body of knowledge but some ideas about how to move forward in its light.


These ideas are primarily epistemological, concerned with the ``theory of knowledge'' and in its implications for \emph{knowledge management}, how to gather, expand and exploit the knowledge which is essential to our future well-being and prosperity.

Having made a fuss about \emph{anthropocentrism} I will occasionally be commenting of the de-humanisation with results from its avoidance.
The first of those concerns how ``theory of knowledge'' is to be construed.

The word ``know'' belongs to the English language, which we can only know by observation, which in turn shows a variety of usage difficult to incorporate into a single coherent model.
If we sought to rigorously address the phenomenon spoken of by the word ``know'' our case would be hopeless.

Advances in science often depend on the introduction of new concepts, such as the the displacement for scientific purposes of the terms ``hot'' and ``cold'' by carefjlly defined temperature scales.
The word ``know'' is similar in its unsuitability for scientific purposes, and our synthetic philosophy is intended to share or exceed those characteristics of science which make it so.

So the first point of de-humanisation is to detatch our story from the word ``knowledge''.
This doesn't mean you won't be seeing the word again, it means it will not play a technical role in what follows, it belongs to the informal descriptions which are inteded to make the technical content intelligible.

So one of my first tasks will be to explain what the ``theory of knowledge'' becomes when we detach it from the English language.





\part{Essay Fragments}

One essay attempt per chapter.
This isn't a historical record, they are here to get mangled and recombined, or discarded.

\chapter{Oracular AI (2)}

\section*{Preface}

It was in 1986 that I first had hands on an ``Interactive Theorem Prover'', the Cambridge HOL system,
engineered to support formal reasoning about digital electronics.
I had just began working on the applications of formal methods to the development of security and safety critical information systems.

One of my early reactions on acquaintance with this system was to perceive the broadness of its potential applications.
The theory hierarchy which HOL supported, enabling the structured development of abstract models and of  the underlying mathematics required to build such modesl was in effect a general purpose \emph{knowledge base} of a kind relevant to GOFAI \footnote{Good Old Fashioned AI} with some extra features not often mentioned in AI.

The ability of the system to ensure logical consistency, surely essential for large scale deductive reasoning to have any value, together with a strong cultural preference for staying within those bounds, and the general perception that applications in engineering require nothing more, provided a seed in my mind which I have nurtured (perhaps intermittently) ever since, without as yet succeeding in articulating its relevance.

Some years later, when my own practical involvement in the application of interactive theorem provers was drawing to a close, that seed found a new place in my more philosophical aspirations.
I have struggled ever since to articulate the ideas which grew from that seed, and in this essay I try once more, focusing as tightly as I can on the core insights.

These swim against the dominant tendency which is evident in AI of emulating the capabilities of human beings, hoping that once reached they will be readily surpassed.
They do so under the inspiration of ideas about how reasoning and the knowledge which can be derived from it can be made both more reliable and more broadly applicable.
The grounds for the belief that this is possible do not come from observation of the ways in which human beings commonly reason, but rather from advances in mathematics, theoretical computer science and philosophy which have transformed our understanding of deductive reasoning and its limits within the last couple of centuries (after a couple of millenia in which progress faltered).

Though these new logical methods were theoretically and philosophically fruitful, and hinged around the invention of potent strictly formal languages and deductive systems, the application of these formal systems (rather than their use in the development of new theoretical disciplines) was severely limited by the complexity of the detailed proofs which they required.
The use of these systems in real world applications, or even in the development of mathematical theories, was impractical until the advent of the digital stored program computer.
Even with this assistance, in the form of brute computational capability, the support remained short of what was needed to realise the full potential.

This became apparent as GOFAI came up against the problem of `combinatorial explosion' and came to understand that intelligent heuristics were essential to success in finding deductive proofs of non-trivial propositions.

\section{Introduction}

When stored program digital computers were first invented their applications primarily concerned doing large amounts of information processing or computation with almost perfect reliability and at superhuman speeds.
They were accurate and reliable.

As their computational power grew their applications were extended progressively, and this sometimes involved attempts to achieve ends which were much less clearly defined, involving more complex instructions which could less certainly be relied upon to achieve the intended purpose.

The kinds of brute computational power exhibited by these early computers might at first have been thought signs of intelligence, since human skill in computation had certainly been presumed a sign of intelligence.
But brute computational power soon came to be distinguished from intelligence.

As I write, generative AI and Large Language Models have momentarily set a new standard for the unreliability of Artificial Intelligence.
Not designed or trained to be reliable repositories if knowledge, or to be capable of any but the most eleentary reasoning, their exposure to vast quantities of human knowledge enables them to perform in many main-line subject matters in an apparently authoritative way, while morphing in more esoteric areas into fantasy, and failing under the most modest interrogation to demonstrate any but the most shallow comprehension in subjects whose generally accepted facts they can replay.

It may not be so hard to improve on this.
LLMs have proven capable of using tools effectively, and tools such as more reliable ways of saving accurately and reliably knowledge acquired, or reasoning about that knowledge may not be difficult to supply.
The discussion in this essay may be thought of as concerning the use by such an AI with a tool which is capable both of storing knowledge and of deductive reasoning in the context of that knowledge.
The effect alleged would be to enable Artificial Intelligence which is \emph{oracular} in relation to logical truth.

Oracles may be thought of as having great wisdom, possibly derived from divine connection, but here I use the term more narrowly.
For the purposes of this essay the term ``oracle'' is used for something which is always truthful in answering questions, but doesn't always answer.

The oracle of interest here can be asked whether a sentence in a formal language is a \emph{logical truth} a concept which I will try to characterise, but which ultimately cannot be made absolutely definite.

The term ``Logical Truth'' is philosophically controversial.
In my usage of that term I stand on a limb, for my use is very similar to that of Rudolf Carnap,and is synonymous with the term \emph{analytic}, a concept central to Quine's repudiation of the philosophy of Carnap in the mid 20th Century.

Its not my purpose here to argue about the terminology.
Some might insist that my conception of logical truth should more properly be spoken of as set theoretic truth, and I do not intend to argue against that opinion.
Carnap, who until 1952 used the terms ``logical truth'' and ``analytic truth'' synonymously %
\footnote{as is explicit in section 2 of ``Meaning and Necessity'' \cite{carnap56}}%
, eventually accepted defeat and began to use the term ``logical truth'' for a narrower concept%
\footnote{W.V. Quine's noted a defect in Carnap's definition of analyticity in \cite{carnap56}, which followed closely a defect first seen in Wittgenstein's ``Tractatus Logico-philosophicus''\cite{Wittgenstein1921}.
Carnap's response appeared in the paper ``Meaning Postulates''\cite{carnap52} in which for the first time he separates the concept of logical truth from that of analyticity.}.

The term ``Oracular AI'' as used here, refers to what AI might in principle be able to achieve if furnished with an oracle for logical truth.

One of the purposes of this essay is to discuss how thus notion of logical truth can be made precise, to consider the difficulties in implementing such a decision procedure and to talk about the value of approximations which fall short of logical omniscience.

\section{Some Historical Background}

The stark difference between the reliability of deduction in mathematics and ways of discovering truth in other domains has been plain since the ancient Greeks began the transformation of mathematics into a theoretical science.
Axiomatic geometry.
The results thus obtained, particularly in the axiomatic development of geometry, were reliable and were to be gradually accumulated and were ultimately gathered together as the Elements of Euclid.

By contrast, those same ancient Greeks, when attempting to reason about nature and the cosmos were unable to establish durable findings, and would find many ways in which deductive reasoning, via reductio, could establish absurd and contradictory conclusions.

The differential success of deductive reason in these distinct domains was to be reflected in the two worlds of Plato's philosophy, of which only that of Platonic ideals was susceptible to truw knowledge, to be reached by reason alone.
Aristotle sought to rescue the applicability of deduction to what we would now call empirical science through his conception of \emph{demonstrative science}, which relied for itscoherence on the special truth assuring characteristics of the fundamental principles of each science.

Millenia followed in which this situation remained largely stable.
The modern conception of science originating in the scientific revolution toward the end of the renaissance resulted in philosophy being split into two camps associated with a primary emphasis on reason and observation respectively as the source of knowledge.
Empiricism retained the idea of reasoning from scientific principles, but insisted on observation and empirical experiment for the discovery and verification of the principles.


\chapter{Oracular AI (1)}

\section*{Preface}

\section{Introduction}

Alongside the enthusiasm for the largest of the Large Language Models initiated by the release of \emph{chatGPT}, the ways in which these models fall short of the ideals of General Artificial Intelligence have come into focus, and the discussion of the associated risks has been more intense, if not more informative.

LLMs were not intended to solve the problem of AGI, the are ``generative AI'', not designed as accurate or reliable stores of knowledge, or to have competence in reasoning or mathematics.
The following discussion is about what might be achieved if the capabilities of LLM's adapted to address some of desiderata around reliable retention of knowledge and reasoning about that knowledge.

The term ``Oracular'' AI is intended to suggest a kind of AI which can be completely relied upon to speak the truth, and which has a completely reliable deductive capability which exhibits that same kind of scale relative to the ability of homo sapiens as a supercomputers ability to do arithmetic calculations exceeds that of humans.

\subsection{Contextual Speculation}

The following speculations are not the point of this essay, but may help to make intelligible the ideas which follow.

Public speculation about how LLM technology might progress has include the following two very general approaches:

\begin{itemize}
\item Giving Large Language Models (LLMs) direct access to tools.
\item Integrating LLMs with other approaches to the development of AGI
\end{itemize}

Most of the intelligent beings which now exist are slow and unreliable by comparison with algorithms running on digital computers in very many kinds of problem.
There is no reason to expect that Artificial Intelligence will change that situation.
Sometimes there is an efficient algorithm and that is the best way to solve the problem.
Once an ``AI'' has access to such tools, then it will use them in preference to its own machinations.

Most of the great accomplishment, perhaps all, of the greatest intellects in history are but one stage in the development of knowledge and skills by an entire culture over long periods of time.
Large engineering enterprises involve very many intelligent designers collaborating to create and refine clear statements of requirements, design, coding and details of the physical and logical structures necessary to meet the requirement.
Most of these contributors will have specialised in some aspect of this process and could not effectively contribute in areas which are too far removed from their speciality.

Nevertheless, AI marches forward toward a goal of complete generality.
That generality might be thought of as being capable of specialising to any area given appropriate training, but even that would exceed the kinds of intelligence which homo sapiens exhibits, for our gene pool is diverse and different individuals have the aptitude to perform well in distinct domains.

The direction of movement which we perceive in some of the most complex feats of engineering design is toward better support for collaboration between even larger distributed teams.
The most prominent manufacture of digital hardware intended to support AI, Nvidia, provides an environment called ``the Omniverse'' in which a complete factory can be designed and simulated, making the most of AI, and through which a widely distributed development team can work together to advance the design.
In such an environment the behaviour of each part of the manufacturing system can be simulated, and the simulated environment can be used to train the software for robots intended to participate in the running of the factory.

As AI becomes capable of contributing to such projects it may at first be directly used by human designers to help them accomplish their part of the work, but later we may expect AI agent to act on a par with human beings, seeking help as necessary, subject to review by the normal processes.

Alongside the Omniverse, Nvidia promotes the idea of ``digital twins'', which are abstract computational models of physical systems.
These models may serve various roles throughout the life cycle of the systems, appearing at first before the construction of the physical counterpart, enabling comprehensive testing verification and mutli-faceted evaluation prior to manufacture, and subsequently tracking the state of their physical instances to facilitate control and maintenance.

\subsection{A Safe Zone}

One aspect of making AI safe is simply making it more reliable in distinguishing truth from falsity.
This does nothing to prevent malfeasance, but a large proportion of pre-AI safety concern addressed the prevention of accidental rather than deliberate harms.
A safe car is not one which will refuse to mow down pedestrians, it is one that will not do so unless its driver intends it, or is incompetent or culpably negligent.

Even this kind of safety is hard to guarantee, and potentially becomes harder as AI permeates the workings of the world.

It may therefore be comforting to know of important domains in which conclusively establishing the truth is feasible (and customary) and in which even greater assurance of truth can be realised with the help of computers and in the context of artificial intelligence.

This is the domain of deductive inference, and logical truth.
It is the purpose of this essay to mention some of the well known ways in which these truths can conclusively be established and to add some ideas which may be less widely appreciated.

If deductive inference were to be automated so well that very lengthy deductions could be reliably conducted and subsequently depended upon, then the context in which those deductions are undertaken is important.
First it is necessary for the meanings of the language (or languages) involved must be clearly and precisely understood, for reasoning and comprehension is otherwise compromised.
Second, it is necessary that the context in which deduction is undertaken is coherent, for otherwise a contradiction may be proven and from that contradiction any conjecture, true or false, will be derivable and proof will confer no confidence.

The verification of engineering design is an important potential application for AI powered deductive inference.
But the reasoning involved must make use of a large body of mathematical and scientific knowledge, it must take place in the context of 
mathematical models of the designed artefact and the context in which it is required to operate.


\subsection{Some Observations on Intelligence}

We know well enough how intelligent beings go about extending our knowledge of the world and applying that knowledge to various practical ends.

Two pervasive features are:
\begin{itemize}
\item The gathering of knowledge is cumulative and cooperative.
\item The knowledge is a shared resource mostly held in media external to the intelligent agents involved.
\end{itemize}

I suggest that this is unlikely to change with the advent of AGI.
So it it likely that the AGIs will be cooperating with other intelligent agents in the continued accumulation of knowledge stored in shared distributed media a large part of which is freely accessible to all.

These are some features of intelligence which I expect to persist as AGI progresses.

Nevertheless there will be important and substantial changes.
In the past, it has been fairly rare for a new area to be automated by computer without significant changes in how it is done.
A simple example of this is the very different expectations we have of the reliability and scale with which arithmetic computation can be performed one digital computers were available.
In many more cases, it is not just the greater reliability and speed which matters, something which was hitherto infeasible may be rendered commonplace.

Another area in which computational machinery has outstripped prior capabilities is in memory.
Very large quantities of information can now be reliably stored, perfectly retrieved, swiftly searched and processed in some way.

In these terms, the current crop of LLM's are a regression.
They were not intended to perform well in any of those ways, they were intended to process languages, and having exhibitied surprising ``emergent capabilities'' are now sometimes judged as if they were intended as AGI.

Much of the effort invested in AI research aims to replicate the kinds of intellgence exhibited by humans, rather than aiming at superior performance in areas of practical importance.
Though early in the history of AI research the automation of formal logical theorem proving was not only an important domain of AI research, but was considered by some to be the route to a general intelligence, it later was outflanked by simulating the brain as neural nets and common sense reasoning became the aspiration.

In the following speculations I am concerned with how to enable GAI which is as reliable as possible in all domains, approaching this by focussing on the kind of reasoning which we can reasonably expect to be made wholly reliable.
This includes reasoning in mathematics, science and engineering.

In these domains, deductive reasoning has a very important role.
It may be said to be almost the whole of pure mathematics.
We may also speak of the nomologico-deductive model of science, in which the evauation of scientific hyotheses or the application of accepted theories is undertaken by deduction from those theories and the particulars of the case in hand.
Engineering is then merely a collection of application of science in which the process of verification of a design against some statement of desiderata is one of logical deduction.


My aim here is to sketch a tool for use by LLMs and other AI as a way of guaranteeing that the kinds of deductive reasoning which they might effectively deploy can be guaranteed sound.
The importance of this cannot be underestimated, due to the difficulty in containing infidelity.
Once a falsehood has been proven a contradiction is not far away, and armed with a contradiction all proofs whether of truths or of falsehoods become very short and completely worthless.

My description of the proposed logical tool will come in several stages, growing more complex as additional desiderata are introduced and address.

In the first instance the tool could simply be the logical kernel of en existing Interactive Theorem Prover, which for reasons which I hope to make clear should I suggest support formal deduction in the variant of the HOL logic devised at the University of Cambridge, originally for reasoning about digital hardware.


\subsection{Alternate Paradigms for System Development}

The approach to systems development exemplified by digital twinning and the Omniverse centres around computational simulation, accelerated and enhanced by AI.

This is an effective approach which we may expect to be progressed and advanced.
It retains the limitation which has always attended verification by testing, notwithstanding the scope for testing coverage improvement by AI.

It a very sophisticated for of informed guesswork.

The best way to ensure compliance of a design with a requirement is by logical analysis, which has been a distant ideal for decades.

\subsection{Language, Knowledge and Models}

Mathematics, science and engineering design and implementation involve a large body of knowledge presented in many distinct languages as well as the statements of requirements and descriptions of design and implementations, construction of one or more formal or abstract models of the

\section{What is an Oracle?}

The word has a variety of usage, so best to be clear about what is intended here.
In ancient Greece it was a mouthpiece for deity, and therefore had not only wisdom but any other characteristic one might attribute to someone with a direct line to God.

Skip, doubtless, many intervening subtleties, and arrive in the twentieth century not so long after mathematicians got into logic (a century is nothing in the scale of these things) and we find that the notion of oracle gets a place in recursion theory, i.e. the theory of effective computability.

In that theory, after the concept itself was precisely and convincingly defined, the next step (for which purpose the concept had been introduced) ws the demonstration that not all numerical functions were effectively computable.

This problem was addressed through the idea of a decision procedure for a set of natural numbers, and it was shown that some sets were not effectively decidable, and hence that their decision problem was ``unsolvable''.
It further transpired not all unsolvable sets were equally unsolvable, but that they could be classified according to their degree of unsolvability.
These degrees were defined through the concept of reduction, in which the decision problem for a set A is reducible to that of a set B if, given an oracle for set A the decision problem for B could be soled.
The notion of oracle for a set A is quite simple here, it is simply something which will tell you of any number whether or not it is in the set A.

There are no vague notions of wisdom in play here, calling something an Oracle for B just tells you that it has the answers to a particaular set of questions.
The idea of Oracular AI discussed in this paper is primarily that of an AI, which is, over some well defined class of problems, able to give the correct answer.

I have to make one retrenchment to that, in acknowledgement of the incompleteness result first discovered and proven by Kurt Godel.
There is also a related issue concerning semantics, which I suppose we might think connects back to Tarski, that the set which I will seek to characterise here as the domain in which Oracular AI might be an Oracle cannot be precisely defined.

With those two caveats the aim of this essay is to argue that corresponding to Hume's ``fork'' there is a domain within which we can reasonably expect AI's to be Oracle's.

\chapter{Formal Abstract Modelling for General Artificial Intelligence}

\section{Preface}

``General Artificial Intelligence'', as is now termed since the ``Artificial Intelligence'' lost its cachet, ought we hope, if not at first, to surpass humanity in the reliability of its memory and the certainty of its reasoning.
The latest wave of excellence fails miserably on both.

Large Language Models (LLMs) have certainly cracked language, which is, as some modern philosophers have pointed out, complicit in the incoherence of common discourse.
Can we now assume that when natural language discourse is required, an LLM technology will supply it, and seek deductive proficiency through simpler more precise notations, or relegate concrete syntax to intermediaries and approach deductive closure through exclusively abstract structures?

That is the possibility which this essay explores.

Despite their present ascendency, Large Language Models have not been the star achievers in recent AI R\&D.
The mantra that proficiency in AI is crucially dependent on the use of very large bodies of data for training neural nets has been disregarded in several of the most impressive achievements of Deep Mind the London AI startup now acquired by Google's parent Alphabet.

First with Chess, then Go and latterly alphafold, Deep Mind showed that in in simple worlds governed by a modest collection of unambiguous rules, neural nets relegated to heuristics could achieve proficiency as the system learns by playing itself, without needing role models to mimic.
Though their complexity is unbounded, the abstract structures of mathematics and their use in the more elaborate mathematical models created by science and the vastly more complex constructions of engineering (especially in digital semiconductors), are susceptible to the same methods, which place Monte-Carlo tree search in pole position with crucial subordinate roles for neural nets (which need no linguistic competence).

Thinking of deductive inference about abstract models as being a precise and simple game, it is merely a speculation on my part that it will be susceptible to similar methods as chess, go and protein folding, and the realisation of that aspiration is not something to which I can contribute, except by the attempt which I offer here to clarify the nature of the game.

There are many different ways in which a broadly universal game of deductive modelling can be framed, and what I offer here is just one of them.

I have been trying to tell this story for a long time, without success.
I don't know whether the story can be told without talking about the history of the ideas in which it appears, and, not being a historian I am likely to be scornful of my attempts to do that.
But i have attempted to tell the story without going into the history, and I cannot see how it can be understood.

The fact is that the ideas which I want to present under this heading are convincing to me only because of the history of ideas which has lead me to them, though my own particular perspective on them us substantially influenced by some of the most recent developments in them (by historical standards, not by the hectic standards of AI research) and by my own professional experience in trying to apply these ideas in the development of highly assured information processing systems in relation to security and safety concerns.

This essay is an attempted compromise, in which I try again to present the ideas in the context of a minimal presentation of those histotical precedent which seem most crucial in illuminating and motivating the approach which I am suggesting.

\section{Introduction}


Nearly half a century ago John McCarthy published a paper \cite{mccarthy1981} that divided the problem of artificial intelligence into two components or domains: an epistemological component, which concerned the representation of knowledge and the rules of reasoning, and a heuristic component that optimised search and pattern matching to permit the solution of  otherwise intractable problems.

In this essay I discuss a further division of the epistemological side of McCarthy's division.

McCarthy's ``epistemological part of AI'' can, I claim, profitably be advanced by separating our that part which concerns purely deductive reasoning.
In the established epistemological terminology, this is the problem of the \emph{a priori}.
The contention is that all deductive reasoning, even in relation to knowledge \emph{a posteriori} can be undertaken and validated in an abstract model with the standards of soundness and consistency which are realisable in that domain, and that an approach which separates out  abstract modelling as a fundamental shared component of all empirically applied AI is not only theoretically elegant but practically effective.

The separation of the whole of this domain yields a general facility for \emph{abstract modelling}, in which any aspect of empirical concern can be addressed by the use of an abstract model.
This is a particular way of thinking about and implementing the use of formalised mathematics and logic for modelling the phenomena in the real world, and thus a critical element of the problem of knowledge representation, and a means to very high levels of rigour and generality in the automation of deductive reasoning.

\section{A First Sketch of The Foundation}

The formalisation and mechanisation of logic and thence of mathematics was anticipated by Aristotle and Leibniz, but only became plausible after Frege devised his \emph{concept script} and attempted to demonstrate the reduction of mathematics to logic.
His own system was shown to be flawed by ``Russell's paradox'', but Russell then went on to devise and apply a type theory for that same purpose.

Frege's mantra was that:

\begin{center}
Mathematics = Logic + Definitions
\end{center}

Which has fallen foul of much philosophical disagreement about exactly logic is, but it was generally accepted that \emph{in practice}:

\begin{center}
Mathematics = Set-theory + Definitions
\end{center}

Where \emph{in practice} means that almost all of mathematics can be derived in the axiomatisation of set theory known as ZFC provided that definitions of the concepts of mathematics in terms of sets are accepted, and that such definitions can be supplied which render the resulting formal theory strictly the same as accepted but less formally conducted mathematics.
We have to say \emph{almost} all here because of the well known incompleteness results of G\"{o}del, but I have been unable to locate any incompleteness which is relevant to any but the most esoteric studies in logic or mathematics.

Henceforth any logical system (and there are many of them) which is capable of formalising logic and mathematics in a similar way I call a \emph{logical foundation system} (often eliding the `logical'), and I note here the importance of the possibility of proceeding in such a foundation system exclusively by the use of definitions of concepts rather than by the introduction of additional axioms or rules of inference as crucial to the integrity of the system, ensuring as it does, that if the foundation system itself is logically consistent the so will be the extensions for modelling purposes undertakem exclusively by definition.\footnote{
Though later we will admit a useful generalisation of the concept of definition as that of \emph{conservative extension}.}

Russell's \emph{Theory of Types} was a complex logical system due to ramifications thought necessary in the type system.
It was not long before Ramsey suggested its simplification into the \emph{Simple Theory of Types}%
\footnote{First published as a fully worked out formal system in 1928 by Hilbert and Ackermann "\cite{hilbert1928}.}%
, arguing that the ramifications were not in fact necessary.

In the 1930's logicians found it necessary to clarify the concept of \emph{effective method}, and in 1936 four simple but general ways of describing computational methods (algorithms) were published.
These were Stephen Kleene's ``recursive functions'', Alan Turings's machines, Post's production systems, and Church's lambda calculus.
\footnote{The relevant papers may all be found in ``The Undecidable'' \cite{davis65}}
These originally devised to demonstrate the existence of unsolvable problems, but Church's lambda calculus was to prove influential in the design of computer programming languages and as the basis of foundations for mathematics which through their simplicity and power would be embraced by those computer scientists and engineers who were interested in reasoning about computer programs and digital hardware.

Closely related to the lambda calculus, in which functions are defined using a variable to represent the argument, and an expression involving that variable for the value of the function at that point, an even simpler but similarly expressive notation was devised which made no use of these ``bound variables'', and there was much research in how these systems might form the basis for mathematical foundation systems.

A major figure in this research was Haskell Curry, whose names were to be enshrined in the terminolgy of functional programming languages (which are closely related to the lambda-calculus), but his work on mathematical foundation systems based on combinators was to prove less influential.
For our story it is the ideas of Church which are more significant, and after proposing a foundation system based on the simple (type-free) lambda-calculus which proved to be logically inconsistent, he added to the lambda-calculus a type system similar to that of the Simple Theory of Types resulting in a variant of that system which would prove influential in computer science.

Church's formulation of the Simple Theory of Types \cite{churchSTT} was to be taken up in research into the formal verification of computer programs and digital hardware.
The resulting elaboration of Church's system was the product of a line of development which began with a logical system devised by Dana Scott for reasoning about programs and programming languages
\footnote{First circulated in 1969 but not published until much later as \cite{scott1993type}}.
This became the logical basis for a line of software systems for computer assisted proof construction and checking intended for reasoning about programming languages and programmes.
The first system was called LCF for ``Logic for Computable Functions'' and after several steps, including migration from Stanford to Edinburgh and then Cambridge (UK) was re-oriented toward hardware verification and adapted to support a variant of Church's STT by Mike Gordon.%
\footnote{For a fuller account of this story see Gordon\cite{gordon2000lcf,plotkin2000proof}}
This logic and the software developed to support reasoning in the logic were both called HOL, or Cambridge HOL if the context left room for doubt.

As far as the Cambridge HOL logic was concerned the principle new feature was its inheritance from LCF of a polymorphic version of the type system.
In Church's exposition of his Simple Type Theory variables appear in the types, but these are syntactic variables belonging to the metatheory, not variables in the object language (STT).

The logical system put together from these two principle lines of influence, after some refinements settled with the following characteristics:

\begin{itemize}
\item From Church's STT:
  \begin{itemize}
  \item The abstract syntax, semantics and deductive system of STT is the base to which the following enhancements are applied.
  \end{itemize}
\item from Edinburgh LCF:
  \begin{itemize}
  \item Hindley-Milner polymorphism:
    
    The practical consequences of the strict type system in STT become onerous once the system is used beyond the confines of arithmetic, for the theorems governing the operation of structures like sequences would need to be proven again for every new kind of object from which lists need to be constructed.  Including type variables in the object language allows for a single theory of lists which can be instantiated to any type for which lists are needed.
  \end{itemize}
\item New in Cambridge HOL
  \begin{itemize}
  \item Definition of Type Constructors:
    
    In the development of mathematics, for example in the sequence of number systems leading up to (and beyond) the complex reals, each new kind of number is constructed from earlier kinds of number and has a new type.  The type is chosen for convenience in defining operators with the required properties, but is not a part of the abstract structure intended.  The definition of a new type which delivers the required structure while concealing the manner in which that result was obtained, is valuable in managing the complexity and intelligibility of layer upon layer of required abstractions.
  \item Definition of Constants:
    
    In many logical system \emph{definitions} are syntactic, permitting a concise expression to stand in stead of something more complex or extended.
    In Cambridge definitions extend the underlying logical system by adding new constant symbols into the language together with an axiom which constrains the interpretations of the constant.
    Simple definition in this form are \emph{conservative}, permitting no new theorems to be derived which do not mention the new constant, hence preserving the consistency of the logical system.
    In Cambridge HOL, for the sake of improved abstraction (or the avoidance of over-specification), the mechanisms which permit new constants to be introduced allow weak constraints.
  \item Sequent Calculus:

    The deduction system is reorganised as a sequent calculus rather than the more traditional Hilbert style inference system used in Church's STT, and for various other pragmatic reasons is equivalent to but distinct from the details of that system.

  \item Theory Hierarchy:

    It is essential for the tool supporting the logical system to keep a record of the definitions which have been entered, to ensure that consistency is not compromised by introducing the same constant more than once with conflicting definitions.
    In the implementation of Cambridge HOL this is done through a heirarchy of theorems which keep a record of the constant definitions which have a occurred in a way which controls the scope of the defintions and permits many different developments of logical theories to be saved in the same logical database.
    This may also be used to save theorems proven in the relevant context.
  \end{itemize}
\end{itemize}

There are many more original aspects of the HOL system which enabled the definition of constants and the construction and checking of formal proofs in this logical system, but these do not concern us here.
It is the abstract language, its semantics and its inference system which are here advocating for more general application, and not any particular software to support those applications.

The principle merits of this system so far as the role here proposed for it are as follows:

\begin{itemize}
\item It is a ``mathematical foundation system''
  \begin{itemize}
  \item It replicates Frege's prescription for the logical structure of mathematics as obtained by logic from the definitions of the mathematical concepts,  and the suitability for the formalisation of mathematics exhibited first by Russell's Theory of Types, in a neater and more practical way.
  \item It permits the derivation of mathematics and of applications of mathematics by conservative extension, subject only to a suitable axiom ensuring that the ontology is sufficiently rich.
    In Church this is simply an axiom of infinity asserting that there are infinitely many individuals, supporting the development of arithmetic and analysis, which for some of the more exotiv mathematical theories (e.g. parts of Set Theory) might not suffice, but which can readily be accomodated by the use of a stronger axiom of infinity, possibly one asserting that the cardinality of the individuals is inaccessible.
    \item only the abstract syntax and semantics of the system is adopted, and this abstract syntax because of its simple and higher-orer structure is ideal to replicate the abstract syntax of arbitrary language using constructors whose definition captures the semantics of the language, hence 
    \end{itemize}
\end{itemize}

\cite{gordon1989mechanizing,birtwistle2012current}

\cite{cohnPIHV}
\cite{mccarthy2022artificial}

\section{A History of Modelling}

\subsection{What is a Model?}

The aim of this document is to talk about formal abstract models, since it seems possible that they will be the next important development in the evolution of models and their use.

In order to give a sense of the importance of such models I aim to sketch the history behind them.
This might involve a bit of stretching of the term, but it already serves a great variety of purposes so I'm sure it will cope.

It may be useful to characterise the sort of model which are of interest here, which are those which consist in something representing information about some other thing of interest.
This encompasses linguistic descriptions as well as information strucures which long pre-date the evolution of language.

Since the beginning of life on earth, even the most simple forms of life have found it necessary to sense their environment, and to adapt their behaviour according to what those senses reveal.
In the simplest cases, the senses will serve to detect potential foods and suspected predators, in order to secure the former and avoid the latter.

This involves some kind of representation of the environment being created by the sense organs, to be transformed into that kind of representation of necessary effect which will secure the best interests of the organism.
In those organisms which possess a nervous system the representation may be as patterns of firing in neurons, and the transformation will be undertaken by a neural net before some other pattern of firings initiates appropriate motor functions.
In yet more primitive organisms, the necessary presentations and processes are likely to be chemical, and the representations less likely to be thought of as models even though serving similar purposes.

\subsection{}

My aim here is to describe a game, which I believe to be susceptible to automation, but which because of its algorithmic complexity depends upon the use of heuristics best supplied by intensively trained neural nets.

The game is the prediction of real world events by the use deduction in the context of abstract models of the relevant real world phenomena.
It also includes the elaboration of the pre-requisite logical and mathematical theories which provide a basis for the construction of the relevant scientific and engineering models.
The whole provides potentially a substrate for the automation of engineering design.

A bare presentation of the game as proposed would I expect be unconvincing and opaque, so I propose to come to the details via some discussion of the history of abstract modelling, or of those kinds of process which have lead the particular conception of abstract modelleing which is presented here.

This leads to a minimal presentation, followed by the consideration of a certain number of desirable elaborations.
Throughout, an understanding will be supported by the presentation of certain philosophical positions, which are offered primarily as useful in supporting this kind of enterprise (i.e. on a pragmatic basis) not as demonstrably superior to the many alternatives.

\section{Some History of Modelling}

\subsection{Pre-History}

Throughout the evolution of life on earth it has been necessary for living organisms to take cognisance of their surroundings in order to take those actions which will secure nourishment, avoid predators, and effect reproduction.
When an organism takes in information about its surroundings and uses it as a basis for its actions in the near future, it is not wholly unreasonable to consider that it has thereby acquired some sort of useful model of its environment, and that is will use that model to predict which courses of action will best serve its need to grow and replicate.
For organisms with a nervoud system we might call these neural models, for single celled organisms perhaps chemical ``models'' (stretching the point beyond its limits perhaps).

As the nervous systems of organisms evolve to greater complexity we may imagine that the complexity of the models grows with that of the nervous systems.

Though these models serve a vital purpose for the organisms, we cannot reasonably say at this point that the models are used deductively.

Deduction can only be considered in the context of \emph{propositional language}, which is that kind of language in which claims can be expressed which will be either true or false according to how things are, and whose assertion may thereby communicate knowledge (about how things are) from one to another.
Such language is probably coeval with homo sapiens, who appears at the end of a million years in which the size of the brain grew quite rapidly.
Any faculty which demands special mental faculties must evolve alongside both the physical and mental adaptations necessary to support it.
Those regions of the brain which are adapted to support language could not have appeared without the languages faculties they support.

With propositional language comes deduction, for even the most elementary language must embrace taxonomic classifications.
If your language has a word for horse and one for animal, then you cannot be said to understand the language unless you know that horses are a kind of animal and are able to deduce from the knowledge that something is a horse the ``conclusion'' that it is also an animal.
Admittedly this kind of deduction is neither formal nor self-conscious.

The advent of language enabled oral culture which would ultimately empower mankind in the transformation of the world.
The evolution of culture must at first have been quite slow, since the first evidence of animal husbandry and then agriculture date back only 13,000 years, 200-300 thousand years after the evolution of homo sapiens.

The first stage in accelerating cultural evolution was the invention of the written word, which, in the form of clay tablets, was probably of modest impact, but the technology of writing and communication has itself evolved and many important cultural advances have been facilitated by new media for storing, and sharing knowledge.

Papyrus was invented by the Egyptians around 3000 BC, and was a medium for the preservation of Egyptian culture, including geometry and arithmetic.
It does not appear to have been adopted in ancient Greece until about the 8th century BC, but then catalysed the flourishing of Greek culture which then shaped the development of the Western and ultimately the whole world.
This began with literary works such as the Odyssey, but of greater interest to us here is the transformation of Mathematics which began around 600BC.

\subsection{Greek Mathematics and Logic}

The period between 600 and 300BC was one in which major advances were made in sophistication with which reason could be applied to modelling the world.

It is this period that mathematics is first systematised as a deductive science, and in which we first see deduction talked and theorised about.
The systematisation of Geometry now known as Euclidean Geometry set a new standard of rigour allowing more elaborate deductive reasoning to be undertaken with great reliability.
This standard was not to be improved upon for the next two millenia, and was to be the envy many philosophers who wished for the emprimateur of logical certainty to be attached to their conclusions.

\subsection{The Organon and Demonstrative Science}

It is in Aristotle's ``Organon'' \cite{aristotleL325,aristotleL391} that the applicability of logic to the understanding of reality was first to be made explicit, and it was in Aristotle that we see the first attempts at formally codifying the rules of sound deductive reasoning.

In his Organon, six books forming his contribution to logic, Aristotle presents the idea of Demonstrative Science in which necessary conlcusions about the world are deductively derived from the fundamental principles of each science.
In these volumes the nature of deductive reason is analysed using the idea of the syllogism.
This is the first known metatheory concerning deduction, and it was to remain dominant until the end of the 19th century.

\subsection{Bacon's Novum Organum and Empiricist Science}

The next major development in the methodology of science, the ways in which we build models of the world, came in the Renaissance and is presented in writings of Francis Bacon, notably his \emph{Novum Organum} \cite{bacon2017novum}.

Whereas Aristotle sought to adapt the axiomatic methods of mathematics to Science, looking in greater depth at the process of deriving conclusions from the principles than that of establishing the principles, Francis Bacon was more concerned with how the principles could be established, and in this the differences between mathematics and science wer more marked.

He described a four stage process as follows:

\begin{itemize}
\item Collect data about the phenema of interest.
\item Look for patterns in the data.
\item Formulate a hypothesis.
\item Test the hypothesis by observation and experiment.
\end{itemize}

This is the new nomologico-deductive method, in which the novelty is primarily in the establishment of the laws rather than their application (though testing hypotheses is similar to application, as involving deductive inference from the hypotheses).

Almost contemporary with Bacon, and with more substantial credentials as a scientist and engineer (as well as a philosopher), Galileo Galilei (1564–1642) spent most of his life trying to give structure to the new science, in which Aristotelian science was largely cast aside.
Gone was the teleological emphasis on ``final cause'', and replace the five fold ontology of ether, fire, air, water and earth with a system built exclusively on matter, introducing a mechanical system which came to dominate modern science.

Modern philosophy from this point divides into two camps, empiricist like Bacon who emphasised observation and experiment as the source of worldly knowledge, and those who sought knowledge through reason.

\subsection{Leibniz and The Mechanisation of Science}

\subsection{Hume's Forks and The Synthetic A Priori}

\chapter{Evolution and Logic}

% Evolution is the mechanism which turns chaos into order,
% not so much by design as by the accident that chance will eventually create complex structures which thrive.
% It has many forms, some very different to what we call Darwinian evolution, and by consideration of how those forms of evolution have themselves evolved we can speculate about the future of evolution and of intelligence.
%My own speculations on this topic say something about epistemology and logic.

\section*{Preface}
\phantomsection

\addcontentsline{toc}{section}{Preface}

This preface is really immaterial, but there might be some point in saying \emph{why} its immaterial.
Here's Bertrand Russell more than a century ago on a theme that remains current.

\begin{quote}
  \emph{That Man is the product of causes which had no prevision of the end they were achieving; that his origin, his growth, his hopes and fears, his loves and his beliefs, are but the outcome of accidental collocations of atoms; that no fire, no heroism, no intensity of thought and feeling, can preserve an individual life beyond the grave; that all the labours of the ages, all the devotion, all the inspiration, all the noonday brightness of human genius, are destined to extinction in the vast death of the solar system, and that the whole temple of Man's achievement must inevitably be buried beneath the débris of a universe in ruins — all these things, if not quite beyond dispute, are yet so nearly certain, that no philosophy which rejects them can hope to stand.}

Bertrand Russell, Mysticism and Logic \cite{russell17}
\end{quote}

Much has changed, in the world and in philosophy, since Russell penned those words, but majority opinion is probably still with Russell's sentiment (if not his way of putting it), and its not even controversial, as he says (in other words) resistance is futile.

I was reminded of Russell's words just as I resolved on this essay, and the contrast between that point of view and the perspective intended for the essay seemed a good way to highlight that perspective.

Russell talks of the death of the solar system, and of a universe in ruins.
The former would be a consequence of the Sun progressing through the observed life cycle of similar stars, which I don't doubt.
It will not be happening soon, not even in evolutionary timescales, so it doesn't seem wholly irrational to hope that humanity or its progeny will already have ventured to havens beyond our solar system before it happens.
I have something to say about how that might happen.

The idea of a universe in ruins has no support which I know of other than the second law of thermodynamics.
I have no problem with the practical applications of thermodynamics, which as far as I am aware mainly talks about entropy changes in closed systems.

I have struggled to understand the concept in the context of the second law, and struggled to comprehend the evidence offered for the second law, and I have neither understood nor believed.
So this essay is written as a speculation about how evolution has and will continue to progress, by someone who has no expectation that this will ever come to an end (and is agnostic about whether it ever had a beginning).

I might add a more comprehensive scepticism about the plausibility about the science of the very greatest extremes.
I doubt that scientists will ever realise perfect models of the microscopic structure of the universe, or about the gross structure of the universe, or about what happens close to the supposed singularities in relativistic models.
However far we have been able to peer with our best instruments, there may yet be something beyond which defies our expectations.

As to the significance of these beliefs to this essay, it is small and mainly psychological.
The timescales in which the second law might be expected to yield heat death are beyond those in which my speculations are credible even to me.
So far as I know, if I did believe the second law of thermodynamics, I would still think it ``academic'' and write the same essay.
Nevertheless I reject Russell's perspective.
Russell has made important contributions in some of his other works to the ideas presented here, and has been for me and many others over the last century, an inspirational figure.
But on this I demur.

\section{Introduction}

There is at the centre of this essay a very simple idea which is not mine.
I believe its importance is much greater than has been recognised, and this essay is intended argue the case for that belief.
More important than the belief itself is its consequences for the way in which artificial intelligence (and synthetic ecology) is approached and progressed.

Having said that, the ideas in the essay are not intended to be a direct contribution to those fields, in which I have no expertise.

The central idea is very simple, and apart perhaps from the details of presentation and the conception of significance, is not original.
A naked presentation could not convey the importance which I attach to it, and this document is primarily intended to provide a backdrop against which it's importance can more clearly be perceived.

That backdrop is evolutionary.
It is a story of the many kinds of evolution which have brought us to this moment in time at this point in space, and a speculation about some of the evolutionary processes which will lead us forward and the product of those very different evolutions.

My own reading of certain aspects of evolution provides a basis for the speculations to follow about how evolution will itself evolve in the future and how that will shape the growing sphere of influence which homo sapiens has in the universe.
From those speculations, some conclusions are drawn about the representation of knowledge and the automation of reasoning which lead to a a sketch of how those requirements might best be satisfied which form the kernel of this essay.

\subsection{Evolutionary Themes}

It is evolution which has shaped life on earth.
But no single kind of evolution can explain it all, and the purpose of this essay is best served by a varied diet.

If we are to consider the future trajectory of evolution, it will be necessary to identify some of the characteristic which those different kinds of evolution have in common, as features most likely to be preserved into the future, as well as to consider those aspects of contemporary evolutionary processes which are most closely aligned to context which will not be preserved into our futures.

Homo sapiens is soon to become and interplanetary species, perhaps then interstellar.
This is such a profound transformation in context that it must surely have impact not only on what evolves, but on how evolution works, both in the minutiae of the reproductive processes, the ways in which variation occurs and the selective pressures which guide the process.

So, what is evolution?
Some kinds of evolutionary thinking go back as far as the ancient Greeks, but modern conceptions of evolution date back to Darwin, who offered evolution as an explanation of the origin of species.
As science has progressed the dominant conception of evolution has also advanced, the development most conspicuously punctuated by the ``modern synthesis'' of which the most important aspect, though by no means the whole, was the incorporation of Mendelian genetics.

For our purposes its best to start with a simple model, since we seek an idea of evolution which will have broad scope.

Darwinian evolution may be characterised in three points:

\begin{itemize}
\item It concerns the evolution of species, which are populations of interbreeding organisms, and it is the species which are said to evolve.
\item As the population of the species reproduce, the offspring share many of the characteristics of the parent or parents, but they are not identical.
  Variation is thereby introduced.
\item Different variations may impact on reproductive success.
  This also happens in selective breeding by farmers, but in nature no deliberate selection is involved, and the differential success in reproduction is attributed to ``natural selection''.
\end{itemize}

It is generally emphasised that the variation which occurs is random and that the selection process is ``natural''.
These emphases are necessary because the intention is to assert that evolution, even of the most complex organisms, can work without any intelligent intervention.
Neither in creating appropriate variations, nor in weeding out those variations which are not advantageous.

Notwithstanding that point, even though intelligent intervention may be inessential, it may still be reasonable to consider a progress evolutionary even if some intelligence is involved in selection (as is arguably the case in sexually reproducing species) or even in variation (which genetic engineering makes possible).
For the sake of talking about a broad swath of different evolutionary processes, I therefore offer a broader conception of evolution which encompasses Darwinian evolution.

One of the challenges which I address in this reformulation is to obtain a conception of evolution which includes the processes which took place in the ``primordial soup'' leading to the first living organisms, and also to include the continuation of evolution when we have artificial intelligence incorporated into possibly inorganic systems which are capable as a whole of self-reproduction, but capable of undertaking design modifications in the process to optimise effectiveness.
Such systems will still exhibit progress which reflects the differential success of the different design decisions, and we can expect the best design decisions to become dominant among the resulting populations.

The single shared feature which I can see here is change to a population arising from differential replication of its members, which process is unlikely to progress indefinitely unless the replication is not invariably perfect.

\section{How We Got Here}

\section{Some Places We Are Going}

\section{Knowledge Representation and Deductive Closure}


\chapter{Synthetic Epistemology for Oracular AI}

% A philosophical perspective on evolution, and an evolutionary approach to philosophy.

\section{Introduction}

Homo sapiens is, as yet, ignorant of any intelligence in the universe but his own.\footnote{With apologies to theists.}
Our concept of knowledge, our epistemology, and our understanding of adjacent subjects such as language and logic, is shaped by the history of our origins.
Many now believe that intelligent artefacts will soon be created, and to a great extent those involved in their development may conceive their task as that of, first mimicking, and then surpassing, human intelligence.
But intelligence, living or synthetic, might have come to us from another part of the universe, and might then have originated in quite different ways.
When we design intelligent artefacts, perhaps intended primarily for service in progressing the exploration of the solar system and the galaxy beyond, very many aspects of the environment which nurtured human intelligence will be irrelevant.

In early approaches to the design of intelligent artefacts, it may be advantageous to rethink the foundations of epistemology which inform our thinking about knowledge and will shape the architecture.
In doing so, we have a freedom which we may not feel when engaged in the philosophical analysis of what we humans call ``knowledge''.
It is the freedom to consider what purposes knowledge might serve and how it might best serve them, and to build, from its foundations a synthetic epistemology with those purposes in mind.

That is the enterprise which I am exploring in this essay.
Though that perspective on the project is fresh, it is an enterprise which has occupied my mind for many decades while I have sought a way forward.
I hope that the essay I write here will seem too simple to have gestated for a lifetime.

\section{Oracular AI}

In the headlong rush toward what we used to call Artificial Intelligence, but now call General Artificial Intelligence, various capabilities in which computers are greatly superior to human beings seem to have passed us by.

The central place which neural nets now take, in which structured knowledge representation is abjured in favour of the digital analogue of synaptic connections and weights results in intelligence which is better at \emph{knowing how} than \emph{knowing what}, i.e. procedural rather than declarative or propositional knowledge.

Procedural knowledge predates declarative knowledge in the evolution of humanity, and as culture began to evolve this was a limiting factor.
In default of declarative language, the transmission of knowledge can only take place between individuals in close proximity.
With verbal language the dissemination can at least encompass a group of people not quite so close by, and with written language knowledge can be transmitted across large distances and over substantial periods with greater reliability.
In addition, a written record permits the gathering or application of knowledge to become an extended collaborative enterprise.

The ability of human beings acting in concert to accumulate a shared body of declarative knowledge, gradually extending into science, technology, and engineering and extending the control we exert over our environment to the benefit of humanity is something we should expect an AI to contribute to.

As the scientific knowledge of humanity has progressed, the languages which we use for science become more refined, precise and expressive and our understanding of how to reason reliability and soundly in the application of that knowledge advances.






Oracular AI is that kind of Artificial Intelligence which is oriented 

In making a connection between AI and synthetic epistemology, I have been been motivated by more than one kind of foundational thinking.
Foundational thinking is for me the quintessence of philosophy, and the synthetic epistemology I discuss here may be thought of as at a polar extreme to the idea of `epistemology naturalised'.

This philosophical synthesis is to be built from a base which is neither human nor even living, but we must begin with some sense of purpose, in default of which no criterion for success could be mooted and no sense of direction would emerge.

I therefore begin thinking of with a diverse collection of intelligent agents which both collaborate and compete in seeking to replicate themselves or some larger system of which they are a part across the cosmos.
Their success in so doing is contingent on their ability to gather knowledge and use that knowledge to plan and optimise their continuing proliferation.



There have been many substantial changes in the dominant paradigms for Artificial Intelligence over the last three quarters of a century.
At one stage, which is sometimes now referred to as GOFAI%
\footnote{Good Old Fashioned AI}, %
epistemological concerns were more prominent than they now are.
Typical epistemological concerns from that period included the representation of propositional knowledge, the ways in which one can use deduction to infer new propositional knowledge from old, and how such deductive techniques could be used for solving a broad range of problems.

Though these methods were capable in principle of solving the broad range of problems which were then thought characteristic of intelligence, they ultimately failed once large problems were tackled due to the combinatorial intractability of the relevant search spaces, and it was recognised that intelligent levels of performance on these problems depended on the ability of intelligent people to noarrow down intuitively the search space sufficiently to have a reasonable success rate in finding a solution.

Alongside the impasse created by the complexity of search spaces, problems whose solution was essential for some some applications of AI, but which were perhaps not previously thought of as requiring ``intelligence'' began to come to tne fore.
And example was the problem not of understanding the work but simply \emph{seeing} the world and being able to identify its principle features, the problem of computer vision.
This was not a problem for deduction, the things we perceive in our surroundings are not deductively inferred from the detailed sensory inputs reaching the brain from the sense organs.
This is indeed an epistemological problem, but of an entirely different kind.


When stored program digital computers were first invented their applications primarily concerned doing large amounts of information processing or computation with almost perfect reliability and at superhuman speeds.
They were accurate and reliable.

As their computational power grew their applications were extended progressively, and this sometimes involved attempts to achieve ends which were much less clearly defined, involving more complex instructions which could less certainly be relied upon to achieve the intended purpose.

The kinds of brute computational power exhibited by these early computers might at first have been thought signs of intelligence, since human skill in computation had certainly been presumed a sign of intelligence.
But brute computational power soon came to be distinguished from intelligence.

As I write, generative AI and Large Language Models have momentarily set a new standard for the unreliability of Artificial Intelligence.
Not designed or trained to be reliable repositories of knowledge, or to be capable of any but the most eleentary reasoning, their exposure to vast quantities of human knowledge enables them to perform in many main-line subject matters in an apparently authoritative way, while morphing in more esoteric areas into fantasy, and failing under even gentle interrogation to demonstrate any but the most shallow comprehension in subjects whose generally accepted facts they can replay.

It may not be so hard to improve on this.
LLMs have proven capable of using tools effectively, and tools such as more reliable ways of saving accurately and reliably knowledge acquired, or reasoning about that knowledge may not be difficult to supply.
The discussion in this essay may be thought of as concerning the use by such an AI with a tool which is capable both of storing knowledge and of deductive reasoning in the context of that knowledge.
The effect alleged would be to enable Artificial Intelligence which is \emph{oracular} in relation to logical truth.

Oracles may be thought of as having great wisdom, possibly derived from divine connection, but here I use the term more narrowly.
For the purposes of this essay the term ``oracle'' is used for something which is always truthful in answering questions, but doesn't always answer.

The oracle of interest here can be asked whether a sentence in a formal language is a \emph{logical truth} a concept which I will try to characterise, but which ultimately cannot be made absolutely definite.

The term ``Logical Truth'' is philosophically controversial.
In my usage of that term I stand on a limb, for my use is very similar to that of Rudolf Carnap, and is synonymous with the term \emph{analytic}, a concept central to Quine's repudiation of the philosophy of Carnap in the mid 20th Century.

Its not my purpose here to argue about the terminology.
Some might insist that my conception of logical truth should more properly be spoken of as set theoretic truth, and I do not intend to argue against that opinion.
Carnap, who until 1952 used the terms ``logical truth'' and ``analytic truth'' synonymously %
\footnote{as is explicit in section 2 of ``Meaning and Necessity'' \cite{carnap56}}%
, eventually accepted defeat and began to use the term ``logical truth'' for a narrower concept%
\footnote{W.V. Quine's noted a defect in Carnap's definition of analyticity in \cite{carnap56}, which followed closely a defect first seen in Wittgenstein's ``Tractatus Logico-philosophicus''\cite{Wittgenstein1921}.
Carnap's response appeared in the paper ``Meaning Postulates''\cite{carnap52} in which for the first time he separates the concept of logical truth from that of analyticity.}.

The term ``Oracular AI'' as used here, refers to what AI might in principle be able to achieve if furnished with an oracle for logical truth.

One of the purposes of this essay is to discuss how thus notion of logical truth can be made precise, to consider the difficulties in implementing such a decision procedure and to talk about the value of approximations which fall short of logical omniscience.

\section{Synthetic Epistemology}

We are concerned here with what propositions can be expressed and how the truth of those propositions can be established.

The general context here is the use of models of reality to facilitate successful planning of actions which may lead to some desirable outcome, the acquisition of food, the avoidance of peril, the construction of a skyscraper, the design of a GPU, an expedition to Mars.

These kinds of knowledge are almost invariably approximate, and the propositions involved, if judged in a black and white way are likely to be false.
As far as empirical claims in science and engineering are concerned, it is therefore preferable to find more subtle and informative ways of characterising propositions.
It is helpful in doing so to separate out an abstract model from the phenomena in question, for one may then engage in computations and reasonings about the abstract model which are exact or sound as appropriate, and subsequently and separately say something about the expected levels of correspondence between the abstract model and the concrete world.

There is, as noted by many philosophers and others, a very great divide between the levels of confidence with which we can come to conclusions in a formal mathematical context by contrast with the reliabilty with which we can measure and predict the behaviour of the real world.
In separating out these domains, we maximise the sphere in which the highest standards of confidence can be realised, and also in which the scope for automation and the application of artificial intelligence is safely (rather than speculative;y) maximised.

Beyond this very sketchy suggestion of how empirical knowledge might be addressed, similar considerations may also apply to matters in the sphere of ethics.
Moral reasoning can equally well be conducted reliably, if there is first some agreement on moral principles, provided only that moral truths are indeed systematisable in some such way.
Greater difficulties arise if moral judgements are not thought to be rational in that way, if for example, subtle judgements are needed which depend upon both a wise head and a life's experience.

Later. more discussion of empirical knowledge will be necessary, but I have sketched here a case for the necessary machinery for building abstract models to be treated as foundational.
It is more tractible than any other area, and it provides the machinery for building precise knowledge in all other areas.

There are some interesting foundational problems within that domain, which I will refer to as that of ``Logical Truth''.

\section{Logical Truth}

\part{Historical Threads}

\chapter{Terminological Notes}

\section{On the notion of Logical Truth}

The concept of logical truth has a long history in which modern controversy plays a role.
It is not my aim here to argue a case for the particular usage of the concept which I have adopted for this essay, but rather to make some observations about how that usage relates to some milestones in the history of logic which seem important in the present context.
Most discussions about the concept proceed as though there is an objective truth about the meaning of the concept, and argue for a particular explanation of what that meaning is.
That is not what is going on here.
I do not claim to know what that concept really means, I aim only to explain the usage which I have adopted and mention some ways in which this usage connects with the ideas of some other philosophers.

Let me first mention the four philosophers who seem to me to have come closest to articulating the same concept, mostly in quite different terms: Plato, Hume, Frege and Carnap.


\subsection{Plato}

Plato lived at a time when systematic deduction had first shown its value in the development of mathematics, and had also been shown capable of proving any nonsense you like in metaphysics or cosmology.
This contrast was exhibited in the conflict between the philosophies of Parmenides (who believed that nothing changes) and Heraclitus (who saw a perpetual flux).

These two philosophies were reconciled through Plato's two worlds, that of platonic ideals, and the world of appearances.
Plato thus made the distinction between logical and empirical truth which is the basis for the conception of logical truth addressed in this essay.

\subsection{Empiricism}

Plato's pupil Aristotle was to make enormous contributions to logic, but possibly not material advancement of this particular distinction.
He was concerned by the difficulty of understanding in the context of Plato's philosophy how it was possible to reason about the concrete world, dismissed by Plato as the shadowy world of appearances of which true knowledge was not possible.
The characterisation of reason as effective only in the realm of ideas and forms unfortunately excluded it from relevance to that shadowy realm of impressions which could yield no true knowledge, but which vitally concerns us all.

In articulating the concept of \emph{demonstrative science} Aristotle gave a good account of how one can reason about the concrete world, not just the etherial world of ideas, but in doing so the line he drew was between what was scientifically necessary as determined by the logical consequences of fundamental scientific principles and the accidents of how things happen to be.
The line he thus drew between necessary and contingent truths, was that between \emph{physical} (rather than \emph{logical}) necessity and his concept of contingency was confined to the accidental rather than embracing scientific laws.

Aristotle's conception of logic, and his conception of necessity was to be dominant for thousands of years, and perhaps held back further refinement of Plato's distinction.

The debates which ultimately lead to its further refinement may be thought to have begun with the division of early modern philosophers into \emph{rationaists} (Descartes, Spinoza and Leibniz) and \emph{empiricists} (Bacon, Gallileo, Locke, Berkeley and Hume).

\pagebreak
\phantomsection
\addcontentsline{toc}{section}{Bibliography}
\bibliographystyle{rbjfmu}
\bibliography{rbj2}

%\addcontentsline{toc}{section}{Index}\label{index}
%{\twocolumn[]
%{\small\printindex}}

%\vfill

\tiny{
Started 2023/07/28


\href{http://www.rbjones.com/rbjpub/www/papers/p034.pdf}{http://www.rbjones.com/rbjpub/www/papers/p035.pdf}

}%tiny

\end{document}

% LocalWords:
